==============================
Using Chapel on a Cray XE (TM)
==============================

The following information is assembled to help users get up and
running on multiple nodes of a Cray XE running the Cray Linux
Environment (TM) (CLE).  If you are not familiar with Chapel, it is
recommended that you try the instructions in the top-level README
first to get started with the language.

1) Set CHPL_HOME and MANPATH as usual.  See README.chplenv for
   details.


2) Set CHPL_HOST_PLATFORM to xe-cle and CHPL_COMM to gasnet.  For
   example:

     setenv CHPL_HOST_PLATFORM xe-cle
     setenv CHPL_COMM gasnet

   See README.multilocale for further information about running using
   multiple locales and GASNet.


3) If your Cray XT requires PBS/qsub to launch jobs onto the compute
   nodes (or you simply want to use it as your job launch mechanism),
   set CHPL_LAUNCHER to 'pbs-aprun'.  For example:

     setenv CHPL_LAUNCHER pbs-aprun

   If it only requires aprun to launch a job, set it to 'aprun'.  If
   you want to manage all job queueing/launching responsibilities
   yourself, set it to 'none'.  If left unset, the value of
   CHPL_LAUNCHER will be set automatically to one of the three
   settings above depending on the presence of aprun and/or qsub.  See
   $CHPL_HOME/doc/README.launcher for more information on Chapel's
   launcher capabilities.


4) If your Cray XE has compute nodes with varying numbers of cores,
   you will need to select a number of cores per node using the
   variable CHPL_LAUNCHER_CORES_PER_LOCALE.  For example, to use
   24-core nodes, you might use:

     setenv CHPL_LAUNCHER_CORES_PER_LOCALE 24


5) When using CHPL_LAUNCHER == pbs-aprun, you can optionally specify a
   queue name using the environment variable CHPL_LAUNCHER_QUEUE.  For
   example:

     setenv CHPL_LAUNCHER_QUEUE batch

   If this variable is left unset, no queue name will be specified.
   You can also optionally set a wall clock time limit for the job
   using CHPL_LAUNCHER_WALLTIME.  For example to specify a 10-minute
   time limit, use:

     setenv CHPL_LAUNCHER_WALLTIME 00:10:00


6) Ensure that you have one of the following Programming Environment
   modules loaded which will specify the C compiler used to compile
   Chapel programs for the compute nodes:

     - PrgEnv-cray
     - PrgEnv-gnu
     - PrgEnv-pathscale
     - PrgEnv-pgi


7) By default, g++ will be used to compile code that runs on the login
   nodes, such as the Chapel compiler and launcher code.  Optionally,
   you can override this default by setting CHPL_HOST_COMPILER to one
   of the following values:

     gnu       : the GNU compiler suite -- gcc and g++
     cray      : the Cray compiler suite -- cc and CC
     intel     : the Intel compiler suite -- icc and icpc
     pathscale : the Pathscale compiler suite -- pathcc and pathCC
     pgi       : the PGI compiler suite -- pgcc and pgCC


8) Make sure you're in the top-level chapel/ directory:

     cd $CHPL_HOME


9) Make/re-make the compiler and runtime:

     gmake


10) Compile your Chapel program as usual.  See README.compiling for
    details.  For example:

      chpl -o hello-multiloc $CHPL_HOME/examples/hello-multiloc.chpl


11) When you compile a Chapel program for the Cray XE, you should see
    two binaries (e.g., hello-multiloc and hello-multiloc_real).  The
    first binary contains code to launch the Chapel program onto the
    compute nodes, as specified by your CHPL_LAUNCHER setting.  The
    second contains the program code itself.


12) Multi-locale executions require the number of locales to be
    specified on the command line.  Other than this, execute your
    Chapel program as usual.  For example:

      ./hello-multiloc -nl 2

   You can use the -v flag to see the commands used to launch your
   program.  See README.launcher for further details.


-----------------------------------------
Cray XE File Systems and Chapel execution
-----------------------------------------

* For best results, it is recommended that you execute your Chapel
  program on a Lustre file system for the Cray XE, as this will
  provide the greatest amount of transparency between the login nodes
  and compute nodes.  In some cases, running a Chapel program from a
  non-Lustre file system will make it impossible to launch onto the
  compute nodes.  In other cases, the launch will succeed, but any
  files read or written by the Chapel program will opened relative to
  the compute node's file system rather than the login node's.  To
  avoid wrestling with such issues, we recommend executing Chapel
  programs from a Lustre file system directory.


--------------------------
Known Constraints and Bugs
--------------------------

This is a very preliminary release of Chapel for the Cray XE using
GASNet over the MPI conduit.  As such, the implementation does not
take full advantage of some of the high performance hardware
instructions.

There is a known issue with the Cray MPI release for XE systems that
causes some programs to assert and then hang during exit.  A
workaround is to set the environment variable, MPICH_GNI_DYNAMIC_CONN
to 'disabled.'  Setting this environment variable may affect memory
usage for all MPI programs linked against the Cray MPI release.  In
addition, an additional performance penalty may be paid at start up
time for thousands of MPI ranks.

