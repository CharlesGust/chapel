==============================
Using Chapel on a Cray XT (TM)
==============================

The following information is assembled to help users get up and
running on multiple nodes of a Cray XT running the Cray Linux
Environment (TM) (CLE).  If you are not familiar with Chapel, it is
recommended that you try the instructions in the top-level README
first to get started with the language.

Chapel is available as a module for Cray XE running CLE.  When it is
installed on your system, you do not need to build the compiler.
Simply load the module with:

     module load chapel

then proceed directly to compiling your Chapel programs (Step 10 below).
For information on obtaining and installing the Chapel module
please contact your system administrator.

If you are a Cray XT user on an NCCS machine, please see the special
notes at the bottom of this file.  If you have any troubles, please
let us know at chapel_info@cray.com.


1) Set CHPL_HOME and MANPATH as indicated in README.chplenv.


2) Set CHPL_HOST_PLATFORM to xt-cle and CHPL_COMM to gasnet.
   For example:

     setenv CHPL_HOST_PLATFORM xt-cle
     setenv CHPL_COMM gasnet

   See README.multilocale for further information about running using
   multiple locales and GASNet.


3) If your Cray XT requires PBS/qsub to launch jobs onto the compute
   nodes (or you simply want to use it as your job launch mechanism),
   set CHPL_LAUNCHER to 'pbs-aprun'.  For example:

     setenv CHPL_LAUNCHER pbs-aprun

   If it only requires aprun to launch a job, set it to 'aprun'.  If
   you want to manage all job queueing/launching responsibilities
   yourself, set it to 'none'.  If left unset, the value of
   CHPL_LAUNCHER will be set automatically to one of the three
   settings above depending on the presence of aprun and/or qsub.  See
   $CHPL_HOME/doc/README.launcher for more information on Chapel's
   launcher capabilities.


4) If your Cray XT has compute nodes with varying numbers of cores,
   you will need to select a number of cores per node using the
   variable CHPL_LAUNCHER_CORES_PER_LOCALE.  For example, to use
   quad-core nodes, you might use:

     setenv CHPL_LAUNCHER_CORES_PER_LOCALE 4


5) When using CHPL_LAUNCHER == pbs-aprun, you can optionally specify a
   queue name using the environment variable CHPL_LAUNCHER_QUEUE.  For
   example:

     setenv CHPL_LAUNCHER_QUEUE batch

   If this variable is left unset, no queue name will be specified.
   You can also optionally set a wall clock time limit for the job
   using CHPL_LAUNCHER_WALLTIME.  For example to specify a 10-minute
   time limit, use:

     setenv CHPL_LAUNCHER_WALLTIME 00:10:00

   NCCS users must specify either a queue or a walltime using the
   mechanisms above.
   

6) Ensure that you have one of the following Programming Environment
   modules loaded which will specify the C compiler used to compile
   Chapel programs for the compute nodes:

     - PrgEnv-cray
     - PrgEnv-gnu
     - PrgEnv-intel
     - PrgEnv-pathscale
     - PrgEnv-pgi


7) By default, g++ will be used to compile code that runs on the login
   nodes, such as the Chapel compiler and launcher code.  Optionally,
   you can override this default by setting CHPL_HOST_COMPILER to one
   of the following values:

     gnu       : the GNU compiler suite -- gcc and g++
     cray      : the Cray compiler suite -- cc and CC
     intel     : the Intel compiler suite -- icc and icpc
     pathscale : the Pathscale compiler suite -- pathcc and pathCC
     pgi       : the PGI compiler suite -- pgcc and pgCC


8) Make sure you're in the top-level chapel/ directory:

     cd $CHPL_HOME

   Make/re-make the compiler and runtime:

     gmake


9) Set your PATH to include the directory $CHPL_HOME/bin/xe-cle
   which is created when you build the compiler.  For example:

     setenv PATH "$PATH":"$CHPL_HOME/bin/xe-cle"


10) Compile your Chapel program as usual.  See README.compiling for
    details.  For example:

      chpl -o hello6-taskpar-dist $CHPL_HOME/examples/hello6-taskpar-dist.chpl


11) When you compile a Chapel program for the Cray XT, you should see
    two binaries (e.g., hello6-taskpar-dist and hello6-taskpar-dist_real).  The
    first binary contains code to launch the Chapel program onto the
    compute nodes, as specified by your CHPL_LAUNCHER setting.  The
    second contains the program code itself; it is not intended to be
    executed directly from the shell prompt.


12) Multi-locale executions require the number of locales to be
    specified on the command line.  Other than this, execute your
    Chapel program as usual.  For example:

      ./hello6-taskpar-dist -nl 2

   You can use the -v flag to see the commands used to launch your
   program.  See README.launcher for further details.


-----------------------------------------
Cray XT File Systems and Chapel execution
-----------------------------------------

* For best results, it is recommended that you execute your Chapel
  program on a Lustre file system for the Cray XT, as this will
  provide the greatest amount of transparency between the login nodes
  and compute nodes.  In some cases, running a Chapel program from a
  non-Lustre file system will make it impossible to launch onto the
  compute nodes.  In other cases, the launch will succeed, but any
  files read or written by the Chapel program will opened relative to
  the compute node's file system rather than the login node's.  To
  avoid wrestling with such issues, we recommend executing Chapel
  programs from a Lustre file system directory.


-----------------------------------------------------
Dynamic linking with the Cray Programming Environment
-----------------------------------------------------

* By default, Chapel programs built for the Cray XT do not specify
  static or dynamic linking.  The current default for the Cray
  Programming Environment compilers is static linking.  If you wish to
  override this, set the environment variable CHPL_ASYNCPE_USE_DYNAMIC
  to 'true'.  Note that it may not be possible to link dynamically
  depending on your particular system configuration.


--------------------------
Memory limits using GASNet
--------------------------

* The amount of memory that is available to a Chapel program running
  over GASNet+portals on the Cray XT is constrained by two environment
  variables: GASNET_MAX_SEGSIZE and GASNET_PHYSMEM_PINNABLE_RATIO.  If
  the user has not set GASNET_MAX_SEGSIZE, we heuristically set it for
  each compute node to be 90% of the MemTotal value reported in
  /proc/meminfo.  During initialization, the GASNet library allocates
  the lesser of GASNET_MAX_SEGSIZE and GASNET_PHYSMEM_PINNABLE_RATIO
  times the amount of available physical memory.

  If GASNET_MAX_SEGSIZE is set too high, your program may terminate
  silently, or with the message:

        _pmii_daemon(SIGCHLD): PE 0 exit signal Killed

  If running again with -v shows that the cause of the termination
  was the OOM killer:

    [NID ###] Apid ######: OOM killer terminated this process.
    Application ###### exit signals: Killed

  then GASNET_MAX_SEGSIZE is set too high.  Set it to a lower value
  and try re-running your program.  For more information on
  GASNET_MAX_SEGSIZE, refer to:

    $CHPL_HOME/third-party/gasnet/GASNet-*/portals-conduit/README

  and:

    $CHPL_HOME/third-party/gasnet/GASNet-*/README



---------------
NCCS user notes
---------------

* NCCS Cray XT machines use a different qsub mechanism in order to
  enforce their queuing policies.  We have attempted to make our
  pbs-aprun launch code work with this version of qsub, but require a
  CHPL_LAUNCHER_ACCOUNT environment variable to be set to specify your
  NCCS account name.  For example:

    setenv CHPL_LAUNCHER_ACCOUNT MYACCOUNTID


* If our PBS launcher does not work for you, you can fall back on a
  more manual launch of your program as always, either by:

  - launching the a.out_real binary manually using aprun within a
    manually-generated qsub script/command

  - setting CHPL_LAUNCHER to aprun, rebuilding the runtime,
    recompiling your program, and executing the resulting binary
    within a manually-generated qsub script.


* NCCS users either need to specify 'debug' as their queue using the
  CHPL_LAUNCHER_QUEUE or a walltime using CHPL_LAUNCHER_WALLTIME.


--------------------------
Known Constraints and Bugs
--------------------------

* GASNet targets multiple "conduits" as the underlying communication
  mechanism.  By default, the Chapel build will use the 'mpi' conduit.
  You can use the native Portals conduit by setting the environment
  variable CHPL_COMM_SUBSTRATE to 'portals.'  As a result of using the
  mpi conduit, you may see the following GASNet warning message at
  program start up:

WARNING: Using GASNet's mpi-conduit, which exists for portability convenience.
WARNING: This system appears to contain recognized network hardware: Cray XT
WARNING: which is supported by a GASNet native conduit, although
WARNING: it was not detected at configure time (missing drivers?)
WARNING: You should *really* use the high-performance native GASNet conduit
WARNING: if communication performance is at all important in this program run.

  To squelch this message, you can set the environment variable
  GASNET_QUIET=yes.

  The reason Chapel defaults to the non-native conduit is because we
  have noticed performance problems for certain Chapel programs using
  the Portals conduit.  We have not had the resources to devote to
  this problem yet, so for now we are assuming that the problems lie
  either in our GASNet port or our GASNet build configuration.

* Redirecting stdin when executing a Chapel program under PBS/qsub
  may not work due to limitations of qsub.
