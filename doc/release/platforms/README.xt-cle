==============================
Using Chapel on a Cray XT (TM)
==============================

The following information is assembled to help users get up and
running on multiple nodes of a Cray XT running the Cray Linux
Environment (TM) (CLE).  If you are not familiar with Chapel, it is
recommended that you try the instructions in the top-level README
first to get started with the language.

If you are a Cray XT user on an NCCS machine, please see the special
notes at the bottom of this file.  If you have any troubles, please
let us know at chapel_info@cray.com.


1) Set CHPL_HOME and MANPATH as usual.  See README.chplenv for
   details.


2) Set CHPL_HOST_PLATFORM to xt-cle and CHPL_COMM to gasnet.  For
   example:

     setenv CHPL_HOST_PLATFORM xt-cle
     setenv CHPL_COMM gasnet

   See README.multilocale for further information about running using
   multiple locales and GASNet.


3) If your Cray XT requires PBS/qsub to launch jobs onto the compute
   nodes (or you simply want to use it as your job launch mechanism),
   set CHPL_LAUNCHER to 'pbs-aprun'.  For example:

     setenv CHPL_LAUNCHER pbs-aprun

   Otherwise, CHPL_LAUNCHER will default to 'aprun' on xt-cle systems.
   Note that for some system installations, you may need to execute
   'module load pbs' from your shell in order to put PBS/qsub into
   your path.  See README.launcher for more information on Chapel's
   launcher capabilities.


4) If your Cray XT has compute nodes with varying numbers of cores,
   you will need to select a number of cores per node using the
   variable CHPL_LAUNCHER_CORES_PER_LOCALE.  For example, to use
   quad-core nodes, you might use:

     setenv CHPL_LAUNCHER_CORES_PER_LOCALE 4


5) When using CHPL_LAUNCHER == pbs-aprun, you can optionally specify a
   queue name using the environment variable CHPL_LAUNCHER_QUEUE.  For
   example:

     setenv CHPL_LAUNCHER_QUEUE 64n24h

   If this variable is left unset, no queue name will be specified.
   You can also optionally set a wall clock time limit for the job
   using CHPL_LAUNCHER_WALLTIME.  For example to specify a 10-minute
   time limit, use:

     setenv CHPL_LAUNCHER_WALLTIME 00:10:00

   NCCS users must specify either a queue or a walltime using the
   mechanisms above.
   

6) Ensure that you have one of the following Programming Environment
   modules loaded which will specify the C compiler used to compile
   Chapel programs for the compute nodes:

     - PrgEnv-gnu
     - PrgEnv-pathscale
     - PrgEnv-pgi


7) By default, g++ will be used to compile code that runs on the login
   nodes, such as the Chapel compiler and launcher code.  Optionally,
   you can override this default by setting CHPL_HOST_COMPILER to one
   of the following values:

     gnu       : the GNU compiler suite -- gcc and g++
     intel     : the Intel compiler suite -- icc and icpc
     pathscale : the Pathscale compiler suite -- pathcc and pathCC
     pgi       : the PGI compiler suite -- pgcc and pgCC


8) Make sure you're in the top-level chapel/ directory:

     cd $CHPL_HOME


9) Make/re-make the compiler and runtime:

     gmake


10) Compile your Chapel program as usual.  See README.compiling for
    details.  For example:

      chpl -o hello-multiloc $CHPL_HOME/examples/hello-multiloc.chpl


11) When you compile a Chapel program for the Cray XT, you should see
    two binaries (e.g., hello-multiloc and hello-multiloc_real).  The
    first binary contains code to launch the Chapel program onto the
    compute nodes, as specified by your CHPL_LAUNCHER setting.  The
    second contains the program code itself.


12) Multi-locale executions require the number of locales to be
    specified on the command line.  Other than this, execute your
    Chapel program as usual.  For example:

      ./hello-multiloc -nl 2

   You can use the -v flag to see the commands used to launch your
   program.  See README.launcher for further details.


-----------------------------------------
Cray XT File Systems and Chapel execution
-----------------------------------------

* For best results, it is recommended that you execute your Chapel
  program on a Lustre file system for the Cray XT, as this will
  provide the greatest amount of transparency between the login nodes
  and compute nodes.  In some cases, running a Chapel program from a
  non-Lustre file system will make it impossible to launch onto the
  compute nodes.  In other cases, the launch will succeed, but any
  files read or written by the Chapel program will opened relative to
  the compute node's file system rather than the login node's.  To
  avoid wrestling with such issues, we recommend executing Chapel
  programs from a Lustre file system directory.


--------------------------
Memory limits using GASNet
--------------------------

* The amount of memory that is available to a Chapel program running
  over GASNet+portals on the Cray XT is constrained by an environment
  variable named GASNET_MAX_SEGSIZE.  If the user has not set this
  variable, we heuristically set it for each compute node to be 90% of
  the MemTotal value reported by /proc/meminfo.  If GASNET_MAX_SEGSIZE
  is set too high, your program may terminate silently, or with the
  message:

        _pmii_daemon(SIGCHLD): PE 0 exit signal Killed

  If running again with -v shows that the cause of the termination
  was the OOM killer:

    [NID ###] Apid ######: OOM killer terminated this process.
    Application ###### exit signals: Killed

  then GASNET_MAX_SEGSIZE is set too high.  Set it to a lower value
  and try re-running your program.  For more information on
  GASNET_MAX_SEGSIZE, refer to:

    $CHPL_HOME/third-party/gasnet/GASNet-*/portals-conduit/README

  and:

    $CHPL_HOME/third-party/gasnet/GASNet-*/README


---------------
NCCS user notes
---------------

* NCCS Cray XT machines use a different qsub mechanism in order to
  enforce their queuing policies.  We have attempted to make our
  pbs-aprun launch code work with this version of qsub, but require a
  CHPL_LAUNCHER_ACCOUNT environment variable to be set to specify your
  NCCS account name.  For example:

    setenv CHPL_LAUNCHER_ACCOUNT MYACCOUNTID


* If our PBS launcher does not work for you, you can fall back on a
  more manual launch of your program as always, either by:

  - launching the a.out_real binary manually using aprun within a
    manually-generated qsub script/command

  - setting CHPL_LAUNCHER to aprun, rebuilding the runtime,
    recompiling your program, and executing the resulting binary
    within a manually-generated qsub script.


* NCCS users either need to specify 'debug' as their queue using the
  CHPL_LAUNCHER_QUEUE or a walltime using CHPL_LAUNCHER_WALLTIME.
