\sekshun{Data Parallelism}
\label{Data_Parallelism}
\index{data parallelism}

Chapel provides two explicit data-parallel constructs (the
forall-statement and the forall-expression) and several idioms that
support data parallelism implicitly (whole-array assignment, function
and operator promotion, reductions, and scans).

\subsection{The Forall Statement}
\label{Forall}
\index{forall@\chpl{forall}}
\index{forall loops}

The forall statement is a concurrent variant of the for statement
described in~\rsec{The_For_Loop}.

\subsubsection{Syntax}
\label{forall_syntax}

The syntax of the forall statement is given by
\begin{syntax}
forall-statement:
  `forall' index-var-declaration `in' iterator-expression `do' statement
  `forall' index-var-declaration `in' iterator-expression block-statement
  `forall' iterator-expression `do' statement
  `forall' iterator-expression block-statement
  [ index-var-declaration `in' iterator-expression ] statement
  [ iterator-expression ] statement
\end{syntax}
As with the for statement, the indices may be omitted if they are
unnecessary and the \chpl{do} keyword may be omitted before a block
statement.  The bracketed form is a syntactic convenience.

\subsubsection{Execution and Serializability}
\label{forall_semantics}

The forall statement evaluates the loop body once for each element in
the \sntx{iterator-expression}.  Each instance of the forall loop's
statement may be executed concurrently with each other, but this is
not guaranteed.  The loop must be serializable.  The definition of the
iterator determines the actual concurrency based on the specification
of the iterator of the loop.

This differs from the semantics of the \chpl{coforall} loop, discussed
in~\rsec{Coforall}, where each iteration is guaranteed to run using
distinct tasks.  The \chpl{coforall} loop thus has potentially higher
overhead than a forall loop, but in cases where concurrency is
required for correctness, it is essential.

Control continues with the statement following the forall loop only
after every iteration has been completely evaluated.

The following statements may not be lexically enclosed in forall
statements: break statements, continue statements, and return
statements.  Yield statement may only be lexically enclosed in forall
statements in parallel iterators~\rsec{Parallel_Iterators}.

\begin{example}
In the code
\begin{chapelpre}
% test_forallStmt.chpl
config const N = 5;
var a: [1..N] int;
var b = [i in 1..N] i;
\end{chapelpre}
\begin{chapel}
forall i in 1..N do
  a(i) = b(i);
\end{chapel}
the user has stated that the element-wise assignments can execute
concurrently.  This loop may be executed serially, using a distinct
task for ever iteration, or somewhere in between (using a number of
tasks where each task executes a number of iterations).  This loop can
also be written as
\begin{chapel}
[i in 1..N] a(i) = b(i);
\end{chapel}
\begin{chapelpost}
writeln(a);
\end{chapelpost}
\begin{chapeloutput}
1 2 3 4 5
\end{chapeloutput}
\end{example}

\subsubsection{Parallelism}
\label{forall_parallelism}

The iterator expression determines the number of tasks that implement
a forall loop as well as which iterations each task computes.  For
ranges, default domains, and default arrays, these values can be
controlled via configuration constants~(\rsec{data_parallel_knobs}).

Additionally, the iterator expression can determine the locales on
which the tasks should execute its loop iterations.  For ranges,
default domains, and default arrays, all tasks are executed on the
current locale.  Domains and arrays that are distributed across
multiple locales will typically implement forall loops with multiple
tasks on multiple locales.

\subsubsection{Zipper Iteration}
\label{forall_zipper}
\index{zipper iteration}

Zipper iteration has the same semantics as described
in~\rsec{Zipper_Iteration}.  With respect to parallelism, the
left-most iterator expression determines the number of tasks, the
iterations each task executes, and the locales on which these tasks
execute.

\subsubsection{Tensor Product Iteration}
\label{forall_tensor}
\index{tensor product iteration}

Tensor product iteration has the same semantics as described
in~\rsec{Tensor_Product_Iteration}.  All iteration expressions impact
parallelism as tensor product iteration is equivalent to nested forall
loops.  The degree of nested parallelism for ranges, default domains,
and default arrays can be controlled via the configuration
constant \chpl{dataParIgnoreRunningTasks}~(\rsec{data_parallel_knobs}).

\subsection{The Forall Expression}
\label{Forall_Expressions}
\index{forall expressions}

The forall expression is a concurrent variant of the for expression
described in~\rsec{For_Expressions}.

\subsubsection{Syntax}
\label{forall_expr_syntax}

The syntax of a forall expression is given by
\begin{syntax}
forall-expression:
  `forall' index-var-declaration `in' iterator-expression `do' expression
  `forall' iterator-expression `do' expression
  [ index-var-declaration `in' iterator-expression ] expression
  [ iterator-expression ] expression
\end{syntax}
As with the for expression, the indices may be omitted if they are
unnecessary.  The \chpl{do} keyword is always required.  The bracketed
form is a syntactic convenience.

\subsubsection{Execution, Serializability, and Parallelism}

As with the forall statement, the forall expression must be
serializable.  In addition, the iterator expression determines the
number of tasks, the iterations each task executes, and the locales on
which these tasks execute.  When multiple iterator expressions are
used in a zipper context, the left-most iterator determines the number
of tasks, the iterations each task executes, and the locales on which
these tasks execute.

The semantics are equivalent to calling a parallel iterator where the
loop expression is yielded~(\rsec{Parallel_Iterators}).

\begin{example}
The code
\begin{chapelpre}
% test_forallExpr.chpl
\end{chapelpre}
\begin{chapel}
writeln(+ reduce [i in 1..10] i**2);
\end{chapel}
\begin{chapeloutput}
385
\end{chapeloutput}
applies a reduction to a forall-expression that evaluates the square
of the indices in the range \chpl{1..10}.
\end{example}

\subsubsection{Filtering Predicates in Forall Expressions}
\label{Filtering_Predicates_Forall}
\index{forall expressions!and conditional expressions}

An if expression that is immediately enclosed by a forall expression
does not require an else part.
\begin{example}
The following expression returns every other element starting with the
first:
\begin{chapelpre}
% test_forallFilter.chpl
var s: [1..10] int = [i in 1..10] i;
var result =
\end{chapelpre}
\begin{chapel}
[i in 1..s.numElements] if i % 2 == 1 then s(i)
\end{chapel}
\begin{chapelpost}
;
writeln(result);
\end{chapelpost}
\begin{chapeloutput}
1 3 5 7 9
\end{chapeloutput}
\end{example}

\subsection{Whole Array Assignment}
\index{whole array assignment}

Whole array assignment is implicitly parallel.  The assignment
statement
\begin{chapel}
LHS = RHS;
\end{chapel}
is equivalent to
\begin{chapel}
forall (e1,e2) in (LHS,RHS) do
  e1 = e2;
\end{chapel}

\subsection{Promotion}
\label{Promotion}
\index{promotion}

A function that expects one or more scalar argument but is called with
one or more arrays, domains, ranges, or iterators is promoted if the
element types of the arrays, the index types of the domains and/or
ranges, and the yielded types of the iterators can resolve to the
scalar type of the argument.  The rules of when an overloaded function
is promoted are discussed in~\rsec{Function_Resolution}.

If a promoted function returns a value, the promoted function becomes
an iterator that is controlled by a loop over the iterator (or array,
domain, or range) that it is promoted by.  If the function does not
return a value, the function is controlled by a loop over the iterator
that it is promoted by, but the promotion does not become an iterator.

In addition to scalar promotion of functions, operators and casts are
also promoted.

\begin{example}
Given the array
\begin{chapel}
var A: [1..5] int = [i in 1..5] i;
\end{chapel}
and the function
\begin{chapel}
proc square(x: int) return x**2;
\end{chapel}
then the call \chpl{square(A)} results in the promotion of
the \chpl{square} function over the values in the array \chpl{A}.  The
result is an iterator that returns the
values \chpl{1}, \chpl{4}, \chpl{9}, \chpl{16}, and \chpl{25}.
\end{example}

Whole array operations are a form of promotion.

\subsubsection{Zipper Promotion}
\label{Zipper_Promotion}
\index{promotion!zipper iteration}

Consider a function \chpl{f} with formal
arguments \chpl{s1}, \chpl{s2},~... that are promoted and formal
arguments \chpl{a1}, \chpl{a2},~... that are not promoted.  The call
\begin{chapel}
f(s1, s2, ..., a1, a2, ...)
\end{chapel}
is equivalent to
\begin{chapel}
[(e1, e2, ...) in (s1, s2, ...)] f(e1, e2, ..., a1, a2, ...)
\end{chapel}
The usual constraints of zipper iteration apply to zipper promotion so
the promoted actuals must have the same shape.

\begin{example}
Given a function defined as
\begin{chapel}
proc foo(i: int, j: int) {
  write(i, " ", j, " ");
}
\end{chapel}
and a call to this function written
\begin{chapel}
foo(1..3, 4..6);
\end{chapel}
then the output is ``1 4 2 5 3 6 ''.
\end{example}

\subsubsection{Tensor Product Promotion}
\label{Tensor_Product_Promotion}
\index{promotion!tensor product iteration}

If the function \chpl{f} were called by using square brackets instead
of parentheses, the equivalent rewrite would be
\begin{chapel}
[(e1, e2, ...) in [s1, s2, ...]] f(e1, e2, ..., a1, a2, ...)
\end{chapel}
There are no constraints on tensor product promotion.

\begin{example}
Given a function defined as
\begin{chapel}
proc foo(i: int, j: int) {
  write(i, " ", j, " ");
}
\end{chapel}
and a call to this function written
\begin{chapel}
foo[1..3, 4..6];
\end{chapel}
then the output is ``1 4 1 5 1 6 2 4 2 5 2 6 3 4 3 5 3 6 ''.
\end{example}

\subsection{Reductions and Scans}
\label{Reductions_and_Scans}
\index{reductions}
\index{scans}

Chapel provides reduction and scan expressions that apply operators to
aggregate expressions in stylized ways.  Reduction expressions
collapse the aggregate's values down to a summary value.  Scan
expressions compute an aggregate of results where each result value
stores the result of a reduction applied to all of the elements in the
aggregate up to that expression.  Chapel provides a number of built-in
reduction and scan operators, and also supports a mechanism for the
user to define additional reductions and
scans~(\rsec{User_Defined_Reductions_and_Scans}).

\subsubsection{Reduction Expressions}
\label{reduce}
\index{reduction expressions}

A reduction expression applies a reduction operator to an aggregate
expression, collapsing the aggregate's dimensions down into a result
value (typically a scalar or summary expression that is independent of
the input aggregate's size).  For example, a sum reduction computes
the sum of all the elements in the input aggregate expression.

The syntax for a reduction expression is given by:
\begin{syntax}
reduce-expression:
  reduce-scan-operator `reduce' expression
  class-type `reduce' expression

reduce-scan-operator: one of
  + * && || & | ^ `min' `max' `minloc' `maxloc'
\end{syntax}

Chapel's built-in reduction operators are defined
by \sntx{reduce-scan-operator} above.  In order, they are: sum,
product, logical-and, logical-or, bitwise-and, bitwise-or,
bitwise-exclusive-or, minimum, maximum, minimum-with-location, and
maximum-with-location.  The minimum reduction returns the minimum
value as defined by the \verb@<@ operator.  The maximum reduction
returns the maximum value as defined by the \verb@>@ operator.  The
minimum-with-location reduction returns the lowest index position with
the minimum value (as defined by the \verb@<@ operator).  The
maximum-with-location reduction returns the lowest index position with
the maximum value (as defined by the \verb@>@ operator).

The expression on the right-hand side of the \chpl{reduce} keyword
can be of any type that can be iterated over and to which the
reduction operator can be applied.  For example, the bitwise-and
operator can be applied to arrays of boolean or integral types to
compute the bitwise-and of all the values in the array.

The minimum-with-location and maximum-with-location reductions take a
2-tuple of arguments where the first tuple element is the collection
of values for which the minimum/maximum value is to be computed.  The
second tuple element is a collection of indices with the same size and
shape that provides names for the locations of the values in the first
argument.  The reduction returns a tuple containing the
minimum/maximum value in the first position and the location of the
value in the second position.

\begin{example}
The first line below computes the smallest element in an array
\chpl{A} as well as its index, storing the results in \chpl{minA} and
\chpl{minALoc}, respectively.  It then computes the largest element in
a forall expression making calls to a function \chpl{foo()}, storing
the value and its number in \chpl{maxVal} and \chpl{maxValNum}.
\begin{chapel}
var (minA, minALoc) = minloc reduce (A, A.domain); 
var (maxVal, maxValNum) = maxloc reduce ([i in 1..n] foo(i), 1..n);
\end{chapel}
\end{example}

User-defined reductions are specified by preceding the
keyword \chpl{reduce} by the class type that implements the reduction
interface as described in~\rsec{User_Defined_Reductions_and_Scans}.

\subsubsection{Scan Expressions}
\label{scan}
\index{scan expressions}

A scan expression applies a scan operator to an aggregate expression,
resulting in an aggregate expression of the same size and shape.  The
output values represent the result of the operator applied to all
elements up to and including the corresponding element in the input.

The syntax for a scan expression is given by:
\begin{syntax}
scan-expression:
  reduce-scan-operator `scan' expression
  class-type `scan' expression
\end{syntax}

The built-in scans are defined in \sntx{reduce-scan-operator}.  These
are identical to the built-in reductions and are described
in~\rsec{reduce}.

The expression on the right-hand side of the scan can be of any type
that can be iterated over and to which the operator can be applied.

User-defined scans are specified by preceding the keyword \chpl{scan}
by the class type that implements the scan interface as described
in~\rsec{User_Defined_Reductions_and_Scans}.

\begin{example}
Given an array
\begin{chapel}
var A: [1..3] int = 1;
\end{chapel}
that is initialized such that each element contains one, then the code
\begin{chapel}
writeln(+ scan A);
\end{chapel}
outputs the results of scanning the array with the sum operator.  The
output is
\begin{chapel}
1 2 3
\end{chapel}
\end{example}

\subsection{Data Parallelism and Evaluation Order}
\index{data parallelism!and evaluation order}

Temporary arrays are never inserted by the Chapel compiler.  The
semantics of whole array assignment, promotion, etc., are thus
different than in array programming languages.

\begin{example}
If \chpl{A} is an array declared over the indices \chpl{1..5}, then
the following codes are not equivalent:
\begin{chapel}
A[2..4] = A[1..3] + A[3..5];
\end{chapel}
and
\begin{chapel}
var T = A[1..3] + A[3..5];
A[2..4] = T;
\end{chapel}
This follows because, in the former code, some of the new values that
are assigned to \chpl{A} may be read to compute the sum depending on
the number of tasks used to implement the data parallel statement.
\end{example}

\subsection{Knobs for Default Data Parallelism}
\label{data_parallel_knobs}
\index{data parallelism!knobs for default data parallelism}

The following configuration constants are provided to control the
degree of data parallelism over ranges, default domains, and default
arrays:

\begin{center}
\begin{tabular}{|l|l|l|}
\hline
{\bf Config Const} & {\bf Type} & {\bf Default} \\
\hline
\chpl{dataParTasksPerLocale} & \chpl{int} & Number of cores per locale \\
\chpl{dataParIgnoreRunningTasks} & \chpl{bool} & \chpl{true} \\
\chpl{dataParMinGranularity} & \chpl{int} & \chpl{1} \\
\hline
\end{tabular}
\end{center}

The configuration constant \chpl{dataParTasksPerLocale} specifies the
number of tasks to use when executing a forall loop over a range,
default domain, or default array.  The actual number of tasks may be
fewer depending on the two other configuration constants.  A value of
zero results in using the default value.

The configuration constant \chpl{dataParIgnoreRunningTasks}, when
true, has no effect on the number of tasks to use to execute the
forall loop.  When false, the number of tasks per locale is decreased
by the number of tasks that are already running on the locale.

The configuration constant \chpl{dataParMinGranularity} specifies the
minimum number of iterations per task created.  The number of tasks is
decreased so that the number of iterations per task is never less than
the specified value.

For distributed domains and arrays that have these same knobs (\eg,
using the Block and Cyclic distributions), these same global
configuration constants are used to specify their default behavior
within each locale.
