\sekshun{Data Parallelism}
\label{Data_Parallelism}

Chapel provides two explicit data-parallel constructs (the
forall-statement and the forall-expression) and several idioms that
support data parallelism implicitly (whole-array assignment, function
and operator promotion, reductions, and scans).

\subsection{The Forall Statement}
\label{Forall}
\index{forall@\chpl{forall}}
\index{forall loops}

The forall statement is a concurrent variant of the for statement
described in~\rsec{The_For_Loop}. The syntax for the
\sntx{forall-statement} is given by
\begin{syntax}
forall-statement:
  `forall' loop-control-part loop-body-part
  [ loop-control-part ] statement
\end{syntax}
The second form of the loop is a syntactic convenience.

The forall loop evaluates the loop body once for each element in the
\sntx{iterator-expression}.  Each instance of the forall loop's
statement may be executed concurrently with each other, but this is
not guaranteed.  The loop must be serializable.  The definition of the
iterator determines the actual concurrency based on the specification
of the iterator of the loop.

This differs from the semantics of the \chpl{coforall} loop, discussed
in~\rsec{Coforall}, where each iteration is guaranteed to run using
distinct tasks.  The \chpl{coforall} loop thus has potentially higher
overhead than a forall loop, but in cases where concurrency is
required for correctness, it is essential.

Control continues with the statement following the forall loop only
after every iteration has been completely evaluated.

The following statements may not be lexically enclosed in forall
statements: break statements, continue statements, and return
statements.  Yield statement may only be lexically enclosed in forall
statements in parallel iterators~\rsec{Parallel_Iterators}.

\begin{example}
In the code
\begin{chapelpre}
% test_forallStmt.chpl
config const N = 5;
var a: [1..N] int;
var b = [i in 1..N] i;
\end{chapelpre}
\begin{chapel}
forall i in 1..N do
  a(i) = b(i);
\end{chapel}
the user has stated that the element-wise assignments can execute
concurrently.  This loop may be executed serially, using a distinct
task for ever iteration, or somewhere in between (using a number of
tasks where each task executes a number of iterations).  This loop can
also be written as
\begin{chapel}
[i in 1..N] a(i) = b(i);
\end{chapel}
\begin{chapelpost}
writeln(a);
\end{chapelpost}
\begin{chapeloutput}
1 2 3 4 5
\end{chapeloutput}
\end{example}

\subsection{The Forall Expression}
\label{Forall_Expressions}
\index{forall expressions}

A forall expression can be used to enable concurrent evaluation of
sub-expressions.  The sub-expressions are evaluated once for each
element in the iterator expression.  The syntax of a forall expression
is given by
\begin{syntax}
forall-expression:
  `forall' loop-control-part `do' expression
  [loop-control-part] expression
\end{syntax}

\begin{example}
The code
\begin{chapelpre}
% test_forallExpr.chpl
\end{chapelpre}
\begin{chapel}
writeln(+ reduce [i in 1..10] i**2);
\end{chapel}
\begin{chapeloutput}
385
\end{chapeloutput}
applies a reduction to a forall-expression that evaluates the square
of the indices in the range \chpl{1..10}.
\end{example}

\subsubsection{Filtering Predicates in Forall Expressions}
\label{Filtering_Predicates_Forall}
\index{forall expressions!and conditional expressions}

An if expression that is immediately enclosed by a forall expression
does not require an else part.
\begin{example}
The following expression returns every other element starting with the
first:
\begin{chapelpre}
% test_forallFilter.chpl
var s: [1..10] int = [i in 1..10] i;
var result =
\end{chapelpre}
\begin{chapel}
[i in 1..s.numElements] if i % 2 == 1 then s(i)
\end{chapel}
\begin{chapelpost}
;
writeln(result);
\end{chapelpost}
\begin{chapeloutput}
1 3 5 7 9
\end{chapeloutput}
\end{example}

\subsection{Whole Array Assignment}

\subsection{Promotion}
\label{Promotion}

A function requires scalar promotion if an iterator (or array, domain,
or range) is passed to a formal argument with a type that allows the
yielded type of the iterator to dispatch to the formal argument.  In
the case of arrays, the yielded type is the element type.  In the case
of domains and ranges, the yielded type is the index type.  The rules
of when an overloaded function is promoted are discussed
in~\rsec{Function_Resolution}.  If a promoted function returns a
value, the promoted function becomes an iterator that is controlled by
a loop over the iterator (or array, domain, or range) that it is
promoted by.  If the function does not return a value, the function is
controlled by a loop over the iterator that it is promoted by, but the
promotion does not become an iterator.

In addition to scalar promotion of functions, operators and casts are
also promoted.

\begin{example}
Given an iterator
\begin{chapel}
def oneToFive() {
  for i in 1..5 do
    yield i;
}
\end{chapel}
and a function
\begin{chapel}
def square(x: int) return x**2;
\end{chapel}
then the call \chpl{square(oneToFive())} results in the promotion of
the \chpl{square} function over the values returned by
the \chpl{oneToFive} iterator.  The result is an iterator that returns
the values \chpl{1}, \chpl{4}, \chpl{9}, \chpl{16}, and \chpl{25}.
Instead of using the \chpl{oneToFive} iterator to promote
the \chpl{square} function, the range \chpl{1..5} could be used
directly as in \chpl{square(1..5)}.  Also note that operator
invocations are treated as function calls in terms of promotion
so \chpl{(1..5)**2} is also equivalent.
\end{example}

\subsubsection{Zipper Promotion}
\label{Zipper_Promotion}
\index{scalar promotion!zipper iteration}

Consider a function \chpl{f} with formal
arguments \chpl{s1}, \chpl{s2},~... that are promoted and formal
arguments \chpl{a1}, \chpl{a2},~... that are not promoted.  The call
\begin{chapel}
f(s1, s2, ..., a1, a2, ...)
\end{chapel}
is equivalent to
\begin{chapel}
[(e1, e2, ...) in (s1, s2, ...)] f(e1, e2, ..., a1, a2, ...)
\end{chapel}
The usual constraints of zipper iteration apply to zipper promotion so
the promoted actuals must have the same shape.

\begin{example}
Given a function defined as
\begin{chapel}
def foo(i: int, j: int) {
  write(i, " ", j, " ");
}
\end{chapel}
and a call to this function written
\begin{chapel}
foo(1..3, 4..6);
\end{chapel}
then the output is ``1 4 2 5 3 6 ''.
\end{example}

\subsubsection{Tensor Product Promotion}
\label{Tensor_Product_Promotion}
\index{scalar promotion!tensor product iteration}

If the function \chpl{f} were called by using square brackets instead
of parentheses, the equivalent rewrite would be
\begin{chapel}
[(e1, e2, ...) in [s1, s2, ...]] f(e1, e2, ..., a1, a2, ...)
\end{chapel}
There are no constraints on tensor product promotion.

\begin{example}
Given a function defined as
\begin{chapel}
def foo(i: int, j: int) {
  write(i, " ", j, " ");
}
\end{chapel}
and a call to this function written
\begin{chapel}
foo[1..3, 4..6];
\end{chapel}
then the output is ``1 4 1 5 1 6 2 4 2 5 2 6 3 4 3 5 3 6 ''.
\end{example}

\subsubsection{Promotion and Evaluation Order}

The evaluation of an iterator is interleaved with the evaluation of
the promoted expression or function.  The values produced by the
iterator are not evaluated first.  This means that the array semantics
of array programming languages are not maintained.

\begin{example}
If \chpl{A} is an array declared over the indices \chpl{1..5}, then
the following codes are not equivalent:
\begin{chapel}
A[2..4] = A[1..3] + A[3..5];
\end{chapel}
and
\begin{chapel}
var T = A[1..3] + A[3..5];
A[2..4] = T;
\end{chapel}
This follows because, in the former code, some of the new values that
are assigned to \chpl{A} may be read to compute the sum depending on
the amount of concurrency in the promotion.
\end{example}

\subsection{Reductions and Scans}
\label{Reductions_and_Scans}

Chapel provides reduction and scan expressions that apply operators to
aggregate expressions in stylized ways.  Reduction expressions
collapse the aggregate's values down to a summary value.  Scan
expressions compute an aggregate of results where each result value
stores the result of a reduction applied to all of the elements in the
aggregate up to that expression.  Chapel provides a number of built-in
reduction and scan operators, and also supports a mechanism for the
user to define additional reductions and scans.  Chapel reductions and
scans result in efficient parallel implementations, and enjoy
syntactic support to make them easy to use.


\subsubsection{Reduction Expressions}
\label{reduce}

A reduction expression applies a reduction operator to an aggregate
expression, collapsing the aggregate's dimensions down into a result
value (typically a scalar or summary expression that is independent of
the input aggregate's size).  For example, a sum reduction computes
the sum of all the elements in the input aggregate expression.

The syntax for a reduction expression is given by:
\begin{syntax}
reduce-expression:
  reduce-scan-operator `reduce' expression
  class-type `reduce' expression

reduce-scan-operator: one of
  + * && || & | ^ `min' `max' `minloc' `maxloc'
\end{syntax}

Chapel's built-in reduction operators are defined
by \sntx{reduce-scan-operator} above.  In order, they are: sum,
product, logical-and, logical-or, bitwise-and, bitwise-or,
bitwise-exclusive-or, minimum, maximum, minimum-with-location, and
maximum-with-location.

The expression on the right-hand side of the \chpl{reduce} keyword
can be of any type that can be iterated over and to which the
reduction operator can be applied.  For example, the bitwise-and
operator can be applied to arrays of boolean or integral types to
compute the bitwise-and of all the values in the array.

The minimum-with-location and maximum-with-location reductions take a
2-tuple of arguments where the first tuple element is the collection
of values for which the minimum/maximum value is to be computed.  The
second tuple element is a collection of indices with the same size and
shape that provides names for the locations of the values in the first
argument.  The reduction returns a tuple containing the
minimum/maximum value in the first position and the location of the
value in the second position.

\begin{example}
The first line below computes the smallest element in an array
\chpl{A} as well as its index, storing the results in \chpl{minA} and
\chpl{minALoc}, respectively.  It then computes the largest element in
a forall expression making calls to a function \chpl{foo()}, storing
the value and its number in \chpl{maxVal} and \chpl{maxValNum}.
\begin{chapel}
var (minA, minALoc) = minloc reduce (A, A.domain); 
var (maxVal, maxValNum) = maxloc reduce ([i in 1..n] foo(i), 1..n);
\end{chapel}
\end{example}

User-defined reductions are specified by preceding the
keyword \chpl{reduce} by the class type that implements the reduction
interface as described in~\rsec{User_Defined_Reductions_and_Scans}.

\subsubsection{Scan Expressions}
\label{scan}

A scan expression applies a scan operator to an aggregate expression,
resulting in an aggregate expression of the same size and shape.  The
output values represent the result of the operator applied to all
elements up to and including the corresponding element in the input.

The syntax for a scan expression is given by:
\begin{syntax}
scan-expression:
  reduce-scan-operator `scan' expression
  class-type `scan' expression
\end{syntax}

The built-in scans are defined in \sntx{reduce-scan-operator}.  These
are identical to the built-in reductions and are described
in~\rsec{reduce}.

The expression on the right-hand side of the scan can be of any type
that can be iterated over and to which the operator can be applied.

User-defined scans are specified by preceding the keyword \chpl{scan}
by the class type that implements the scan interface as described
in~\rsec{User_Defined_Reductions_and_Scans}.

\begin{example}
Given an array
\begin{chapel}
var A: [1..3] int = 1;
\end{chapel}
that is initialized such that each element contains one, then the code
\begin{chapel}
writeln(+ scan A);
\end{chapel}
outputs the results of scanning the array with the sum operator.  The
output is
\begin{chapel}
1 2 3
\end{chapel}
\end{example}

\subsection{Knobs for Default Data Parallelism}
