This is starpu.info, produced by makeinfo version 4.13 from starpu.texi.

Copyright (C) 2009-2011  Universite' de Bordeaux 1

Copyright (C) 2010, 2011, 2012  Centre National de la Recherche
Scientifique

Copyright (C) 2011, 2012 Institut National de Recherche en Informatique
et Automatique

     Permission is granted to copy, distribute and/or modify this
     document under the terms of the GNU Free Documentation License,
     Version 1.3 or any later version published by the Free Software
     Foundation; with no Invariant Sections, no Front-Cover Texts, and
     no Back-Cover Texts.  A copy of the license is included in the
     section entitled "GNU Free Documentation License".

INFO-DIR-SECTION Development
START-INFO-DIR-ENTRY
* StarPU: (starpu).             StarPU Handbook
END-INFO-DIR-ENTRY


File: starpu.info,  Node: Top,  Next: Introduction,  Up: (dir)

Preface
*******

This manual documents the usage of StarPU version 1.0.0rc4.  It was
last updated on 21 March 2012.

   Copyright (C) 2009-2011  Universite' de Bordeaux 1

Copyright (C) 2010, 2011, 2012  Centre National de la Recherche
Scientifique

Copyright (C) 2011, 2012 Institut National de Recherche en Informatique
et Automatique

     Permission is granted to copy, distribute and/or modify this
     document under the terms of the GNU Free Documentation License,
     Version 1.3 or any later version published by the Free Software
     Foundation; with no Invariant Sections, no Front-Cover Texts, and
     no Back-Cover Texts.  A copy of the license is included in the
     section entitled "GNU Free Documentation License".

* Menu:

* Introduction::                Getting started
* Installing StarPU::           How to configure, build and install StarPU
* Using StarPU::                How to run StarPU application
* Basic Examples::              Basic examples of the use of StarPU
* Advanced Examples::           Advanced examples of the use of StarPU
* Performance optimization::    How to optimize performance with StarPU
* Performance feedback::        Performance debugging tools
* Tips and Tricks::             Tips and tricks to know about
* StarPU MPI support::          How to combine StarPU with MPI
* StarPU FFT support::          How to perform FFT computations with StarPU
* C Extensions::                Easier StarPU programming with GCC
* SOCL OpenCL Extensions::      How to use OpenCL on top of StarPU
* StarPU Basic API::            The Basic API to use StarPU
* StarPU Advanced API::         Advanced use of StarPU
* Configuring StarPU::          How to configure StarPU
* Full source code for the 'Scaling a Vector' example::
* GNU Free Documentation License::  How you can copy and share this manual.

* Concept Index::               Index of programming concepts.
* Function Index::              Index of C functions.
* Datatype Index::              Index of C datatypes


File: starpu.info,  Node: Introduction,  Next: Installing StarPU,  Prev: Top,  Up: Top

1 Introduction to StarPU
************************

* Menu:

* Motivation::                  Why StarPU ?
* StarPU in a Nutshell::        The Fundamentals of StarPU


File: starpu.info,  Node: Motivation,  Next: StarPU in a Nutshell,  Up: Introduction

1.1 Motivation
==============

The use of specialized hardware such as accelerators or coprocessors
offers an interesting approach to overcome the physical limits
encountered by processor architects. As a result, many machines are now
equipped with one or several accelerators (e.g. a GPU), in addition to
the usual processor(s). While a lot of efforts have been devoted to
offload computation onto such accelerators, very little attention as
been paid to portability concerns on the one hand, and to the
possibility of having heterogeneous accelerators and processors to
interact on the other hand.

   StarPU is a runtime system that offers support for heterogeneous
multicore architectures, it not only offers a unified view of the
computational resources (i.e. CPUs and accelerators at the same time),
but it also takes care of efficiently mapping and executing tasks onto
an heterogeneous machine while transparently handling low-level issues
such as data transfers in a portable fashion.


File: starpu.info,  Node: StarPU in a Nutshell,  Prev: Motivation,  Up: Introduction

1.2 StarPU in a Nutshell
========================

StarPU is a software tool aiming to allow programmers to exploit the
computing power of the available CPUs and GPUs, while relieving them
from the need to specially adapt their programs to the target machine
and processing units.

   At the core of StarPU is its run-time support library, which is
responsible for scheduling application-provided tasks on heterogeneous
CPU/GPU machines.  In addition, StarPU comes with programming language
support, in the form of extensions to languages of the C family (*note
C Extensions::), as well as an OpenCL front-end (*note SOCL OpenCL
Extensions::).

   StarPU's run-time and programming language extensions support a
"task-based programming model".  Applications submit computational
tasks, with CPU and/or GPU implementations, and StarPU schedules these
tasks and associated data transfers on available CPUs and GPUs.  The
data that a task manipulates are automatically transferred among
accelerators and the main memory, so that programmers are freed from the
scheduling issues and technical details associated with these transfers.

   StarPU takes particular care of scheduling tasks efficiently, using
well-known algorithms from the literature (*note Task scheduling
policy::).  In addition, it allows scheduling experts, such as compiler
or computational library developers, to implement custom scheduling
policies in a portable fashion (*note Scheduling Policy API::).

   The remainder of this section describes the main concepts used in
StarPU.

* Menu:

* Codelet and Tasks::
* StarPU Data Management Library::
* Glossary::
* Research Papers::


File: starpu.info,  Node: Codelet and Tasks,  Next: StarPU Data Management Library,  Up: StarPU in a Nutshell

1.2.1 Codelet and Tasks
-----------------------

One of the StarPU primary data structures is the codelet. A codelet
describes a computational kernel that can possibly be implemented on
multiple architectures such as a CPU, a CUDA device or a Cell's SPU.

   Another important data structure is the task. Executing a StarPU task
consists in applying a codelet on a data set, on one of the
architectures on which the codelet is implemented. A task thus
describes the codelet that it uses, but also which data are accessed,
and how they are accessed during the computation (read and/or write).
StarPU tasks are asynchronous: submitting a task to StarPU is a
non-blocking operation. The task structure can also specify a callback
function that is called once StarPU has properly executed the task. It
also contains optional fields that the application may use to give
hints to the scheduler (such as priority levels).

   By default, task dependencies are inferred from data dependency
(sequential coherence) by StarPU. The application can however disable
sequential coherency for some data, and dependencies be expressed by
hand.  A task may be identified by a unique 64-bit number chosen by the
application which we refer as a tag.  Task dependencies can be enforced
by hand either by the means of callback functions, by submitting other
tasks, or by expressing dependencies between tags (which can thus
correspond to tasks that have not been submitted yet).


File: starpu.info,  Node: StarPU Data Management Library,  Next: Glossary,  Prev: Codelet and Tasks,  Up: StarPU in a Nutshell

1.2.2 StarPU Data Management Library
------------------------------------

Because StarPU schedules tasks at runtime, data transfers have to be
done automatically and "just-in-time" between processing units,
relieving the application programmer from explicit data transfers.
Moreover, to avoid unnecessary transfers, StarPU keeps data where it
was last needed, even if was modified there, and it allows multiple
copies of the same data to reside at the same time on several
processing units as long as it is not modified.


File: starpu.info,  Node: Glossary,  Next: Research Papers,  Prev: StarPU Data Management Library,  Up: StarPU in a Nutshell

1.2.3 Glossary
--------------

A codelet records pointers to various implementations of the same
theoretical function.

   A memory node can be either the main RAM or GPU-embedded memory.

   A bus is a link between memory nodes.

   A data handle keeps track of replicates of the same data (registered
by the application) over various memory nodes. The data management
library manages keeping them coherent.

   The home memory node of a data handle is the memory node from which
the data was registered (usually the main memory node).

   A task represents a scheduled execution of a codelet on some data
handles.

   A tag is a rendez-vous point. Tasks typically have their own tag,
and can depend on other tags. The value is chosen by the application.

   A worker execute tasks. There is typically one per CPU computation
core and one per accelerator (for which a whole CPU core is dedicated).

   A driver drives a given kind of workers. There are currently CPU,
CUDA, OpenCL and Gordon drivers. They usually start several workers to
actually drive them.

   A performance model is a (dynamic or static) model of the
performance of a given codelet. Codelets can have execution time
performance model as well as power consumption performance models.

   A data interface describes the layout of the data: for a vector, a
pointer for the start, the number of elements and the size of elements
; for a matrix, a pointer for the start, the number of elements per
row, the offset between rows, and the size of each element ; etc. To
access their data, codelet functions are given interfaces for the local
memory node replicates of the data handles of the scheduled task.

   Partitioning data means dividing the data of a given data handle
(called father) into a series of children data handles which designate
various portions of the former.

   A filter is the function which computes children data handles from a
father data handle, and thus describes how the partitioning should be
done (horizontal, vertical, etc.)

   Acquiring a data handle can be done from the main application, to
safely access the data of a data handle from its home node, without
having to unregister it.


File: starpu.info,  Node: Research Papers,  Prev: Glossary,  Up: StarPU in a Nutshell

1.2.4 Research Papers
---------------------

Research papers about StarPU can be found at

   <http://runtime.bordeaux.inria.fr/Publis/Keyword/STARPU.html>

   Notably a good overview in the research report

   <http://hal.archives-ouvertes.fr/inria-00467677>


File: starpu.info,  Node: Installing StarPU,  Next: Using StarPU,  Prev: Introduction,  Up: Top

2 Installing StarPU
*******************

* Menu:

* Downloading StarPU::
* Configuration of StarPU::
* Building and Installing StarPU::

   StarPU can be built and installed by the standard means of the GNU
autotools. The following chapter is intended to briefly remind how
these tools can be used to install StarPU.


File: starpu.info,  Node: Downloading StarPU,  Next: Configuration of StarPU,  Up: Installing StarPU

2.1 Downloading StarPU
======================

* Menu:

* Getting Sources::
* Optional dependencies::


File: starpu.info,  Node: Getting Sources,  Next: Optional dependencies,  Up: Downloading StarPU

2.1.1 Getting Sources
---------------------

The latest official release tarballs of StarPU sources are available
for download from <https://gforge.inria.fr/frs/?group_id=1570>.

   The latest nightly development snapshot is available from
<http://starpu.gforge.inria.fr/testing/>.

     % wget http://starpu.gforge.inria.fr/testing/starpu-nightly-latest.tar.gz

   Additionally, the code can be directly checked out of Subversion, it
should be done only if you need the very latest changes (i.e. less than
a day!).(1).

     % svn checkout svn://scm.gforge.inria.fr/svn/starpu/trunk

   ---------- Footnotes ----------

   (1) The client side of the software Subversion can be obtained from
<http://subversion.tigris.org>. If you are running on Windows, you will
probably prefer to use TortoiseSVN from <http://tortoisesvn.tigris.org/>


File: starpu.info,  Node: Optional dependencies,  Prev: Getting Sources,  Up: Downloading StarPU

2.1.2 Optional dependencies
---------------------------

The topology discovery library, `hwloc', is not mandatory to use StarPU
but strongly recommended. It allows to increase performance, and to
perform some topology aware scheduling.

   `hwloc' is available in major distributions and for most OSes and
can be downloaded from <http://www.open-mpi.org/software/hwloc>.


File: starpu.info,  Node: Configuration of StarPU,  Next: Building and Installing StarPU,  Prev: Downloading StarPU,  Up: Installing StarPU

2.2 Configuration of StarPU
===========================

* Menu:

* Generating Makefiles and configuration scripts::
* Running the configuration::


File: starpu.info,  Node: Generating Makefiles and configuration scripts,  Next: Running the configuration,  Up: Configuration of StarPU

2.2.1 Generating Makefiles and configuration scripts
----------------------------------------------------

This step is not necessary when using the tarball releases of StarPU.
If you are using the source code from the svn repository, you first
need to generate the configure scripts and the Makefiles. This requires
the availability of `autoconf', `automake' >= 2.60, and `makeinfo'.

     % ./autogen.sh


File: starpu.info,  Node: Running the configuration,  Prev: Generating Makefiles and configuration scripts,  Up: Configuration of StarPU

2.2.2 Running the configuration
-------------------------------

     % ./configure

   Details about options that are useful to give to `./configure' are
given in *note Compilation configuration::.


File: starpu.info,  Node: Building and Installing StarPU,  Prev: Configuration of StarPU,  Up: Installing StarPU

2.3 Building and Installing StarPU
==================================

* Menu:

* Building::
* Sanity Checks::
* Installing::


File: starpu.info,  Node: Building,  Next: Sanity Checks,  Up: Building and Installing StarPU

2.3.1 Building
--------------

     % make


File: starpu.info,  Node: Sanity Checks,  Next: Installing,  Prev: Building,  Up: Building and Installing StarPU

2.3.2 Sanity Checks
-------------------

In order to make sure that StarPU is working properly on the system, it
is also possible to run a test suite.

     % make check


File: starpu.info,  Node: Installing,  Prev: Sanity Checks,  Up: Building and Installing StarPU

2.3.3 Installing
----------------

In order to install StarPU at the location that was specified during
configuration:

     % make install

   Libtool interface versioning information are included in libraries
names (libstarpu-1.0.so, libstarpumpi-1.0.so and libstarpufft-1.0.so).


File: starpu.info,  Node: Using StarPU,  Next: Basic Examples,  Prev: Installing StarPU,  Up: Top

3 Using StarPU
**************

* Menu:

* Setting flags for compiling and linking applications::
* Running a basic StarPU application::
* Kernel threads started by StarPU::
* Enabling OpenCL::


File: starpu.info,  Node: Setting flags for compiling and linking applications,  Next: Running a basic StarPU application,  Up: Using StarPU

3.1 Setting flags for compiling and linking applications
========================================================

Compiling and linking an application against StarPU may require to use
specific flags or libraries (for instance `CUDA' or `libspe2').  To
this end, it is possible to use the `pkg-config' tool.

   If StarPU was not installed at some standard location, the path of
StarPU's library must be specified in the `PKG_CONFIG_PATH' environment
variable so that `pkg-config' can find it. For example if StarPU was
installed in `$prefix_dir':

     % PKG_CONFIG_PATH=$PKG_CONFIG_PATH:$prefix_dir/lib/pkgconfig

   The flags required to compile or link against StarPU are then
accessible with the following commands(1):

     % pkg-config --cflags starpu-1.0  # options for the compiler
     % pkg-config --libs starpu-1.0    # options for the linker

   Also pass the `--static' option if the application is to be linked
statically.

   ---------- Footnotes ----------

   (1) It is still possible to use the API provided in the version 0.9
of StarPU by calling `pkg-config' with the `libstarpu' package. Similar
packages are provided for `libstarpumpi' and `libstarpufft'.


File: starpu.info,  Node: Running a basic StarPU application,  Next: Kernel threads started by StarPU,  Prev: Setting flags for compiling and linking applications,  Up: Using StarPU

3.2 Running a basic StarPU application
======================================

Basic examples using StarPU are built in the directory
`examples/basic_examples/' (and installed in
`$prefix_dir/lib/starpu/examples/'). You can for example run the example
`vector_scal'.

     % ./examples/basic_examples/vector_scal
     BEFORE: First element was 1.000000
     AFTER: First element is 3.140000
     %

   When StarPU is used for the first time, the directory
`$STARPU_HOME/.starpu/' is created, performance models will be stored in
that directory (`STARPU_HOME' defaults to `$HOME')

   Please note that buses are benchmarked when StarPU is launched for
the first time. This may take a few minutes, or less if `hwloc' is
installed. This step is done only once per user and per machine.


File: starpu.info,  Node: Kernel threads started by StarPU,  Next: Enabling OpenCL,  Prev: Running a basic StarPU application,  Up: Using StarPU

3.3 Kernel threads started by StarPU
====================================

StarPU automatically binds one thread per CPU core. It does not use
SMT/hyperthreading because kernels are usually already optimized for
using a full core, and using hyperthreading would make kernel
calibration rather random.

   Since driving GPUs is a CPU-consuming task, StarPU dedicates one
core per GPU

   While StarPU tasks are executing, the application is not supposed to
do computations in the threads it starts itself, tasks should be used
instead.

   TODO: add a StarPU function to bind an application thread (e.g. the
main thread) to a dedicated core (and thus disable the corresponding
StarPU CPU worker).


File: starpu.info,  Node: Enabling OpenCL,  Prev: Kernel threads started by StarPU,  Up: Using StarPU

3.4 Enabling OpenCL
===================

When both CUDA and OpenCL drivers are enabled, StarPU will launch an
OpenCL worker for NVIDIA GPUs only if CUDA is not already running on
them.  This design choice was necessary as OpenCL and CUDA can not run
at the same time on the same NVIDIA GPU, as there is currently no
interoperability between them.

   To enable OpenCL, you need either to disable CUDA when configuring
StarPU:

     % ./configure --disable-cuda

   or when running applications:

     % STARPU_NCUDA=0 ./application

   OpenCL will automatically be started on any device not yet used by
CUDA. So on a machine running 4 GPUS, it is therefore possible to
enable CUDA on 2 devices, and OpenCL on the 2 other devices by doing so:

     % STARPU_NCUDA=2 ./application


File: starpu.info,  Node: Basic Examples,  Next: Advanced Examples,  Prev: Using StarPU,  Up: Top

4 Basic Examples
****************

* Menu:

* Compiling and linking options::
* Hello World::                 Submitting Tasks
* Vector Scaling Using the C Extension::
* Vector Scaling Using StarPu's API::
* Vector Scaling on an Hybrid CPU/GPU Machine::  Handling Heterogeneous Architectures


File: starpu.info,  Node: Compiling and linking options,  Next: Hello World,  Up: Basic Examples

4.1 Compiling and linking options
=================================

Let's suppose StarPU has been installed in the directory `$STARPU_DIR'.
As explained in *note Setting flags for compiling and linking
applications::, the variable `PKG_CONFIG_PATH' needs to be set. It is
also necessary to set the variable `LD_LIBRARY_PATH' to locate dynamic
libraries at runtime.

     % PKG_CONFIG_PATH=$STARPU_DIR/lib/pkgconfig:$PKG_CONFIG_PATH
     % LD_LIBRARY_PATH=$STARPU_DIR/lib:$LD_LIBRARY_PATH

   The Makefile could for instance contain the following lines to
define which options must be given to the compiler and to the linker:

     CFLAGS          +=      $$(pkg-config --cflags starpu-1.0)
     LDFLAGS         +=      $$(pkg-config --libs starpu-1.0)

   Make sure that `pkg-config --libs starpu-1.0' actually produces some
output before going further: `PKG_CONFIG_PATH' has to point to the
place where `starpu-1.0.pc' was installed during `make install'.

   Also pass the `--static' option if the application is to be linked
statically.


File: starpu.info,  Node: Hello World,  Next: Vector Scaling Using the C Extension,  Prev: Compiling and linking options,  Up: Basic Examples

4.2 Hello World
===============

This section shows how to implement a simple program that submits a task
to StarPU. You can either use the StarPU C extension (*note C
Extensions::) or directly use the StarPU's API.

* Menu:

* Hello World using the C Extension::
* Hello World using StarPU's API::


File: starpu.info,  Node: Hello World using the C Extension,  Next: Hello World using StarPU's API,  Up: Hello World

4.2.1 Hello World using the C Extension
---------------------------------------

GCC from version 4.5 permit to use the StarPU GCC plug-in (*note C
Extensions::). This makes writing a task both simpler and less
error-prone.  In a nutshell, all it takes is to declare a task, declare
and define its implementations (for CPU, OpenCL, and/or CUDA), and
invoke the task like a regular C function.  The example below defines
`my_task', which has a single implementation for CPU:

     /* Task declaration.  */
     static void my_task (int x) __attribute__ ((task));

     /* Declaration of the CPU implementation of `my_task'.  */
     static void my_task_cpu (int x)
        __attribute__ ((task_implementation ("cpu", my_task)));

     /* Definition of said CPU implementation.  */
     static void my_task_cpu (int x)
     {
       printf ("Hello, world!  With x = %d\n", x);
     }

     int main ()
     {
       /* Initialize StarPU.  */
     #pragma starpu initialize

       /* Do an asynchronous call to `my_task'.  */
       my_task (42);

       /* Wait for the call to complete.  */
     #pragma starpu wait

       /* Terminate.  */
     #pragma starpu shutdown

       return 0;
     }

The code can then be compiled and linked with GCC and the `-fplugin'
flag:

     $ gcc hello-starpu.c \
         -fplugin=`pkg-config starpu-1.0 --variable=gccplugin` \
         `pkg-config starpu-1.0 --libs`

   As can be seen above, basic use the C extensions allows programmers
to use StarPU tasks while essentially annotating "regular" C code.


File: starpu.info,  Node: Hello World using StarPU's API,  Prev: Hello World using the C Extension,  Up: Hello World

4.2.2 Hello World using StarPU's API
------------------------------------

The remainder of this section shows how to achieve the same result using
StarPU's standard C API.

* Menu:

* Required Headers::
* Defining a Codelet::
* Submitting a Task::
* Execution of Hello World::


File: starpu.info,  Node: Required Headers,  Next: Defining a Codelet,  Up: Hello World using StarPU's API

4.2.2.1 Required Headers
........................

The `starpu.h' header should be included in any code using StarPU.

     #include <starpu.h>


File: starpu.info,  Node: Defining a Codelet,  Next: Submitting a Task,  Prev: Required Headers,  Up: Hello World using StarPU's API

4.2.2.2 Defining a Codelet
..........................

     struct params {
         int i;
         float f;
     };
     void cpu_func(void *buffers[], void *cl_arg)
     {
         struct params *params = cl_arg;

         printf("Hello world (params = {%i, %f} )\n", params->i, params->f);
     }

     struct starpu_codelet cl =
     {
         .where = STARPU_CPU,
         .cpu_funcs = { cpu_func, NULL },
         .nbuffers = 0
     };

A codelet is a structure that represents a computational kernel. Such a
codelet may contain an implementation of the same kernel on different
architectures (e.g. CUDA, Cell's SPU, x86, ...).

   The `nbuffers' field specifies the number of data buffers that are
manipulated by the codelet: here the codelet does not access or modify
any data that is controlled by our data management library. Note that
the argument passed to the codelet (the `cl_arg' field of the
`starpu_task' structure) does not count as a buffer since it is not
managed by our data management library, but just contain trivial
parameters.

   We create a codelet which may only be executed on the CPUs. The
`where' field is a bitmask that defines where the codelet may be
executed. Here, the `STARPU_CPU' value means that only CPUs can execute
this codelet (*note Codelets and Tasks:: for more details on this
field). Note that the `where' field is optional, when unset its value is
automatically set based on the availability of the different
`XXX_funcs' fields.  When a CPU core executes a codelet, it calls the
`cpu_func' function, which _must_ have the following prototype:

   `void (*cpu_func)(void *buffers[], void *cl_arg);'

   In this example, we can ignore the first argument of this function
which gives a description of the input and output buffers (e.g. the
size and the location of the matrices) since there is none.  The second
argument is a pointer to a buffer passed as an argument to the codelet
by the means of the `cl_arg' field of the `starpu_task' structure.

   Be aware that this may be a pointer to a _copy_ of the actual
buffer, and not the pointer given by the programmer: if the codelet
modifies this buffer, there is no guarantee that the initial buffer
will be modified as well: this for instance implies that the buffer
cannot be used as a synchronization medium. If synchronization is
needed, data has to be registered to StarPU, see *note Vector Scaling
Using StarPu's API::.


File: starpu.info,  Node: Submitting a Task,  Next: Execution of Hello World,  Prev: Defining a Codelet,  Up: Hello World using StarPU's API

4.2.2.3 Submitting a Task
.........................

     void callback_func(void *callback_arg)
     {
         printf("Callback function (arg %x)\n", callback_arg);
     }

     int main(int argc, char **argv)
     {
         /* initialize StarPU */
         starpu_init(NULL);

         struct starpu_task *task = starpu_task_create();

         task->cl = &cl; /* Pointer to the codelet defined above */

         struct params params = { 1, 2.0f };
         task->cl_arg = &params;
         task->cl_arg_size = sizeof(params);

         task->callback_func = callback_func;
         task->callback_arg = 0x42;

         /* starpu_task_submit will be a blocking call */
         task->synchronous = 1;

         /* submit the task to StarPU */
         starpu_task_submit(task);

         /* terminate StarPU */
         starpu_shutdown();

         return 0;
     }

Before submitting any tasks to StarPU, `starpu_init' must be called. The
`NULL' argument specifies that we use default configuration. Tasks
cannot be submitted after the termination of StarPU by a call to
`starpu_shutdown'.

   In the example above, a task structure is allocated by a call to
`starpu_task_create'. This function only allocates and fills the
corresponding structure with the default settings (*note
starpu_task_create: Codelets and Tasks.), but it does not submit the
task to StarPU.

   The `cl' field is a pointer to the codelet which the task will
execute: in other words, the codelet structure describes which
computational kernel should be offloaded on the different
architectures, and the task structure is a wrapper containing a codelet
and the piece of data on which the codelet should operate.

   The optional `cl_arg' field is a pointer to a buffer (of size
`cl_arg_size') with some parameters for the kernel described by the
codelet. For instance, if a codelet implements a computational kernel
that multiplies its input vector by a constant, the constant could be
specified by the means of this buffer, instead of registering it as a
StarPU data. It must however be noted that StarPU avoids making copy
whenever possible and rather passes the pointer as such, so the buffer
which is pointed at must kept allocated until the task terminates, and
if several tasks are submitted with various parameters, each of them
must be given a pointer to their own buffer.

   Once a task has been executed, an optional callback function is be
called.  While the computational kernel could be offloaded on various
architectures, the callback function is always executed on a CPU. The
`callback_arg' pointer is passed as an argument of the callback. The
prototype of a callback function must be:

   `void (*callback_function)(void *);'

   If the `synchronous' field is non-zero, task submission will be
synchronous: the `starpu_task_submit' function will not return until the
task was executed. Note that the `starpu_shutdown' method does not
guarantee that asynchronous tasks have been executed before it returns,
`starpu_task_wait_for_all' can be used to that effect, or data can be
unregistered (`starpu_data_unregister(vector_handle);'), which will
implicitly wait for all the tasks scheduled to work on it, unless
explicitly disabled thanks to
`starpu_data_set_default_sequential_consistency_flag' or
`starpu_data_set_sequential_consistency_flag'.


File: starpu.info,  Node: Execution of Hello World,  Prev: Submitting a Task,  Up: Hello World using StarPU's API

4.2.2.4 Execution of Hello World
................................

     % make hello_world
     cc $(pkg-config --cflags starpu-1.0)  $(pkg-config --libs starpu-1.0) hello_world.c -o hello_world
     % ./hello_world
     Hello world (params = {1, 2.000000} )
     Callback function (arg 42)


File: starpu.info,  Node: Vector Scaling Using the C Extension,  Next: Vector Scaling Using StarPu's API,  Prev: Hello World,  Up: Basic Examples

4.3 Vector Scaling Using the C Extension
========================================

The previous example has shown how to submit tasks. In this section, we
show how StarPU tasks can manipulate data. The version of this example
using StarPU's API is given in the next sections.

* Menu:

* Adding an OpenCL Task Implementation::
* Adding a CUDA Task Implementation::

   The simplest way to get started writing StarPU programs is using the
C language extensions provided by the GCC plug-in (*note C
Extensions::).  These extensions map directly to StarPU's main
concepts: tasks, task implementations for CPU, OpenCL, or CUDA, and
registered data buffers.

   The example below is a vector-scaling program, that multiplies
elements of a vector by a given factor(1).  For comparison, the
standard C version that uses StarPU's standard C programming interface
is given in the next section (*note standard C version of the example:
Vector Scaling Using StarPu's API.).

   First of all, the vector-scaling task and its simple CPU
implementation has to be defined:

     /* Declare the `vector_scal' task.  */

     static void vector_scal (unsigned size, float vector[size],
                              float factor)
       __attribute__ ((task));

     /* Declare and define the standard CPU implementation.  */

     static void vector_scal_cpu (unsigned size, float vector[size],
                                  float factor)
       __attribute__ ((task_implementation ("cpu", vector_scal)));

     static void
     vector_scal_cpu (unsigned size, float vector[size], float factor)
     {
       unsigned i;
       for (i = 0; i < size; i++)
         vector[i] *= factor;
     }

   Next, the body of the program, which uses the task defined above,
can be implemented:

     int
     main (void)
     {
     #pragma starpu initialize

     #define NX     0x100000
     #define FACTOR 3.14

       {
         float vector[NX] __attribute__ ((heap_allocated));

     #pragma starpu register vector

         size_t i;
         for (i = 0; i < NX; i++)
           vector[i] = (float) i;

         vector_scal (NX, vector, FACTOR);

     #pragma starpu wait
       } /* VECTOR is automatically freed here.  */

     #pragma starpu shutdown

       return valid ? EXIT_SUCCESS : EXIT_FAILURE;
     }

The `main' function above does several things:

   * It initializes StarPU.

   * It allocates VECTOR in the heap; it will automatically be freed
     when its scope is left.  Alternatively, good old `malloc' and
     `free' could have been used, but they are more error-prone and
     require more typing.

   * It "registers" the memory pointed to by VECTOR.  Eventually, when
     OpenCL or CUDA task implementations are added, this will allow
     StarPU to transfer that memory region between GPUs and the main
     memory.  Removing this `pragma' is an error.

   * It invokes the `vector_scal' task.  The invocation looks the same
     as a standard C function call.  However, it is an "asynchronous
     invocation", meaning that the actual call is performed in parallel
     with the caller's continuation.

   * It "waits" for the termination of the `vector_scal' asynchronous
     call.

   * Finally, StarPU is shut down.


   The program can be compiled and linked with GCC and the `-fplugin'
flag:

     $ gcc hello-starpu.c \
         -fplugin=`pkg-config starpu-1.0 --variable=gccplugin` \
         `pkg-config starpu-1.0 --libs`

   And voila`!

   ---------- Footnotes ----------

   (1) The complete example, and additional examples, is available in
the `gcc-plugin/examples' directory of the StarPU distribution.


File: starpu.info,  Node: Adding an OpenCL Task Implementation,  Next: Adding a CUDA Task Implementation,  Up: Vector Scaling Using the C Extension

4.3.1 Adding an OpenCL Task Implementation
------------------------------------------

Now, this is all fine and great, but you certainly want to take
advantage of these newfangled GPUs that your lab just bought, don't you?

   So, let's add an OpenCL implementation of the `vector_scal' task.
We assume that the OpenCL kernel is available in a file,
`vector_scal_opencl_kernel.cl', not shown here.  The OpenCL task
implementation is similar to that used with the standard C API (*note
Definition of the OpenCL Kernel::).  It is declared and defined in our
C file like this:

     /* Include StarPU's OpenCL integration.  */
     #include <starpu_opencl.h>

     /* The OpenCL programs, loaded from `main' (see below).  */
     static struct starpu_opencl_program cl_programs;

     static void vector_scal_opencl (unsigned size, float vector[size],
                                     float factor)
       __attribute__ ((task_implementation ("opencl", vector_scal)));

     static void
     vector_scal_opencl (unsigned size, float vector[size], float factor)
     {
       int id, devid, err;
       cl_kernel kernel;
       cl_command_queue queue;
       cl_event event;

       /* VECTOR is GPU memory pointer, not a main memory pointer.  */
       cl_mem val = (cl_mem) vector;

       id = starpu_worker_get_id ();
       devid = starpu_worker_get_devid (id);

       /* Prepare to invoke the kernel.  In the future, this will be largely
          automated.  */
       err = starpu_opencl_load_kernel (&kernel, &queue, &cl_programs,
                                        "vector_mult_opencl", devid);
       if (err != CL_SUCCESS)
         STARPU_OPENCL_REPORT_ERROR (err);

       err = clSetKernelArg (kernel, 0, sizeof (val), &val);
       err |= clSetKernelArg (kernel, 1, sizeof (size), &size);
       err |= clSetKernelArg (kernel, 2, sizeof (factor), &factor);
       if (err)
         STARPU_OPENCL_REPORT_ERROR (err);

       size_t global = 1, local = 1;
       err = clEnqueueNDRangeKernel (queue, kernel, 1, NULL, &global,
                                     &local, 0, NULL, &event);
       if (err != CL_SUCCESS)
         STARPU_OPENCL_REPORT_ERROR (err);

       clFinish (queue);
       starpu_opencl_collect_stats (event);
       clReleaseEvent (event);

       /* Done with KERNEL.  */
       starpu_opencl_release_kernel (kernel);
     }

The OpenCL kernel itself must be loaded from `main', sometime after the
`initialize' pragma:

       starpu_opencl_load_opencl_from_file ("vector_scal_opencl_kernel.cl",
                                            &cl_programs, "");

And that's it.  The `vector_scal' task now has an additional
implementation, for OpenCL, which StarPU's scheduler may choose to use
at run-time.  Unfortunately, the `vector_scal_opencl' above still has
to go through the common OpenCL boilerplate; in the future, additional
extensions will automate most of it.


File: starpu.info,  Node: Adding a CUDA Task Implementation,  Prev: Adding an OpenCL Task Implementation,  Up: Vector Scaling Using the C Extension

4.3.2 Adding a CUDA Task Implementation
---------------------------------------

Adding a CUDA implementation of the task is very similar, except that
the implementation itself is typically written in CUDA, and compiled
with `nvcc'.  Thus, the C file only needs to contain an external
declaration for the task implementation:

     extern void vector_scal_cuda (unsigned size, float vector[size],
                                   float factor)
       __attribute__ ((task_implementation ("cuda", vector_scal)));

   The actual implementation of the CUDA task goes into a separate
compilation unit, in a `.cu' file.  It is very close to the
implementation when using StarPU's standard C API (*note Definition of
the CUDA Kernel::).

     /* CUDA implementation of the `vector_scal' task, to be compiled
        with `nvcc'.  */

     #include <starpu.h>
     #include <starpu_cuda.h>
     #include <stdlib.h>

     static __global__ void
     vector_mult_cuda (float *val, unsigned n, float factor)
     {
       unsigned i = blockIdx.x * blockDim.x + threadIdx.x;

       if (i < n)
         val[i] *= factor;
     }

     /* Definition of the task implementation declared in the C file.   */
     extern "C" void
     vector_scal_cuda (size_t size, float vector[], float factor)
     {
       unsigned threads_per_block = 64;
       unsigned nblocks = (size + threads_per_block - 1) / threads_per_block;

       vector_mult_cuda <<< nblocks, threads_per_block, 0,
         starpu_cuda_get_local_stream () >>> (vector, size, factor);

       cudaStreamSynchronize (starpu_cuda_get_local_stream ());
     }

   The complete source code, in the `gcc-plugin/examples/vector_scal'
directory of the StarPU distribution, also shows how an SSE-specialized
CPU task implementation can be added.

   For more details on the C extensions provided by StarPU's GCC
plug-in, *Note C Extensions::.


File: starpu.info,  Node: Vector Scaling Using StarPu's API,  Next: Vector Scaling on an Hybrid CPU/GPU Machine,  Prev: Vector Scaling Using the C Extension,  Up: Basic Examples

4.4 Vector Scaling Using StarPu's API
=====================================

This section shows how to achieve the same result as explained in the
previous section using StarPU's standard C API.

   The full source code for this example is given in *note Full source
code for the 'Scaling a Vector' example::.

* Menu:

* Source Code of Vector Scaling::
* Execution of Vector Scaling::  Running the program


File: starpu.info,  Node: Source Code of Vector Scaling,  Next: Execution of Vector Scaling,  Up: Vector Scaling Using StarPu's API

4.4.1 Source Code of Vector Scaling
-----------------------------------

Programmers can describe the data layout of their application so that
StarPU is responsible for enforcing data coherency and availability
across the machine.  Instead of handling complex (and non-portable)
mechanisms to perform data movements, programmers only declare which
piece of data is accessed and/or modified by a task, and StarPU makes
sure that when a computational kernel starts somewhere (e.g. on a GPU),
its data are available locally.

   Before submitting those tasks, the programmer first needs to declare
the different pieces of data to StarPU using the
`starpu_*_data_register' functions. To ease the development of
applications for StarPU, it is possible to describe multiple types of
data layout. A type of data layout is called an interface. There are
different predefined interfaces available in StarPU: here we will
consider the vector interface.

   The following lines show how to declare an array of `NX' elements of
type `float' using the vector interface:

     float vector[NX];

     starpu_data_handle_t vector_handle;
     starpu_vector_data_register(&vector_handle, 0, (uintptr_t)vector, NX,
                                 sizeof(vector[0]));

   The first argument, called the data handle, is an opaque pointer
which designates the array in StarPU. This is also the structure which
is used to describe which data is used by a task. The second argument
is the node number where the data originally resides. Here it is 0
since the `vector' array is in the main memory. Then comes the pointer
`vector' where the data can be found in main memory, the number of
elements in the vector and the size of each element.  The following
shows how to construct a StarPU task that will manipulate the vector
and a constant factor.

     float factor = 3.14;
     struct starpu_task *task = starpu_task_create();

     task->cl = &cl;                      /* Pointer to the codelet defined below */
     task->handles[0] = vector_handle;    /* First parameter of the codelet */
     task->cl_arg = &factor;
     task->cl_arg_size = sizeof(factor);
     task->synchronous = 1;

     starpu_task_submit(task);

   Since the factor is a mere constant float value parameter, it does
not need a preliminary registration, and can just be passed through the
`cl_arg' pointer like in the previous example.  The vector parameter is
described by its handle.  There are two fields in each element of the
`buffers' array.  `handle' is the handle of the data, and `mode'
specifies how the kernel will access the data (`STARPU_R' for
read-only, `STARPU_W' for write-only and `STARPU_RW' for read and write
access).

   The definition of the codelet can be written as follows:

     void scal_cpu_func(void *buffers[], void *cl_arg)
     {
         unsigned i;
         float *factor = cl_arg;

         /* length of the vector */
         unsigned n = STARPU_VECTOR_GET_NX(buffers[0]);
         /* CPU copy of the vector pointer */
         float *val = (float *)STARPU_VECTOR_GET_PTR(buffers[0]);

         for (i = 0; i < n; i++)
             val[i] *= *factor;
     }

     struct starpu_codelet cl = {
         .where = STARPU_CPU,
         .cpu_funcs = { scal_cpu_func, NULL },
         .nbuffers = 1,
         .modes = { STARPU_RW }
     };

   The first argument is an array that gives a description of all the
buffers passed in the `task->handles' array. The size of this array is
given by the `nbuffers' field of the codelet structure. For the sake of
genericity, this array contains pointers to the different interfaces
describing each buffer.  In the case of the vector interface, the
location of the vector (resp. its length) is accessible in the `ptr'
(resp. `nx') of this array. Since the vector is accessed in a
read-write fashion, any modification will automatically affect future
accesses to this vector made by other tasks.

   The second argument of the `scal_cpu_func' function contains a
pointer to the parameters of the codelet (given in `task->cl_arg'), so
that we read the constant factor from this pointer.


File: starpu.info,  Node: Execution of Vector Scaling,  Prev: Source Code of Vector Scaling,  Up: Vector Scaling Using StarPu's API

4.4.2 Execution of Vector Scaling
---------------------------------

     % make vector_scal
     cc $(pkg-config --cflags starpu-1.0)  $(pkg-config --libs starpu-1.0)  vector_scal.c   -o vector_scal
     % ./vector_scal
     0.000000 3.000000 6.000000 9.000000 12.000000


File: starpu.info,  Node: Vector Scaling on an Hybrid CPU/GPU Machine,  Prev: Vector Scaling Using StarPu's API,  Up: Basic Examples

4.5 Vector Scaling on an Hybrid CPU/GPU Machine
===============================================

Contrary to the previous examples, the task submitted in this example
may not only be executed by the CPUs, but also by a CUDA device.

* Menu:

* Definition of the CUDA Kernel::
* Definition of the OpenCL Kernel::
* Definition of the Main Code::
* Execution of Hybrid Vector Scaling::


File: starpu.info,  Node: Definition of the CUDA Kernel,  Next: Definition of the OpenCL Kernel,  Up: Vector Scaling on an Hybrid CPU/GPU Machine

4.5.1 Definition of the CUDA Kernel
-----------------------------------

The CUDA implementation can be written as follows. It needs to be
compiled with a CUDA compiler such as nvcc, the NVIDIA CUDA compiler
driver. It must be noted that the vector pointer returned by
STARPU_VECTOR_GET_PTR is here a pointer in GPU memory, so that it can
be passed as such to the `vector_mult_cuda' kernel call.

     #include <starpu.h>
     #include <starpu_cuda.h>

     static __global__ void vector_mult_cuda(float *val, unsigned n,
                                             float factor)
     {
         unsigned i =  blockIdx.x*blockDim.x + threadIdx.x;
         if (i < n)
             val[i] *= factor;
     }

     extern "C" void scal_cuda_func(void *buffers[], void *_args)
     {
         float *factor = (float *)_args;

         /* length of the vector */
         unsigned n = STARPU_VECTOR_GET_NX(buffers[0]);
         /* CUDA copy of the vector pointer */
         float *val = (float *)STARPU_VECTOR_GET_PTR(buffers[0]);
         unsigned threads_per_block = 64;
         unsigned nblocks = (n + threads_per_block-1) / threads_per_block;

         vector_mult_cuda<<<nblocks,threads_per_block, 0, starpu_cuda_get_local_stream()>>>(val, n, *factor);

         cudaStreamSynchronize(starpu_cuda_get_local_stream());
     }


File: starpu.info,  Node: Definition of the OpenCL Kernel,  Next: Definition of the Main Code,  Prev: Definition of the CUDA Kernel,  Up: Vector Scaling on an Hybrid CPU/GPU Machine

4.5.2 Definition of the OpenCL Kernel
-------------------------------------

The OpenCL implementation can be written as follows. StarPU provides
tools to compile a OpenCL kernel stored in a file.

     __kernel void vector_mult_opencl(__global float* val, int nx, float factor)
     {
             const int i = get_global_id(0);
             if (i < nx) {
                     val[i] *= factor;
             }
     }

   Contrary to CUDA and CPU, `STARPU_VECTOR_GET_DEV_HANDLE' has to be
used, which returns a `cl_mem' (which is not a device pointer, but an
OpenCL handle), which can be passed as such to the OpenCL kernel. The
difference is important when using partitioning, see *note Partitioning
Data::.

     #include <starpu.h>
     #include <starpu_opencl.h>

     extern struct starpu_opencl_program programs;

     void scal_opencl_func(void *buffers[], void *_args)
     {
         float *factor = _args;
         int id, devid, err;
         cl_kernel kernel;
         cl_command_queue queue;
         cl_event event;

         /* length of the vector */
         unsigned n = STARPU_VECTOR_GET_NX(buffers[0]);
         /* OpenCL copy of the vector pointer */
         cl_mem val = (cl_mem) STARPU_VECTOR_GET_DEV_HANDLE(buffers[0]);

         id = starpu_worker_get_id();
         devid = starpu_worker_get_devid(id);

         err = starpu_opencl_load_kernel(&kernel, &queue, &programs,
                         "vector_mult_opencl", devid);   /* Name of the codelet defined above */
         if (err != CL_SUCCESS) STARPU_OPENCL_REPORT_ERROR(err);

         err = clSetKernelArg(kernel, 0, sizeof(val), &val);
         err |= clSetKernelArg(kernel, 1, sizeof(n), &n);
         err |= clSetKernelArg(kernel, 2, sizeof(*factor), factor);
         if (err) STARPU_OPENCL_REPORT_ERROR(err);

         {
             size_t global=n;
             size_t local=1;
             err = clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &global, &local, 0, NULL, &event);
             if (err != CL_SUCCESS) STARPU_OPENCL_REPORT_ERROR(err);
         }

         clFinish(queue);
         starpu_opencl_collect_stats(event);
         clReleaseEvent(event);

         starpu_opencl_release_kernel(kernel);
     }


File: starpu.info,  Node: Definition of the Main Code,  Next: Execution of Hybrid Vector Scaling,  Prev: Definition of the OpenCL Kernel,  Up: Vector Scaling on an Hybrid CPU/GPU Machine

4.5.3 Definition of the Main Code
---------------------------------

The CPU implementation is the same as in the previous section.

   Here is the source of the main application. You can notice the value
of the field `where' for the codelet. We specify
`STARPU_CPU|STARPU_CUDA|STARPU_OPENCL' to indicate to StarPU that the
codelet can be executed either on a CPU or on a CUDA or an OpenCL
device.

     #include <starpu.h>

     #define NX 2048

     extern void scal_cuda_func(void *buffers[], void *_args);
     extern void scal_cpu_func(void *buffers[], void *_args);
     extern void scal_opencl_func(void *buffers[], void *_args);

     /* Definition of the codelet */
     static struct starpu_codelet cl = {
         .where = STARPU_CPU|STARPU_CUDA|STARPU_OPENCL; /* It can be executed on a CPU, */
                                          /* on a CUDA device, or on an OpenCL device */
         .cuda_funcs = { scal_cuda_func, NULL },
         .cpu_funcs = { scal_cpu_func, NULL },
         .opencl_funcs = { scal_opencl_func, NULL },
         .nbuffers = 1,
         .modes = { STARPU_RW }
     }

     #ifdef STARPU_USE_OPENCL
     /* The compiled version of the OpenCL program */
     struct starpu_opencl_program programs;
     #endif

     int main(int argc, char **argv)
     {
         float *vector;
         int i, ret;
         float factor=3.0;
         struct starpu_task *task;
         starpu_data_handle_t vector_handle;

         starpu_init(NULL);                            /* Initialising StarPU */

     #ifdef STARPU_USE_OPENCL
         starpu_opencl_load_opencl_from_file(
                 "examples/basic_examples/vector_scal_opencl_codelet.cl",
                 &programs, NULL);
     #endif

         vector = malloc(NX*sizeof(vector[0]));
         assert(vector);
         for(i=0 ; i<NX ; i++) vector[i] = i;

         /* Registering data within StarPU */
         starpu_vector_data_register(&vector_handle, 0, (uintptr_t)vector,
                                     NX, sizeof(vector[0]));

         /* Definition of the task */
         task = starpu_task_create();
         task->cl = &cl;
         task->handles[0] = vector_handle;
         task->cl_arg = &factor;
         task->cl_arg_size = sizeof(factor);

         /* Submitting the task */
         ret = starpu_task_submit(task);
         if (ret == -ENODEV) {
                 fprintf(stderr, "No worker may execute this task\n");
                 return 1;
         }

         /* Waiting for its termination */
         starpu_task_wait_for_all();

         /* Update the vector in RAM */
         starpu_data_acquire(vector_handle, STARPU_R);

         /* Access the data */
         for(i=0 ; i<NX; i++) {
           fprintf(stderr, "%f ", vector[i]);
         }
         fprintf(stderr, "\n");

         /* Release the RAM view of the data before unregistering it and shutting down StarPU */
         starpu_data_release(vector_handle);
         starpu_data_unregister(vector_handle);
         starpu_shutdown();

         return 0;
     }


File: starpu.info,  Node: Execution of Hybrid Vector Scaling,  Prev: Definition of the Main Code,  Up: Vector Scaling on an Hybrid CPU/GPU Machine

4.5.4 Execution of Hybrid Vector Scaling
----------------------------------------

The Makefile given at the beginning of the section must be extended to
give the rules to compile the CUDA source code. Note that the source
file of the OpenCL kernel does not need to be compiled now, it will be
compiled at run-time when calling the function
`starpu_opencl_load_opencl_from_file()' (*note
starpu_opencl_load_opencl_from_file::).

     CFLAGS  += $(shell pkg-config --cflags starpu-1.0)
     LDFLAGS += $(shell pkg-config --libs starpu-1.0)
     CC       = gcc

     vector_scal: vector_scal.o vector_scal_cpu.o vector_scal_cuda.o vector_scal_opencl.o

     %.o: %.cu
            nvcc $(CFLAGS) $< -c $
     clean:
            rm -f vector_scal *.o

     % make

   and to execute it, with the default configuration:

     % ./vector_scal
     0.000000 3.000000 6.000000 9.000000 12.000000

   or for example, by disabling CPU devices:

     % STARPU_NCPUS=0 ./vector_scal
     0.000000 3.000000 6.000000 9.000000 12.000000

   or by disabling CUDA devices (which may permit to enable the use of
OpenCL, see *note Enabling OpenCL::):

     % STARPU_NCUDA=0 ./vector_scal
     0.000000 3.000000 6.000000 9.000000 12.000000


File: starpu.info,  Node: Advanced Examples,  Next: Performance optimization,  Prev: Basic Examples,  Up: Top

5 Advanced Examples
*******************

* Menu:

* Using multiple implementations of a codelet::
* Enabling implementation according to capabilities::
* Task and Worker Profiling::
* Partitioning Data::
* Performance model example::
* Theoretical lower bound on execution time::
* Insert Task Utility::
* Data reduction::
* Parallel Tasks::
* Debugging::
* The multiformat interface::
* On-GPU rendering::
* More examples::               More examples shipped with StarPU


File: starpu.info,  Node: Using multiple implementations of a codelet,  Next: Enabling implementation according to capabilities,  Up: Advanced Examples

5.1 Using multiple implementations of a codelet
===============================================

One may want to write multiple implementations of a codelet for a
single type of device and let StarPU choose which one to run. As an
example, we will show how to use SSE to scale a vector. The codelet can
be written as follows:

     #include <xmmintrin.h>

     void scal_sse_func(void *buffers[], void *cl_arg)
     {
         float *vector = (float *) STARPU_VECTOR_GET_PTR(buffers[0]);
         unsigned int n = STARPU_VECTOR_GET_NX(buffers[0]);
         unsigned int n_iterations = n/4;
         if (n % 4 != 0)
             n_iterations++;

         __m128 *VECTOR = (__m128*) vector;
         __m128 factor __attribute__((aligned(16)));
         factor = _mm_set1_ps(*(float *) cl_arg);

         unsigned int i;
         for (i = 0; i < n_iterations; i++)
             VECTOR[i] = _mm_mul_ps(factor, VECTOR[i]);
     }

     struct starpu_codelet cl = {
         .where = STARPU_CPU,
         .cpu_funcs = { scal_cpu_func, scal_sse_func, NULL },
         .nbuffers = 1,
         .modes = { STARPU_RW }
     };

   Schedulers which are multi-implementation aware (only `dmda', `heft'
and `pheft' for now) will use the performance models of all the
implementations it was given, and pick the one that seems to be the
fastest.


File: starpu.info,  Node: Enabling implementation according to capabilities,  Next: Task and Worker Profiling,  Prev: Using multiple implementations of a codelet,  Up: Advanced Examples

5.2 Enabling implementation according to capabilities
=====================================================

Some implementations may not run on some devices. For instance, some
CUDA devices do not support double floating point precision, and thus
the kernel execution would just fail; or the device may not have enough
shared memory for the implementation being used. The `can_execute'
field of the `struct starpu_codelet' structure permits to express this.
For instance:

     static int can_execute(unsigned workerid, struct starpu_task *task, unsigned nimpl)
     {
       const struct cudaDeviceProp *props;
       if (starpu_worker_get_type(workerid) == STARPU_CPU_WORKER)
         return 1;
       /* Cuda device */
       props = starpu_cuda_get_device_properties(workerid);
       if (props->major >= 2 || props->minor >= 3)
         /* At least compute capability 1.3, supports doubles */
         return 1;
       /* Old card, does not support doubles */
       return 0;
     }

     struct starpu_codelet cl = {
         .where = STARPU_CPU|STARPU_CUDA,
         .can_execute = can_execute,
         .cpu_funcs = { cpu_func, NULL },
         .cuda_funcs = { gpu_func, NULL }
         .nbuffers = 1,
         .modes = { STARPU_RW }
     };

   This can be essential e.g. when running on a machine which mixes
various models of CUDA devices, to take benefit from the new models
without crashing on old models.

   Note: the `can_execute' function is called by the scheduler each
time it tries to match a task with a worker, and should thus be very
fast. The `starpu_cuda_get_device_properties' provides a quick access
to CUDA properties of CUDA devices to achieve such efficiency.

   Another example is compiling CUDA code for various compute
capabilities, resulting with two CUDA functions, e.g. `scal_gpu_13' for
compute capability 1.3, and `scal_gpu_20' for compute capability 2.0.
Both functions can be provided to StarPU by using `cuda_funcs', and
`can_execute' can then be used to rule out the `scal_gpu_20' variant on
a CUDA device which will not be able to execute it:

     static int can_execute(unsigned workerid, struct starpu_task *task, unsigned nimpl)
     {
       const struct cudaDeviceProp *props;
       if (starpu_worker_get_type(workerid) == STARPU_CPU_WORKER)
         return 1;
       /* Cuda device */
       if (nimpl == 0)
         /* Trying to execute the 1.3 capability variant, we assume it is ok in all cases.  */
         return 1;
       /* Trying to execute the 2.0 capability variant, check that the card can do it.  */
       props = starpu_cuda_get_device_properties(workerid);
       if (props->major >= 2 || props->minor >= 0)
         /* At least compute capability 2.0, can run it */
         return 1;
       /* Old card, does not support 2.0, will not be able to execute the 2.0 variant.  */
       return 0;
     }

     struct starpu_codelet cl = {
         .where = STARPU_CPU|STARPU_CUDA,
         .can_execute = can_execute,
         .cpu_funcs = { cpu_func, NULL },
         .cuda_funcs = { scal_gpu_13, scal_gpu_20, NULL },
         .nbuffers = 1,
         .modes = { STARPU_RW }
     };

   Note: the most generic variant should be provided first, as some
schedulers are not able to try the different variants.


File: starpu.info,  Node: Task and Worker Profiling,  Next: Partitioning Data,  Prev: Enabling implementation according to capabilities,  Up: Advanced Examples

5.3 Task and Worker Profiling
=============================

A full example showing how to use the profiling API is available in the
StarPU sources in the directory `examples/profiling/'.

     struct starpu_task *task = starpu_task_create();
     task->cl = &cl;
     task->synchronous = 1;
     /* We will destroy the task structure by hand so that we can
      * query the profiling info before the task is destroyed. */
     task->destroy = 0;

     /* Submit and wait for completion (since synchronous was set to 1) */
     starpu_task_submit(task);

     /* The task is finished, get profiling information */
     struct starpu_task_profiling_info *info = task->profiling_info;

     /* How much time did it take before the task started ? */
     double delay += starpu_timing_timespec_delay_us(&info->submit_time, &info->start_time);

     /* How long was the task execution ? */
     double length += starpu_timing_timespec_delay_us(&info->start_time, &info->end_time);

     /* We don't need the task structure anymore */
     starpu_task_destroy(task);

     /* Display the occupancy of all workers during the test */
     int worker;
     for (worker = 0; worker < starpu_worker_get_count(); worker++)
     {
             struct starpu_worker_profiling_info worker_info;
             int ret = starpu_worker_get_profiling_info(worker, &worker_info);
             STARPU_ASSERT(!ret);

             double total_time = starpu_timing_timespec_to_us(&worker_info.total_time);
             double executing_time = starpu_timing_timespec_to_us(&worker_info.executing_time);
             double sleeping_time = starpu_timing_timespec_to_us(&worker_info.sleeping_time);

             float executing_ratio = 100.0*executing_time/total_time;
             float sleeping_ratio = 100.0*sleeping_time/total_time;

             char workername[128];
             starpu_worker_get_name(worker, workername, 128);
             fprintf(stderr, "Worker %s:\n", workername);
             fprintf(stderr, "\ttotal time: %.2lf ms\n", total_time*1e-3);
             fprintf(stderr, "\texec time: %.2lf ms (%.2f %%)\n", executing_time*1e-3,
                     executing_ratio);
             fprintf(stderr, "\tblocked time: %.2lf ms (%.2f %%)\n", sleeping_time*1e-3,
                     sleeping_ratio);
     }


File: starpu.info,  Node: Partitioning Data,  Next: Performance model example,  Prev: Task and Worker Profiling,  Up: Advanced Examples

5.4 Partitioning Data
=====================

An existing piece of data can be partitioned in sub parts to be used by
different tasks, for instance:

     int vector[NX];
     starpu_data_handle_t handle;

     /* Declare data to StarPU */
     starpu_vector_data_register(&handle, 0, (uintptr_t)vector, NX, sizeof(vector[0]));

     /* Partition the vector in PARTS sub-vectors */
     starpu_filter f =
     {
         .filter_func = starpu_block_filter_func_vector,
         .nchildren = PARTS
     };
     starpu_data_partition(handle, &f);

   The task submission then uses `starpu_data_get_sub_data' to retrive
the sub-handles to be passed as tasks parameters.

     /* Submit a task on each sub-vector */
     for (i=0; i<starpu_data_get_nb_children(handle); i++) {
         /* Get subdata number i (there is only 1 dimension) */
         starpu_data_handle_t sub_handle = starpu_data_get_sub_data(handle, 1, i);
         struct starpu_task *task = starpu_task_create();

         task->handles[0] = sub_handle;
         task->cl = &cl;
         task->synchronous = 1;
         task->cl_arg = &factor;
         task->cl_arg_size = sizeof(factor);

         starpu_task_submit(task);
     }

   Partitioning can be applied several times, see
`examples/basic_examples/mult.c' and `examples/filters/'.

   Wherever the whole piece of data is already available, the
partitioning will be done in-place, i.e. without allocating new buffers
but just using pointers inside the existing copy. This is particularly
important to be aware of when using OpenCL, where the kernel parameters
are not pointers, but handles. The kernel thus needs to be also passed
the offset within the OpenCL buffer:

     void opencl_func(void *buffers[], void *cl_arg)
     {
         cl_mem vector = (cl_mem) STARPU_VECTOR_GET_DEV_HANDLE(buffers[0]);
         unsigned offset = STARPU_BLOCK_GET_OFFSET(buffers[0]);

         ...
         clSetKernelArg(kernel, 0, sizeof(vector), &vector);
         clSetKernelArg(kernel, 1, sizeof(offset), &offset);
         ...
     }

   And the kernel has to shift from the pointer passed by the OpenCL
driver:

     __kernel void opencl_kernel(__global int *vector, unsigned offset)
     {
         block = (__global void *)block + offset;
         ...
     }


File: starpu.info,  Node: Performance model example,  Next: Theoretical lower bound on execution time,  Prev: Partitioning Data,  Up: Advanced Examples

5.5 Performance model example
=============================

To achieve good scheduling, StarPU scheduling policies need to be able
to estimate in advance the duration of a task. This is done by giving
to codelets a performance model, by defining a `starpu_perfmodel'
structure and providing its address in the `model' field of the `struct
starpu_codelet' structure. The `symbol' and `type' fields of
`starpu_perfmodel' are mandatory, to give a name to the model, and the
type of the model, since there are several kinds of performance models.

   * Measured at runtime (`STARPU_HISTORY_BASED' model type). This
     assumes that for a given set of data input/output sizes, the
     performance will always be about the same. This is very true for
     regular kernels on GPUs for instance (<0.1% error), and just a bit
     less true on CPUs (~=1% error). This also assumes that there are
     few different sets of data input/output sizes. StarPU will then
     keep record of the average time of previous executions on the
     various processing units, and use it as an estimation. History is
     done per task size, by using a hash of the input and ouput sizes
     as an index.  It will also save it in `~/.starpu/sampling/codelets'
     for further executions, and can be observed by using the
     `starpu_perfmodel_display' command, or drawn by using
     the `starpu_perfmodel_plot'.  The models are indexed by machine
     name. To share the models between machines (e.g. for a homogeneous
     cluster), use `export STARPU_HOSTNAME=some_global_name'.
     Measurements are only done when using a task scheduler which makes
     use of it, such as `heft' or `dmda'.

     The following is a small code example.

     If e.g. the code is recompiled with other compilation options, or
     several variants of the code are used, the symbol string should be
     changed to reflect that, in order to recalibrate a new model from
     zero. The symbol string can even be constructed dynamically at
     execution time, as long as this is done before submitting any task
     using it.

          static struct starpu_perfmodel mult_perf_model = {
              .type = STARPU_HISTORY_BASED,
              .symbol = "mult_perf_model"
          };

          struct starpu_codelet cl = {
              .where = STARPU_CPU,
              .cpu_funcs = { cpu_mult, NULL },
              .nbuffers = 3,
              .modes = { STARPU_R, STARPU_R, STARPU_W },
              /* for the scheduling policy to be able to use performance models */
              .model = &mult_perf_model
          };

   * Measured at runtime and refined by regression
     (`STARPU_*REGRESSION_BASED' model type). This still assumes
     performance regularity, but can work with various data input
     sizes, by applying regression over observed execution times.
     STARPU_REGRESSION_BASED uses an a*n^b regression form,
     STARPU_NL_REGRESSION_BASED uses an a*n^b+c (more precise than
     STARPU_REGRESSION_BASED, but costs a lot more to compute). For
     instance, `tests/perfmodels/regression_based.c' uses a
     regression-based performance model for the `memset' operation. Of
     course, the application has to issue tasks with varying size so
     that the regression can be computed. StarPU will not trust the
     regression unless there is at least 10% difference between the
     minimum and maximum observed input size. For non-linear
     regression, since computing it is quite expensive, it is only done
     at termination of the application. This means that the first
     execution uses history-based performance model to perform
     scheduling.

   * Provided as an estimation from the application itself
     (`STARPU_COMMON' model type and `cost_function' field), see for
     instance `examples/common/blas_model.h' and
     `examples/common/blas_model.c'.

   * Provided explicitly by the application (`STARPU_PER_ARCH' model
     type): the `.per_arch[arch][nimpl].cost_function' fields have to
     be filled with pointers to functions which return the expected
     duration of the task in micro-seconds, one per architecture.


   For the `STARPU_HISTORY_BASED' and `STARPU_*REGRESSION_BASE', the
total size of task data (both input and output) is used as an index by
default. The `size_base' field of `struct starpu_perfmodel' however
permits the application to override that, when for instance some of the
data do not matter for task cost (e.g. mere reference table), or when
using sparse structures (in which case it is the number of non-zeros
which matter), or when there is some hidden parameter such as the
number of iterations, etc.

   How to use schedulers which can benefit from such performance model
is explained in *note Task scheduling policy::.

   The same can be done for task power consumption estimation, by
setting the `power_model' field the same way as the `model' field.
Note: for now, the application has to give to the power consumption
performance model a name which is different from the execution time
performance model.

   The application can request time estimations from the StarPU
performance models by filling a task structure as usual without
actually submitting it. The data handles can be created by calling
`starpu_data_register' functions with a `NULL' pointer (and need to be
unregistered as usual) and the desired data sizes. The
`starpu_task_expected_length' and `starpu_task_expected_power'
functions can then be called to get an estimation of the task duration
on a given arch. `starpu_task_destroy' needs to be called to destroy
the dummy task afterwards. See `tests/perfmodels/regression_based.c'
for an example.


File: starpu.info,  Node: Theoretical lower bound on execution time,  Next: Insert Task Utility,  Prev: Performance model example,  Up: Advanced Examples

5.6 Theoretical lower bound on execution time
=============================================

For kernels with history-based performance models, StarPU can very
easily provide a theoretical lower bound for the execution time of a
whole set of tasks. See for instance `examples/lu/lu_example.c': before
submitting tasks, call `starpu_bound_start', and after complete
execution, call `starpu_bound_stop'. `starpu_bound_print_lp' or
`starpu_bound_print_mps' can then be used to output a Linear Programming
problem corresponding to the schedule of your tasks. Run it through
`lp_solve' or any other linear programming solver, and that will give
you a lower bound for the total execution time of your tasks. If StarPU
was compiled with the glpk library installed, `starpu_bound_compute'
can be used to solve it immediately and get the optimized minimum, in
ms. Its `integer' parameter allows to decide whether integer resolution
should be computed and returned too.

   The `deps' parameter tells StarPU whether to take tasks and implicit
data dependencies into account. It must be understood that the linear
programming problem size is quadratic with the number of tasks and thus
the time to solve it will be very long, it could be minutes for just a
few dozen tasks. You should probably use `lp_solve -timeout 1 test.pl
-wmps test.mps' to convert the problem to MPS format and then use a
better solver, `glpsol' might be better than `lp_solve' for instance
(the `--pcost' option may be useful), but sometimes doesn't manage to
converge. `cbc' might look slower, but it is parallel. Be sure to try
at least all the `-B' options of `lp_solve'. For instance, we often
just use `lp_solve -cc -B1 -Bb -Bg -Bp -Bf -Br -BG -Bd -Bs -BB -Bo -Bc
-Bi' , and the `-gr' option can also be quite useful.

   Setting `deps' to 0 will only take into account the actual
computations on processing units. It however still properly takes into
account the varying performances of kernels and processing units, which
is quite more accurate than just comparing StarPU performances with the
fastest of the kernels being used.

   The `prio' parameter tells StarPU whether to simulate taking into
account the priorities as the StarPU scheduler would, i.e. schedule
prioritized tasks before less prioritized tasks, to check to which
extend this results to a less optimal solution. This increases even
more computation time.

   Note that for simplicity, all this however doesn't take into account
data transfers, which are assumed to be completely overlapped.


File: starpu.info,  Node: Insert Task Utility,  Next: Data reduction,  Prev: Theoretical lower bound on execution time,  Up: Advanced Examples

5.7 Insert Task Utility
=======================

StarPU provides the wrapper function `starpu_insert_task' to ease the
creation and submission of tasks.

 -- Function: int starpu_insert_task (struct starpu_codelet *CL, ...)
     Create and submit a task corresponding to CL with the following
     arguments.  The argument list must be zero-terminated.

     The arguments following the codelets can be of the following types:

        * `STARPU_R', `STARPU_W', `STARPU_RW', `STARPU_SCRATCH',
          `STARPU_REDUX' an access mode followed by a data handle;

        * the specific values `STARPU_VALUE', `STARPU_CALLBACK',
          `STARPU_CALLBACK_ARG', `STARPU_CALLBACK_WITH_ARG',
          `STARPU_PRIORITY', followed by the appropriated objects as
          defined below.

     Parameters to be passed to the codelet implementation are defined
     through the type `STARPU_VALUE'. The function
     `starpu_codelet_unpack_args' must be called within the codelet
     implementation to retrieve them.

 -- Macro: STARPU_VALUE
     this macro is used when calling `starpu_insert_task', and must be
     followed by a pointer to a constant value and the size of the
     constant

 -- Macro: STARPU_CALLBACK
     this macro is used when calling `starpu_insert_task', and must be
     followed by a pointer to a callback function

 -- Macro: STARPU_CALLBACK_ARG
     this macro is used when calling `starpu_insert_task', and must be
     followed by a pointer to be given as an argument to the callback
     function

 -- Macro: STARPU_CALLBACK_WITH_ARG
     this macro is used when calling `starpu_insert_task', and must be
     followed by two pointers: one to a callback function, and the
     other to be given as an argument to the callback function; this is
     equivalent to using both `STARPU_CALLBACK' and
     `STARPU_CALLBACK_WITH_ARG'

 -- Macro: STARPU_PRIORITY
     this macro is used when calling `starpu_insert_task', and must be
     followed by a integer defining a priority level

 -- Function: void starpu_codelet_pack_args (char **ARG_BUFFER, size_t
          *ARG_BUFFER_SIZE, ...)
     Pack arguments of type `STARPU_VALUE' into a buffer which can be
     given to a codelet and later unpacked with the function
     `starpu_codelet_unpack_args' defined below.

 -- Function: void starpu_codelet_unpack_args (void *CL_ARG, ...)
     Retrieve the arguments of type `STARPU_VALUE' associated to a task
     automatically created using the function `starpu_insert_task'
     defined above.

   Here the implementation of the codelet:

     void func_cpu(void *descr[], void *_args)
     {
             int *x0 = (int *)STARPU_VARIABLE_GET_PTR(descr[0]);
             float *x1 = (float *)STARPU_VARIABLE_GET_PTR(descr[1]);
             int ifactor;
             float ffactor;

             starpu_codelet_unpack_args(_args, &ifactor, &ffactor);
             *x0 = *x0 * ifactor;
             *x1 = *x1 * ffactor;
     }

     struct starpu_codelet mycodelet = {
             .where = STARPU_CPU,
             .cpu_funcs = { func_cpu, NULL },
             .nbuffers = 2,
             .modes = { STARPU_RW, STARPU_RW }
     };

   And the call to the `starpu_insert_task' wrapper:

     starpu_insert_task(&mycodelet,
                        STARPU_VALUE, &ifactor, sizeof(ifactor),
                        STARPU_VALUE, &ffactor, sizeof(ffactor),
                        STARPU_RW, data_handles[0], STARPU_RW, data_handles[1],
                        0);

   The call to `starpu_insert_task' is equivalent to the following code:

     struct starpu_task *task = starpu_task_create();
     task->cl = &mycodelet;
     task->handles[0] = data_handles[0];
     task->handles[1] = data_handles[1];
     char *arg_buffer;
     size_t arg_buffer_size;
     starpu_codelet_pack_args(&arg_buffer, &arg_buffer_size,
                         STARPU_VALUE, &ifactor, sizeof(ifactor),
                         STARPU_VALUE, &ffactor, sizeof(ffactor),
                         0);
     task->cl_arg = arg_buffer;
     task->cl_arg_size = arg_buffer_size;
     int ret = starpu_task_submit(task);

   If some part of the task insertion depends on the value of some
computation, the `STARPU_DATA_ACQUIRE_CB' macro can be very convenient.
For instance, assuming that the index variable `i' was registered as
handle `i_handle':

     /* Compute which portion we will work on, e.g. pivot */
     starpu_insert_task(&which_index, STARPU_W, i_handle, 0);

     /* And submit the corresponding task */
     STARPU_DATA_ACQUIRE_CB(i_handle, STARPU_R, starpu_insert_task(&work, STARPU_RW, A_handle[i], 0));

   The `STARPU_DATA_ACQUIRE_CB' macro submits an asynchronous request
for acquiring data `i' for the main application, and will execute the
code given as third parameter when it is acquired. In other words, as
soon as the value of `i' computed by the `which_index' codelet can be
read, the portion of code passed as third parameter of
`STARPU_DATA_ACQUIRE_CB' will be executed, and is allowed to read from
`i' to use it e.g. as an index. Note that this macro is only avaible
when compiling StarPU with the compiler `gcc'.


File: starpu.info,  Node: Data reduction,  Next: Parallel Tasks,  Prev: Insert Task Utility,  Up: Advanced Examples

5.8 Data reduction
==================

In various cases, some piece of data is used to accumulate intermediate
results. For instances, the dot product of a vector, maximum/minimum
finding, the histogram of a photograph, etc. When these results are
produced along the whole machine, it would not be efficient to
accumulate them in only one place, incurring data transmission each and
access concurrency.

   StarPU provides a `STARPU_REDUX' mode, which permits to optimize
that case: it will allocate a buffer on each memory node, and accumulate
intermediate results there. When the data is eventually accessed in the
normal `STARPU_R' mode, StarPU will collect the intermediate results in
just one buffer.

   For this to work, the user has to use the
`starpu_data_set_reduction_methods' to declare how to initialize these
buffers, and how to assemble partial results.

   For instance, `cg' uses that to optimize its dot product: it first
defines the codelets for initialization and reduction:

     struct starpu_codelet bzero_variable_cl =
     {
             .cpu_funcs = { bzero_variable_cpu, NULL },
             .cuda_funcs = { bzero_variable_cuda, NULL },
             .nbuffers = 1,
     }

     static void accumulate_variable_cpu(void *descr[], void *cl_arg)
     {
             double *v_dst = (double *)STARPU_VARIABLE_GET_PTR(descr[0]);
             double *v_src = (double *)STARPU_VARIABLE_GET_PTR(descr[1]);
             *v_dst = *v_dst + *v_src;
     }

     static void accumulate_variable_cuda(void *descr[], void *cl_arg)
     {
             double *v_dst = (double *)STARPU_VARIABLE_GET_PTR(descr[0]);
             double *v_src = (double *)STARPU_VARIABLE_GET_PTR(descr[1]);
             cublasaxpy(1, (double)1.0, v_src, 1, v_dst, 1);
             cudaStreamSynchronize(starpu_cuda_get_local_stream());
     }

     struct starpu_codelet accumulate_variable_cl =
     {
             .cpu_funcs = { accumulate_variable_cpu, NULL },
             .cuda_funcs = { accumulate_variable_cuda, NULL },
             .nbuffers = 1,
     }

   and attaches them as reduction methods for its dtq handle:

     starpu_data_set_reduction_methods(dtq_handle,
             &accumulate_variable_cl, &bzero_variable_cl);

   and dtq_handle can now be used in `STARPU_REDUX' mode for the dot
products with partitioned vectors:

     int dots(starpu_data_handle v1, starpu_data_handle v2,
              starpu_data_handle s, unsigned nblocks)
     {
         starpu_insert_task(&bzero_variable_cl, STARPU_W, s, 0);
         for (b = 0; b < nblocks; b++)
             starpu_insert_task(&dot_kernel_cl,
                 STARPU_RW, s,
                 STARPU_R, starpu_data_get_sub_data(v1, 1, b),
                 STARPU_R, starpu_data_get_sub_data(v2, 1, b),
                 0);
     }

   The `cg' example also uses reduction for the blocked gemv kernel,
leading to yet more relaxed dependencies and more parallelism.


File: starpu.info,  Node: Parallel Tasks,  Next: Debugging,  Prev: Data reduction,  Up: Advanced Examples

5.9 Parallel Tasks
==================

StarPU can leverage existing parallel computation libraries by the
means of parallel tasks. A parallel task is a task which gets worked on
by a set of CPUs (called a parallel or combined worker) at the same
time, by using an existing parallel CPU implementation of the
computation to be achieved. This can also be useful to improve the load
balance between slow CPUs and fast GPUs: since CPUs work collectively
on a single task, the completion time of tasks on CPUs become
comparable to the completion time on GPUs, thus relieving from
granularity discrepancy concerns. Hwloc support needs to be enabled to
get good performance, otherwise StarPU will not know how to better
group cores.

   Two modes of execution exist to accomodate with existing usages.

5.9.1 Fork-mode parallel tasks
------------------------------

In the Fork mode, StarPU will call the codelet function on one of the
CPUs of the combined worker. The codelet function can use
`starpu_combined_worker_get_size()' to get the number of threads it is
allowed to start to achieve the computation. The CPU binding mask is
already enforced, so that threads created by the function will inherit
the mask, and thus execute where StarPU expected. For instance, using
OpenMP (full source is available in `examples/openmp/vector_scal.c'):

     void scal_cpu_func(void *buffers[], void *_args)
     {
         unsigned i;
         float *factor = _args;
         struct starpu_vector_interface *vector = buffers[0];
         unsigned n = STARPU_VECTOR_GET_NX(vector);
         float *val = (float *)STARPU_VECTOR_GET_PTR(vector);

     #pragma omp parallel for num_threads(starpu_combined_worker_get_size())
         for (i = 0; i < n; i++)
             val[i] *= *factor;
     }

     static struct starpu_codelet cl =
     {
         .modes = { STARPU_RW },
         .where = STARPU_CPU,
         .type = STARPU_FORKJOIN,
         .max_parallelism = INT_MAX,
         .cpu_funcs = {scal_cpu_func, NULL},
         .nbuffers = 1,
     };

   Other examples include for instance calling a BLAS parallel CPU
implementation (see `examples/mult/xgemm.c').

5.9.2 SPMD-mode parallel tasks
------------------------------

In the SPMD mode, StarPU will call the codelet function on each CPU of
the combined worker. The codelet function can use
`starpu_combined_worker_get_size()' to get the total number of CPUs
involved in the combined worker, and thus the number of calls that are
made in parallel to the function, and
`starpu_combined_worker_get_rank()' to get the rank of the current CPU
within the combined worker. For instance:

     static void func(void *buffers[], void *args)
     {
         unsigned i;
         float *factor = _args;
         struct starpu_vector_interface *vector = buffers[0];
         unsigned n = STARPU_VECTOR_GET_NX(vector);
         float *val = (float *)STARPU_VECTOR_GET_PTR(vector);

         /* Compute slice to compute */
         unsigned m = starpu_combined_worker_get_size();
         unsigned j = starpu_combined_worker_get_rank();
         unsigned slice = (n+m-1)/m;

         for (i = j * slice; i < (j+1) * slice && i < n; i++)
             val[i] *= *factor;
     }

     static struct starpu_codelet cl =
     {
         .modes = { STARPU_RW },
         .where = STARP_CPU,
         .type = STARPU_SPMD,
         .max_parallelism = INT_MAX,
         .cpu_funcs = { func, NULL },
         .nbuffers = 1,
     }

   Of course, this trivial example will not really benefit from
parallel task execution, and was only meant to be simple to understand.
The benefit comes when the computation to be done is so that threads
have to e.g. exchange intermediate results, or write to the data in a
complex but safe way in the same buffer.

5.9.3 Parallel tasks performance
--------------------------------

To benefit from parallel tasks, a parallel-task-aware StarPU scheduler
has to be used. When exposed to codelets with a Fork or SPMD flag, the
`pheft' (parallel-heft) and `pgreedy' (parallel greedy) schedulers will
indeed also try to execute tasks with several CPUs. It will
automatically try the various available combined worker sizes and thus
be able to avoid choosing a large combined worker if the codelet does
not actually scale so much.

5.9.4 Combined worker sizes
---------------------------

By default, StarPU creates combined workers according to the
architecture structure as detected by hwloc. It means that for each
object of the hwloc topology (NUMA node, socket, cache, ...) a combined
worker will be created. If some nodes of the hierarchy have a big arity
(e.g. many cores in a socket without a hierarchy of shared caches),
StarPU will create combined workers of intermediate sizes.

5.9.5 Concurrent parallel tasks
-------------------------------

Unfortunately, many environments and librairies do not support
concurrent calls.

   For instance, most OpenMP implementations (including the main ones)
do not support concurrent `pragma omp parallel' statements without
nesting them in another `pragma omp parallel' statement, but StarPU
does not yet support creating its CPU workers by using such pragma.

   Other parallel libraries are also not safe when being invoked
concurrently from different threads, due to the use of global variables
in their sequential sections for instance.

   The solution is then to use only one combined worker at a time.
This can be done by setting `single_combined_worker' to 1 in the
`starpu_conf' structure, or setting the `STARPU_SINGLE_COMBINED_WORKER'
environment variable to 1. StarPU will then run only one parallel task
at a time.


File: starpu.info,  Node: Debugging,  Next: The multiformat interface,  Prev: Parallel Tasks,  Up: Advanced Examples

5.10 Debugging
==============

StarPU provides several tools to help debugging aplications. Execution
traces can be generated and displayed graphically, see *note Generating
traces::. Some gdb helpers are also provided to show the whole StarPU
state:

     (gdb) source tools/gdbinit
     (gdb) help starpu


File: starpu.info,  Node: The multiformat interface,  Next: On-GPU rendering,  Prev: Debugging,  Up: Advanced Examples

5.11 The multiformat interface
==============================

It may be interesting to represent the same piece of data using two
different data structures: one that would only be used on CPUs, and one
that would only be used on GPUs. This can be done by using the
multiformat interface. StarPU will be able to convert data from one
data structure to the other when needed.  Note that the heft scheduler
is the only one optimized for this interface. The user must provide
StarPU with conversion codelets:

     #define NX 1024
     struct point array_of_structs[NX];
     starpu_data_handle_t handle;

     /*
      * The conversion of a piece of data is itself a task, though it is created,
      * submitted and destroyed by StarPU internals and not by the user. Therefore,
      * we have to define two codelets.
      * Note that for now the conversion from the CPU format to the GPU format has to
      * be executed on the GPU, and the conversion from the GPU to the CPU has to be
      * executed on the CPU.
      */
     #ifdef STARPU_USE_OPENCL
     void cpu_to_opencl_opencl_func(void *buffers[], void *args);
     struct starpu_codelet cpu_to_opencl_cl = {
         .where = STARPU_OPENCL,
         .opencl_funcs = { cpu_to_opencl_opencl_func, NULL },
         .nbuffers = 1,
         .modes = { STARPU_RW }
     };

     void opencl_to_cpu_func(void *buffers[], void *args);
     struct starpu_codelet opencl_to_cpu_cl = {
         .where = STARPU_CPU,
         .cpu_funcs = { opencl_to_cpu_func, NULL },
         .nbuffers = 1,
         .modes = { STARPU_RW }
     };
     #endif

     struct starpu_multiformat_data_interface_ops format_ops = {
     #ifdef STARPU_USE_OPENCL
         .opencl_elemsize = 2 * sizeof(float),
         .cpu_to_opencl_cl = &cpu_to_opencl_cl,
         .opencl_to_cpu_cl = &opencl_to_cpu_cl,
     #endif
         .cpu_elemsize = 2 * sizeof(float),
         ...
     };
     starpu_multiformat_data_register(handle, 0, &array_of_structs, NX, &format_ops);

   Kernels can be written almost as for any other interface. Note that
STARPU_MULTIFORMAT_GET_CPU_PTR shall only be used for CPU kernels. CUDA
kernels must use STARPU_MULTIFORMAT_GET_CUDA_PTR, and OpenCL kernels
must use STARPU_MULTIFORMAT_GET_OPENCL_PTR. STARPU_MULTIFORMAT_GET_NX
may be used in any kind of kernel.
     static void
     multiformat_scal_cpu_func(void *buffers[], void *args)
     {
         struct point *aos;
         unsigned int n;

         aos = STARPU_MULTIFORMAT_GET_CPU_PTR(buffers[0]);
         n = STARPU_MULTIFORMAT_GET_NX(buffers[0]);
         ...
     }

     extern "C" void multiformat_scal_cuda_func(void *buffers[], void *_args)
     {
         unsigned int n;
         struct struct_of_arrays *soa;

         soa = (struct struct_of_arrays *) STARPU_MULTIFORMAT_GET_CUDA_PTR(buffers[0]);
         n = STARPU_MULTIFORMAT_GET_NX(buffers[0]);

         ...
     }

   A full example may be found in
`examples/basic_examples/multiformat.c'.


File: starpu.info,  Node: On-GPU rendering,  Next: More examples,  Prev: The multiformat interface,  Up: Advanced Examples

5.12 On-GPU rendering
=====================

Graphical-oriented applications need to draw the result of their
computations, typically on the very GPU where these happened.
Technologies such as OpenGL/CUDA interoperability permit to let CUDA
directly work on the OpenGL buffers, making them thus immediately ready
for drawing, by mapping OpenGL buffer, textures or renderbuffer objects
into CUDA. To achieve this with StarPU, it simply needs to be given the
CUDA pointer at registration, for instance:

     for (workerid = 0; workerid < starpu_worker_get_count(); workerid++)
             if (starpu_worker_get_type(workerid) == STARPU_CUDA_WORKER)
                     break;

     cudaSetDevice(starpu_worker_get_devid(workerid));
     cudaGraphicsResourceGetMappedPointer((void**)&output, &num_bytes, resource);
     starpu_vector_data_register(&handle, starpu_worker_get_memory_node(workerid), output, num_bytes / sizeof(float4), sizeof(float4));

     starpu_insert_task(&cl, STARPU_RW, handle, 0);

     starpu_data_unregister(handle);

     cudaSetDevice(starpu_worker_get_devid(workerid));
     cudaGraphicsUnmapResources(1, &resource, 0);

     /* Now display it */


File: starpu.info,  Node: More examples,  Prev: On-GPU rendering,  Up: Advanced Examples

5.13 More examples
==================

More examples are available in the StarPU sources in the `examples/'
directory. Simple examples include:

`incrementer/':
     Trivial incrementation test.

`basic_examples/':
     Simple documented Hello world (as shown in *note Hello World::),
     vector/scalar product (as shown         in *note Vector Scaling on
     an Hybrid CPU/GPU Machine::), matrix         product examples (as
     shown in *note Performance model example::), an example using the
     blocked matrix data         interface, an example using the
     variable data interface, and an example         using different
     formats on CPUs and GPUs.

`matvecmult/':
     OpenCL example from NVidia, adapted to StarPU.

`axpy/':
     AXPY CUBLAS operation adapted to StarPU.

`fortran/':
     Example of Fortran bindings.

   More advanced examples include:

`filters/':
     Examples using filters, as shown in *note Partitioning Data::.

`lu/':
     LU matrix factorization, see for instance `xlu_implicit.c'

`cholesky/':
     Cholesky matrix factorization, see for instance
     `cholesky_implicit.c'.


File: starpu.info,  Node: Performance optimization,  Next: Performance feedback,  Prev: Advanced Examples,  Up: Top

6 How to optimize performance with StarPU
*****************************************

TODO: improve!

* Menu:

* Data management::
* Task granularity::
* Task submission::
* Task priorities::
* Task scheduling policy::
* Performance model calibration::
* Task distribution vs Data transfer::
* Data prefetch::
* Power-based scheduling::
* Profiling::
* CUDA-specific optimizations::
* Performance debugging::

   Simply encapsulating application kernels into tasks already permits
to seamlessly support CPU and GPUs at the same time. To achieve good
performance, a few additional changes are needed.


File: starpu.info,  Node: Data management,  Next: Task granularity,  Up: Performance optimization

6.1 Data management
===================

When the application allocates data, whenever possible it should use the
`starpu_malloc' function, which will ask CUDA or OpenCL to make the
allocation itself and pin the corresponding allocated memory. This is
needed to permit asynchronous data transfer, i.e. permit data transfer
to overlap with computations. Otherwise, the trace will show that the
`DriverCopyAsync' state takes a lot of time, this is because CUDA or
OpenCL then reverts to synchronous transfers.

   By default, StarPU leaves replicates of data wherever they were
used, in case they will be re-used by other tasks, thus saving the data
transfer time. When some task modifies some data, all the other
replicates are invalidated, and only the processing unit which ran that
task will have a valid replicate of the data. If the application knows
that this data will not be re-used by further tasks, it should advise
StarPU to immediately replicate it to a desired list of memory nodes
(given through a bitmask). This can be understood like the
write-through mode of CPU caches.

     starpu_data_set_wt_mask(img_handle, 1<<0);

   will for instance request to always automatically transfer a
replicate into the main memory (node 0), as bit 0 of the write-through
bitmask is being set.

     starpu_data_set_wt_mask(img_handle, ~0U);

   will request to always automatically broadcast the updated data to
all memory nodes.

   Setting the write-through mask to `~0U' can also be useful to make
sure all memory nodes always have a copy of the data, so that it is
never evicted when memory gets scarse.

   Implicit data dependency computation can become expensive if a lot
of tasks access the same piece of data. If no dependency is required on
some piece of data (e.g. because it is only accessed in read-only mode,
or because write accesses are actually commutative), use the
`starpu_data_set_sequential_consistency_flag' function to disable
implicit dependencies on that data.

   In the same vein, accumulation of results in the same data can
become a bottleneck. The use of the `STARPU_REDUX' mode permits to
optimize such accumulation (*note Data reduction::).


File: starpu.info,  Node: Task granularity,  Next: Task submission,  Prev: Data management,  Up: Performance optimization

6.2 Task granularity
====================

Like any other runtime, StarPU has some overhead to manage tasks. Since
it does smart scheduling and data management, that overhead is not
always neglectable. The order of magnitude of the overhead is typically
a couple of microseconds. The amount of work that a task should do
should thus be somewhat bigger, to make sure that the overhead becomes
neglectible. The offline performance feedback can provide a measure of
task length, which should thus be checked if bad performance are
observed.


File: starpu.info,  Node: Task submission,  Next: Task priorities,  Prev: Task granularity,  Up: Performance optimization

6.3 Task submission
===================

To let StarPU make online optimizations, tasks should be submitted
asynchronously as much as possible. Ideally, all the tasks should be
submitted, and mere calls to `starpu_task_wait_for_all' or
`starpu_data_unregister' be done to wait for termination. StarPU will
then be able to rework the whole schedule, overlap computation with
communication, manage accelerator local memory usage, etc.


File: starpu.info,  Node: Task priorities,  Next: Task scheduling policy,  Prev: Task submission,  Up: Performance optimization

6.4 Task priorities
===================

By default, StarPU will consider the tasks in the order they are
submitted by the application. If the application programmer knows that
some tasks should be performed in priority (for instance because their
output is needed by many other tasks and may thus be a bottleneck if
not executed early enough), the `priority' field of the task structure
should be set to transmit the priority information to StarPU.


File: starpu.info,  Node: Task scheduling policy,  Next: Performance model calibration,  Prev: Task priorities,  Up: Performance optimization

6.5 Task scheduling policy
==========================

By default, StarPU uses the `eager' simple greedy scheduler. This is
because it provides correct load balance even if the application
codelets do not have performance models. If your application codelets
have performance models (*note Performance model example:: for examples
showing how to do it), you should change the scheduler thanks to the
`STARPU_SCHED' environment variable. For instance `export
STARPU_SCHED=dmda' . Use `help' to get the list of available schedulers.

   The eager scheduler uses a central task queue, from which workers
draw tasks to work on. This however does not permit to prefetch data
since the scheduling decision is taken late. If a task has a non-0
priority, it is put at the front of the queue.

   The prio scheduler also uses a central task queue, but sorts tasks by
priority (between -5 and 5).

   The random scheduler distributes tasks randomly according to assumed
worker overall performance.

   The ws (work stealing) scheduler schedules tasks on the local worker
by default. When a worker becomes idle, it steals a task from the most
loaded worker.

   The dm (deque model) scheduler uses task execution performance
models into account to perform an HEFT-similar scheduling strategy: it
schedules tasks where their termination time will be minimal.

   The dmda (deque model data aware) scheduler is similar to dm, it
also takes into account data transfer time.

   The dmdar (deque model data aware ready) scheduler is similar to
dmda, it also sorts tasks on per-worker queues by number of
already-available data buffers.

   The dmdas (deque model data aware sorted) scheduler is similar to
dmda, it also supports arbitrary priority values.

   The heft (heterogeneous earliest finish time) scheduler is similar
to dmda, it also supports task bundles.

   The pheft (parallel HEFT) scheduler is similar to heft, it also
supports parallel tasks (still experimental).

   The pgreedy (parallel greedy) scheduler is similar to greedy, it also
supports parallel tasks (still experimental).


File: starpu.info,  Node: Performance model calibration,  Next: Task distribution vs Data transfer,  Prev: Task scheduling policy,  Up: Performance optimization

6.6 Performance model calibration
=================================

Most schedulers are based on an estimation of codelet duration on each
kind of processing unit. For this to be possible, the application
programmer needs to configure a performance model for the codelets of
the application (see *note Performance model example:: for instance).
History-based performance models use on-line calibration.  StarPU will
automatically calibrate codelets which have never been calibrated yet,
and save the result in `~/.starpu/sampling/codelets'.  The models are
indexed by machine name. To share the models between machines (e.g. for
a homogeneous cluster), use `export STARPU_HOSTNAME=some_global_name'.
To force continuing calibration, use `export STARPU_CALIBRATE=1' . This
may be necessary if your application has not-so-stable performance.
StarPU will force calibration (and thus ignore the current result)
until 10 (_STARPU_CALIBRATION_MINIMUM) measurements have been made on
each architecture, to avoid badly scheduling tasks just because the
first measurements were not so good. Details on the current performance
model status can be obtained from the `starpu_perfmodel_display'
command: the `-l' option lists the available performance models, and
the `-s' option permits to choose the performance model to be
displayed. The result looks like:

     $ starpu_perfmodel_display -s starpu_dlu_lu_model_22
     performance model for cpu
     # hash    size     mean          dev           n
     880805ba  98304    2.731309e+02  6.010210e+01  1240
     b50b6605  393216   1.469926e+03  1.088828e+02  1240
     5c6c3401  1572864  1.125983e+04  3.265296e+03  1240

   Which shows that for the LU 22 kernel with a 1.5MiB matrix, the
average execution time on CPUs was about 11ms, with a 3ms standard
deviation, over 1240 samples. It is a good idea to check this before
doing actual performance measurements.

   A graph can be drawn by using the `starpu_perfmodel_plot':

     $ starpu_perfmodel_plot -s starpu_dlu_lu_model_22
     98304 393216 1572864
     $ gnuplot starpu_starpu_dlu_lu_model_22.gp
     $ gv starpu_starpu_dlu_lu_model_22.eps

   If a kernel source code was modified (e.g. performance improvement),
the calibration information is stale and should be dropped, to
re-calibrate from start. This can be done by using `export
STARPU_CALIBRATE=2'.

   Note: due to CUDA limitations, to be able to measure kernel duration,
calibration mode needs to disable asynchronous data transfers.
Calibration thus disables data transfer / computation overlapping, and
should thus not be used for eventual benchmarks. Note 2: history-based
performance models get calibrated only if a performance-model-based
scheduler is chosen.


File: starpu.info,  Node: Task distribution vs Data transfer,  Next: Data prefetch,  Prev: Performance model calibration,  Up: Performance optimization

6.7 Task distribution vs Data transfer
======================================

Distributing tasks to balance the load induces data transfer penalty.
StarPU thus needs to find a balance between both. The target function
that the `dmda' scheduler of StarPU tries to minimize is `alpha *
T_execution + beta * T_data_transfer', where `T_execution' is the
estimated execution time of the codelet (usually accurate), and
`T_data_transfer' is the estimated data transfer time. The latter is
estimated based on bus calibration before execution start, i.e. with an
idle machine, thus without contention. You can force bus re-calibration
by running `starpu_calibrate_bus'. The beta parameter defaults to 1,
but it can be worth trying to tweak it by using `export
STARPU_SCHED_BETA=2' for instance, since during real application
execution, contention makes transfer times bigger.  This is of course
imprecise, but in practice, a rough estimation already gives the good
results that a precise estimation would give.


File: starpu.info,  Node: Data prefetch,  Next: Power-based scheduling,  Prev: Task distribution vs Data transfer,  Up: Performance optimization

6.8 Data prefetch
=================

The `heft', `dmda' and `pheft' scheduling policies perform data
prefetch (see *note STARPU_PREFETCH::): as soon as a scheduling
decision is taken for a task, requests are issued to transfer its
required data to the target processing unit, if needeed, so that when
the processing unit actually starts the task, its data will hopefully be
already available and it will not have to wait for the transfer to
finish.

   The application may want to perform some manual prefetching, for
several reasons such as excluding initial data transfers from
performance measurements, or setting up an initial statically-computed
data distribution on the machine before submitting tasks, which will
thus guide StarPU toward an initial task distribution (since StarPU
will try to avoid further transfers).

   This can be achieved by giving the `starpu_data_prefetch_on_node'
function the handle and the desired target memory node.


File: starpu.info,  Node: Power-based scheduling,  Next: Profiling,  Prev: Data prefetch,  Up: Performance optimization

6.9 Power-based scheduling
==========================

If the application can provide some power performance model (through
the `power_model' field of the codelet structure), StarPU will take it
into account when distributing tasks. The target function that the
`dmda' scheduler minimizes becomes `alpha * T_execution + beta *
T_data_transfer + gamma * Consumption' , where `Consumption' is the
estimated task consumption in Joules. To tune this parameter, use
`export STARPU_SCHED_GAMMA=3000' for instance, to express that each
Joule (i.e kW during 1000us) is worth 3000us execution time penalty.
Setting `alpha' and `beta' to zero permits to only take into account
power consumption.

   This is however not sufficient to correctly optimize power: the
scheduler would simply tend to run all computations on the most
energy-conservative processing unit. To account for the consumption of
the whole machine (including idle processing units), the idle power of
the machine should be given by setting `export STARPU_IDLE_POWER=200'
for 200W, for instance. This value can often be obtained from the
machine power supplier.

   The power actually consumed by the total execution can be displayed
by setting `export STARPU_PROFILING=1 STARPU_WORKER_STATS=1' .


File: starpu.info,  Node: Profiling,  Next: CUDA-specific optimizations,  Prev: Power-based scheduling,  Up: Performance optimization

6.10 Profiling
==============

A quick view of how many tasks each worker has executed can be obtained
by setting `export STARPU_WORKER_STATS=1' This is a convenient way to
check that execution did happen on accelerators without penalizing
performance with the profiling overhead.

   A quick view of how much data transfers have been issued can be
obtained by setting `export STARPU_BUS_STATS=1' .

   More detailed profiling information can be enabled by using `export
STARPU_PROFILING=1' or by calling `starpu_profiling_status_set' from
the source code.  Statistics on the execution can then be obtained by
using `export STARPU_BUS_STATS=1' and `export STARPU_WORKER_STATS=1' .
More details on performance feedback are provided by the next chapter.


File: starpu.info,  Node: CUDA-specific optimizations,  Next: Performance debugging,  Prev: Profiling,  Up: Performance optimization

6.11 CUDA-specific optimizations
================================

Due to CUDA limitations, StarPU will have a hard time overlapping its
own communications and the codelet computations if the application does
not use a dedicated CUDA stream for its computations. StarPU provides
one by the use of `starpu_cuda_get_local_stream()' which should be used
by all CUDA codelet operations. For instance:

     func <<<grid,block,0,starpu_cuda_get_local_stream()>>> (foo, bar);
     cudaStreamSynchronize(starpu_cuda_get_local_stream());

   StarPU already does appropriate calls for the CUBLAS library.

   Unfortunately, some CUDA libraries do not have stream variants of
kernels. That will lower the potential for overlapping.


File: starpu.info,  Node: Performance debugging,  Prev: CUDA-specific optimizations,  Up: Performance optimization

6.12 Performance debugging
==========================

To get an idea of what is happening, a lot of performance feedback is
available, detailed in the next chapter. The various informations
should be checked for.

   * What does the Gantt diagram look like? (see *note Gantt diagram::)
        * If it's mostly green (running tasks), then the machine is
          properly   utilized, and perhaps the codelets are just slow.
          Check their performance, see   *note Codelet performance::.

        * If it's mostly purple (FetchingInput), tasks keep waiting for
          data   transfers, do you perhaps have far more communication
          than computation? Did   you properly use CUDA streams to make
          sure communication can be   overlapped? Did you use
          data-locality aware schedulers to avoid transfers as   much
          as possible?

        * If it's mostly red (Blocked), tasks keep waiting for
          dependencies,   do you have enough parallelism? It might be a
          good idea to check what the DAG   looks like (see *note
          DAG::).

        * If only some workers are completely red (Blocked), for some
          reason the   scheduler didn't assign tasks to them. Perhaps
          the performance model is bogus,   check it (see *note Codelet
          performance::). Do all your codelets have a   performance
          model?  When some of them don't, the schedulers switches to a
           greedy algorithm which thus performs badly.


File: starpu.info,  Node: Performance feedback,  Next: Tips and Tricks,  Prev: Performance optimization,  Up: Top

7 Performance feedback
**********************

* Menu:

* On-line::                     On-line performance feedback
* Off-line::                    Off-line performance feedback
* Codelet performance::         Performance of codelets
* Theoretical lower bound on execution time API::


File: starpu.info,  Node: On-line,  Next: Off-line,  Up: Performance feedback

7.1 On-line performance feedback
================================

* Menu:

* Enabling monitoring::         Enabling on-line performance monitoring
* Task feedback::               Per-task feedback
* Codelet feedback::            Per-codelet feedback
* Worker feedback::             Per-worker feedback
* Bus feedback::                Bus-related feedback
* StarPU-Top::                  StarPU-Top interface


File: starpu.info,  Node: Enabling monitoring,  Next: Task feedback,  Up: On-line

7.1.1 Enabling on-line performance monitoring
---------------------------------------------

In order to enable online performance monitoring, the application can
call `starpu_profiling_status_set(STARPU_PROFILING_ENABLE)'. It is
possible to detect whether monitoring is already enabled or not by
calling `starpu_profiling_status_get()'. Enabling monitoring also
reinitialize all previously collected feedback. The `STARPU_PROFILING'
environment variable can also be set to 1 to achieve the same effect.

   Likewise, performance monitoring is stopped by calling
`starpu_profiling_status_set(STARPU_PROFILING_DISABLE)'. Note that this
does not reset the performance counters so that the application may
consult them later on.

   More details about the performance monitoring API are available in
section *note Profiling API::.


File: starpu.info,  Node: Task feedback,  Next: Codelet feedback,  Prev: Enabling monitoring,  Up: On-line

7.1.2 Per-task feedback
-----------------------

If profiling is enabled, a pointer to a `starpu_task_profiling_info'
structure is put in the `.profiling_info' field of the `starpu_task'
structure when a task terminates.  This structure is automatically
destroyed when the task structure is destroyed, either automatically or
by calling `starpu_task_destroy'.

   The `starpu_task_profiling_info' structure indicates the date when
the task was submitted (`submit_time'), started (`start_time'), and
terminated (`end_time'), relative to the initialization of StarPU with
`starpu_init'. It also specifies the identifier of the worker that has
executed the task (`workerid').  These date are stored as `timespec'
structures which the user may convert into micro-seconds using the
`starpu_timing_timespec_to_us' helper function.

   It it worth noting that the application may directly access this
structure from the callback executed at the end of the task. The
`starpu_task' structure associated to the callback currently being
executed is indeed accessible with the `starpu_get_current_task()'
function.


File: starpu.info,  Node: Codelet feedback,  Next: Worker feedback,  Prev: Task feedback,  Up: On-line

7.1.3 Per-codelet feedback
--------------------------

The `per_worker_stats' field of the `struct starpu_codelet' structure is
an array of counters. The i-th entry of the array is incremented every
time a task implementing the codelet is executed on the i-th worker.
This array is not reinitialized when profiling is enabled or disabled.


File: starpu.info,  Node: Worker feedback,  Next: Bus feedback,  Prev: Codelet feedback,  Up: On-line

7.1.4 Per-worker feedback
-------------------------

The second argument returned by the `starpu_worker_get_profiling_info'
function is a `starpu_worker_profiling_info' structure that gives
statistics about the specified worker. This structure specifies when
StarPU started collecting profiling information for that worker
(`start_time'), the duration of the profiling measurement interval
(`total_time'), the time spent executing kernels (`executing_time'),
the time spent sleeping because there is no task to execute at all
(`sleeping_time'), and the number of tasks that were executed while
profiling was enabled.  These values give an estimation of the
proportion of time spent do real work, and the time spent either
sleeping because there are not enough executable tasks or simply wasted
in pure StarPU overhead.

   Calling `starpu_worker_get_profiling_info' resets the profiling
information associated to a worker.

   When an FxT trace is generated (see *note Generating traces::), it
is also possible to use the `starpu_workers_activity' script (described
in *note starpu-workers-activity::) to generate a graphic showing the
evolution of these values during the time, for the different workers.


File: starpu.info,  Node: Bus feedback,  Next: StarPU-Top,  Prev: Worker feedback,  Up: On-line

7.1.5 Bus-related feedback
--------------------------

TODO: ajouter STARPU_BUS_STATS

   The bus speed measured by StarPU can be displayed by using the
`starpu_machine_display' tool, for instance:

     StarPU has found:
             3 CUDA devices
                     CUDA 0 (Tesla C2050 02:00.0)
                     CUDA 1 (Tesla C2050 03:00.0)
                     CUDA 2 (Tesla C2050 84:00.0)
     from    to RAM          to CUDA 0       to CUDA 1       to CUDA 2
     RAM     0.000000        5176.530428     5176.492994     5191.710722
     CUDA 0  4523.732446     0.000000        2414.074751     2417.379201
     CUDA 1  4523.718152     2414.078822     0.000000        2417.375119
     CUDA 2  4534.229519     2417.069025     2417.060863     0.000000


File: starpu.info,  Node: StarPU-Top,  Prev: Bus feedback,  Up: On-line

7.1.6 StarPU-Top interface
--------------------------

StarPU-Top is an interface which remotely displays the on-line state of
a StarPU application and permits the user to change parameters on the
fly.

   Variables to be monitored can be registered by calling the
`starpu_top_add_data_boolean', `starpu_top_add_data_integer',
`starpu_top_add_data_float' functions, e.g.:

     starpu_top_data *data = starpu_top_add_data_integer("mynum", 0, 100, 1);

   The application should then call `starpu_top_init_and_wait' to give
its name and wait for StarPU-Top to get a start request from the user.
The name is used by StarPU-Top to quickly reload a previously-saved
layout of parameter display.

     starpu_top_init_and_wait("the application");

   The new values can then be provided thanks to
`starpu_top_update_data_boolean', `starpu_top_update_data_integer',
`starpu_top_update_data_float', e.g.:

     starpu_top_update_data_integer(data, mynum);

   Updateable parameters can be registered thanks to
`starpu_top_register_parameter_boolean',
`starpu_top_register_parameter_integer',
`starpu_top_register_parameter_float', e.g.:

     float alpha;
     starpu_top_register_parameter_float("alpha", &alpha, 0, 10, modif_hook);

   `modif_hook' is a function which will be called when the parameter
is being modified, it can for instance print the new value:

     void modif_hook(struct starpu_top_param *d) {
         fprintf(stderr,"%s has been modified: %f\n", d->name, alpha);
     }

   Task schedulers should notify StarPU-Top when it has decided when a
task will be scheduled, so that it can show it in its Gantt chart, for
instance:

     starpu_top_task_prevision(task, workerid, begin, end);

   Starting StarPU-Top(1) and the application can be done two ways:

   * The application is started by hand on some machine (and thus
     already waiting for the start event). In the Preference dialog of
     StarPU-Top, the SSH checkbox should be unchecked, and the hostname
     and port (default is 2011) on which the application is already
     running should be specified. Clicking on the connection button
     will thus connect to the already-running application.

   * StarPU-Top is started first, and clicking on the connection button
     will start the application itself (possibly on a remote machine).
     The SSH checkbox should be checked, and a command line provided,
     e.g.:

          ssh myserver STARPU_SCHED=heft ./application

     If port 2011 of the remote machine can not be accessed directly,
     an ssh port bridge should be added:

          ssh -L 2011:localhost:2011 myserver STARPU_SCHED=heft ./application

     and "localhost" should be used as IP Address to connect to.

   ---------- Footnotes ----------

   (1) StarPU-Top is started via the binary `starpu_top'.


File: starpu.info,  Node: Off-line,  Next: Codelet performance,  Prev: On-line,  Up: Performance feedback

7.2 Off-line performance feedback
=================================

* Menu:

* Generating traces::           Generating traces with FxT
* Gantt diagram::               Creating a Gantt Diagram
* DAG::                         Creating a DAG with graphviz
* starpu-workers-activity::     Monitoring activity


File: starpu.info,  Node: Generating traces,  Next: Gantt diagram,  Up: Off-line

7.2.1 Generating traces with FxT
--------------------------------

StarPU can use the FxT library (see
<https://savannah.nongnu.org/projects/fkt/>) to generate traces with a
limited runtime overhead.

   You can either get a tarball:
     % wget http://download.savannah.gnu.org/releases/fkt/fxt-0.2.2.tar.gz

   or use the FxT library from CVS (autotools are required):
     % cvs -d :pserver:anonymous@cvs.sv.gnu.org:/sources/fkt co FxT
     % ./bootstrap

   Compiling and installing the FxT library in the `$FXTDIR' path is
done following the standard procedure:
     % ./configure --prefix=$FXTDIR
     % make
     % make install

   In order to have StarPU to generate traces, StarPU should be
configured with the `--with-fxt' option:
     $ ./configure --with-fxt=$FXTDIR

   Or you can simply point the `PKG_CONFIG_PATH' to
`$FXTDIR/lib/pkgconfig' and pass `--with-fxt' to `./configure'

   When FxT is enabled, a trace is generated when StarPU is terminated
by calling `starpu_shutdown()'). The trace is a binary file whose name
has the form `prof_file_XXX_YYY' where `XXX' is the user name, and
`YYY' is the pid of the process that used StarPU. This file is saved in
the `/tmp/' directory by default, or by the directory specified by the
`STARPU_FXT_PREFIX' environment variable.


File: starpu.info,  Node: Gantt diagram,  Next: DAG,  Prev: Generating traces,  Up: Off-line

7.2.2 Creating a Gantt Diagram
------------------------------

When the FxT trace file `filename' has been generated, it is possible to
generate a trace in the Paje format by calling:
     % starpu_fxt_tool -i filename

   Or alternatively, setting the `STARPU_GENERATE_TRACE' environment
variable to 1 before application execution will make StarPU do it
automatically at application shutdown.

   This will create a `paje.trace' file in the current directory that
can be inspected with the ViTE trace visualizing open-source tool. More
information about ViTE is available at <http://vite.gforge.inria.fr/>.
It is possible to open the `paje.trace' file with ViTE by using the
following command:
     % vite paje.trace


File: starpu.info,  Node: DAG,  Next: starpu-workers-activity,  Prev: Gantt diagram,  Up: Off-line

7.2.3 Creating a DAG with graphviz
----------------------------------

When the FxT trace file `filename' has been generated, it is possible to
generate a task graph in the DOT format by calling:
     $ starpu_fxt_tool -i filename

   This will create a `dag.dot' file in the current directory. This
file is a task graph described using the DOT language. It is possible
to get a graphical output of the graph by using the graphviz library:
     $ dot -Tpdf dag.dot -o output.pdf


File: starpu.info,  Node: starpu-workers-activity,  Prev: DAG,  Up: Off-line

7.2.4 Monitoring activity
-------------------------

When the FxT trace file `filename' has been generated, it is possible to
generate an activity trace by calling:
     $ starpu_fxt_tool -i filename

   This will create an `activity.data' file in the current directory. A
profile of the application showing the activity of StarPU during the
execution of the program can be generated:
     $ starpu_workers_activity activity.data

   This will create a file named `activity.eps' in the current
directory.  This picture is composed of two parts.  The first part
shows the activity of the different workers. The green sections
indicate which proportion of the time was spent executed kernels on the
processing unit. The red sections indicate the proportion of time spent
in StartPU: an important overhead may indicate that the granularity may
be too low, and that bigger tasks may be appropriate to use the
processing unit more efficiently. The black sections indicate that the
processing unit was blocked because there was no task to process: this
may indicate a lack of parallelism which may be alleviated by creating
more tasks when it is possible.

   The second part of the `activity.eps' picture is a graph showing the
evolution of the number of tasks available in the system during the
execution.  Ready tasks are shown in black, and tasks that are
submitted but not schedulable yet are shown in grey.


File: starpu.info,  Node: Codelet performance,  Next: Theoretical lower bound on execution time API,  Prev: Off-line,  Up: Performance feedback

7.3 Performance of codelets
===========================

The performance model of codelets (described in *note Performance model
example::) can be examined by using the `starpu_perfmodel_display' tool:

     $ starpu_perfmodel_display -l
     file: <malloc_pinned.hannibal>
     file: <starpu_slu_lu_model_21.hannibal>
     file: <starpu_slu_lu_model_11.hannibal>
     file: <starpu_slu_lu_model_22.hannibal>
     file: <starpu_slu_lu_model_12.hannibal>

   Here, the codelets of the lu example are available. We can examine
the performance of the 22 kernel (in micro-seconds):

     $ starpu_perfmodel_display -s starpu_slu_lu_model_22
     performance model for cpu
     # hash      size       mean          dev           n
     57618ab0    19660800   2.851069e+05  1.829369e+04  109
     performance model for cuda_0
     # hash      size       mean          dev           n
     57618ab0    19660800   1.164144e+04  1.556094e+01  315
     performance model for cuda_1
     # hash      size       mean          dev           n
     57618ab0    19660800   1.164271e+04  1.330628e+01  360
     performance model for cuda_2
     # hash      size       mean          dev           n
     57618ab0    19660800   1.166730e+04  3.390395e+02  456

   We can see that for the given size, over a sample of a few hundreds
of execution, the GPUs are about 20 times faster than the CPUs (numbers
are in us). The standard deviation is extremely low for the GPUs, and
less than 10% for CPUs.

   The `starpu_regression_display' tool does the same for
regression-based performance models. It also writes a `.gp' file in the
current directory, to be run in the `gnuplot' tool, which shows the
corresponding curve.

   The same can also be achieved by using StarPU's library API, see
*note Performance Model API:: and notably the
`starpu_load_history_debug' function. The source code of the
`starpu_perfmodel_display' tool can be a useful example.


File: starpu.info,  Node: Theoretical lower bound on execution time API,  Prev: Codelet performance,  Up: Performance feedback

7.4 Theoretical lower bound on execution time
=============================================

See *note Theoretical lower bound on execution time:: for an example on
how to use this API. It permits to record a trace of what tasks are
needed to complete the application, and then, by using a linear system,
provide a theoretical lower bound of the execution time (i.e. with an
ideal scheduling).

   The computed bound is not really correct when not taking into account
dependencies, but for an application which have enough parallelism, it
is very near to the bound computed with dependencies enabled (which
takes a huge lot more time to compute), and thus provides a good-enough
estimation of the ideal execution time.

 -- Function: void starpu_bound_start (int DEPS, int PRIO)
     Start recording tasks (resets stats).  DEPS tells whether
     dependencies should be recorded too (this is quite expensive)

 -- Function: void starpu_bound_stop (void)
     Stop recording tasks

 -- Function: void starpu_bound_print_dot (FILE *OUTPUT)
     Print the DAG that was recorded

 -- Function: void starpu_bound_compute (double *RES, double
          *INTEGER_RES, int INTEGER)
     Get theoretical upper bound (in ms) (needs glpk support detected
     by `configure' script)

 -- Function: void starpu_bound_print_lp (FILE *OUTPUT)
     Emit the Linear Programming system on OUTPUT for the recorded
     tasks, in the lp format

 -- Function: void starpu_bound_print_mps (FILE *OUTPUT)
     Emit the Linear Programming system on OUTPUT for the recorded
     tasks, in the mps format

 -- Function: void starpu_bound_print (FILE *OUTPUT, int INTEGER)
     Emit statistics of actual execution vs theoretical upper bound.
     INTEGER permits to choose between integer solving (which takes a
     long time but is correct), and relaxed solving (which provides an
     approximate solution).


File: starpu.info,  Node: Tips and Tricks,  Next: StarPU MPI support,  Prev: Performance feedback,  Up: Top

8 Tips and Tricks to know about
*******************************

* Menu:

* Per-worker library initialization::  How to initialize a computation library once for each worker?


File: starpu.info,  Node: Per-worker library initialization,  Up: Tips and Tricks

8.1 How to initialize a computation library once for each worker?
=================================================================

Some libraries need to be initialized once for each concurrent instance
that may run on the machine. For instance, a C++ computation class
which is not thread-safe by itself, but for which several instanciated
objects of that class can be used concurrently. This can be used in
StarPU by initializing one such object per worker. For instance, the
libstarpufft example does the following to be able to use FFTW.

   Some global array stores the instanciated objects:

     fftw_plan plan_cpu[STARPU_NMAXWORKERS];

   At initialisation time of libstarpu, the objects are initialized:

     int workerid;
     for (workerid = 0; workerid < starpu_worker_get_count(); workerid++) {
         switch (starpu_worker_get_type(workerid)) {
             case STARPU_CPU_WORKER:
                 plan_cpu[workerid] = fftw_plan(...);
                 break;
         }
     }

   And in the codelet body, they are used:

     static void fft(void *descr[], void *_args)
     {
         int workerid = starpu_worker_get_id();
         fftw_plan plan = plan_cpu[workerid];
         ...

         fftw_execute(plan, ...);
     }

   Another way to go which may be needed is to execute some code from
the workers themselves thanks to `starpu_execute_on_each_worker'. This
may be required by CUDA to behave properly due to threading issues. For
instance, StarPU's `starpu_helper_cublas_init' looks like the following
to call `cublasInit' from the workers themselves:

     static void init_cublas_func(void *args STARPU_ATTRIBUTE_UNUSED)
     {
         cublasStatus cublasst = cublasInit();
         cublasSetKernelStream(starpu_cuda_get_local_stream());
     }
     void starpu_helper_cublas_init(void)
     {
         starpu_execute_on_each_worker(init_cublas_func, NULL, STARPU_CUDA);
     }


File: starpu.info,  Node: StarPU MPI support,  Next: StarPU FFT support,  Prev: Tips and Tricks,  Up: Top

9 StarPU MPI support
********************

The integration of MPI transfers within task parallelism is done in a
very natural way by the means of asynchronous interactions between the
application and StarPU.  This is implemented in a separate libstarpumpi
library which basically provides "StarPU" equivalents of `MPI_*'
functions, where `void *' buffers are replaced with
`starpu_data_handle_t's, and all GPU-RAM-NIC transfers are handled
efficiently by StarPU-MPI.  The user has to use the usual `mpirun'
command of the MPI implementation to start StarPU on the different MPI
nodes.

   An MPI Insert Task function provides an even more seamless
transition to a distributed application, by automatically issuing all
required data transfers according to the task graph and an
application-provided distribution.

* Menu:

* The API::
* Simple Example::
* MPI Insert Task Utility::
* MPI Collective Operations::


File: starpu.info,  Node: The API,  Next: Simple Example,  Up: StarPU MPI support

9.1 The API
===========

9.1.1 Compilation
-----------------

The flags required to compile or link against the MPI layer are then
accessible with the following commands:

     % pkg-config --cflags starpumpi-1.0  # options for the compiler
     % pkg-config --libs starpumpi-1.0    # options for the linker

   Also pass the `--static' option if the application is to be linked
statically.

9.1.2 Initialisation
--------------------

 -- Function: int starpu_mpi_initialize (void)
     Initializes the starpumpi library. This must be called between
     calling `starpu_init' and other `starpu_mpi' functions. This
     function does not call `MPI_Init', it should be called beforehand.

 -- Function: int starpu_mpi_initialize_extended (int *RANK, int
          *WORLD_SIZE)
     Initializes the starpumpi library. This must be called between
     calling `starpu_init' and other `starpu_mpi' functions.  This
     function calls `MPI_Init', and therefore should be prefered to the
     previous one for MPI implementations which are not thread-safe.
     Returns the current MPI node rank and world size.

 -- Function: int starpu_mpi_shutdown (void)
     Cleans the starpumpi library. This must be called between calling
     `starpu_mpi' functions and `starpu_shutdown'.  `MPI_Finalize' will
     be called if StarPU-MPI has been initialized by calling
     `starpu_mpi_initialize_extended'.

9.1.3 Communication
-------------------

The standard point to point communications of MPI have been
implemented. The semantic is similar to the MPI one, but adapted to the
DSM provided by StarPU. A MPI request will only be submitted when the
data is available in the main memory of the node submitting the request.

 -- Function: int starpu_mpi_send (starpu_data_handle_t DATA_HANDLE,
          int DEST, int MPI_TAG, MPI_Comm COMM)
     Performs a standard-mode, blocking send of DATA_HANDLE to the node
     DEST using the message tag `mpi_tag' within the communicator COMM.

 -- Function: int starpu_mpi_recv (starpu_data_handle_t DATA_HANDLE,
          int SOURCE, int MPI_TAG, MPI_Comm COMM, MPI_Status *STATUS)
     Performs a standard-mode, blocking receive in DATA_HANDLE from the
     node SOURCE using the message tag `mpi_tag' within the
     communicator COMM.

 -- Function: int starpu_mpi_isend (starpu_data_handle_t DATA_HANDLE,
          starpu_mpi_req *REQ, int DEST, int MPI_TAG, MPI_Comm COMM)
     Posts a standard-mode, non blocking send of DATA_HANDLE to the
     node DEST using the message tag `mpi_tag' within the communicator
     COMM. After the call, the pointer to the request REQ can be used
     to test the completion of the communication.

 -- Function: int starpu_mpi_irecv (starpu_data_handle_t DATA_HANDLE,
          starpu_mpi_req *REQ, int SOURCE, int MPI_TAG, MPI_Comm COMM)
     Posts a nonblocking receive in DATA_HANDLE from the node SOURCE
     using the message tag `mpi_tag' within the communicator COMM.
     After the call, the pointer to the request REQ can be used to test
     the completion of the communication.

 -- Function: int starpu_mpi_isend_detached (starpu_data_handle_t
          DATA_HANDLE, int DEST, int MPI_TAG, MPI_Comm COMM, void
          (*CALLBACK)(void *), void *ARG)
     Posts a standard-mode, non blocking send of DATA_HANDLE to the
     node DEST using the message tag `mpi_tag' within the communicator
     COMM. On completion, the CALLBACK function is called with the
     argument ARG.

 -- Function: int starpu_mpi_irecv_detached (starpu_data_handle_t
          DATA_HANDLE, int SOURCE, int MPI_TAG, MPI_Comm COMM, void
          (*CALLBACK)(void *), void *ARG)
     Posts a nonblocking receive in DATA_HANDLE from the node SOURCE
     using the message tag `mpi_tag' within the communicator COMM. On
     completion, the CALLBACK function is called with the argument ARG.

 -- Function: int starpu_mpi_wait (starpu_mpi_req *REQ, MPI_Status
          *STATUS)
     Returns when the operation identified by request REQ is complete.

 -- Function: int starpu_mpi_test (starpu_mpi_req *REQ, int *FLAG,
          MPI_Status *STATUS)
     If the operation identified by REQ is complete, set FLAG to 1. The
     STATUS object is set to contain information on the completed
     operation.

 -- Function: int starpu_mpi_barrier (MPI_Comm COMM)
     Blocks the caller until all group members of the communicator COMM
     have called it.

 -- Function: int starpu_mpi_isend_detached_unlock_tag
          (starpu_data_handle_t DATA_HANDLE, int DEST, int MPI_TAG,
          MPI_Comm COMM, starpu_tag_t TAG)
     Posts a standard-mode, non blocking send of DATA_HANDLE to the
     node DEST using the message tag `mpi_tag' within the communicator
     COMM. On completion, TAG is unlocked.

 -- Function: int starpu_mpi_irecv_detached_unlock_tag
          (starpu_data_handle_t DATA_HANDLE, int SOURCE, int MPI_TAG,
          MPI_Comm COMM, starpu_tag_t TAG)
     Posts a nonblocking receive in DATA_HANDLE from the node SOURCE
     using the message tag `mpi_tag' within the communicator COMM. On
     completion, TAG is unlocked.

 -- Function: int starpu_mpi_isend_array_detached_unlock_tag (unsigned
          ARRAY_SIZE, starpu_data_handle_t *DATA_HANDLE, int *DEST, int
          *MPI_TAG, MPI_Comm *COMM, starpu_tag_t TAG)
     Posts ARRAY_SIZE standard-mode, non blocking send of the data of
     data DATA_HANDLE[X] to the node DEST[X] using the message tag
     `mpi_tag[x]' within the communicator COMM[X]. On completion of the
     all the requests, TAG is unlocked.

 -- Function: int starpu_mpi_irecv_array_detached_unlock_tag (unsigned
          ARRAY_SIZE, starpu_data_handle_t *DATA_HANDLE, int *SOURCE,
          int *MPI_TAG, MPI_Comm *COMM, starpu_tag_t TAG)
     Posts ARRAY_SIZE nonblocking receive in DATA_HANDLE[X] from the
     node SOURCE[X] using the message tag `mpi_tag[x]' within the
     communicator COMM[X]. On completion of the all the requests, TAG
     is unlocked.


File: starpu.info,  Node: Simple Example,  Next: MPI Insert Task Utility,  Prev: The API,  Up: StarPU MPI support

9.2 Simple Example
==================

     void increment_token(void)
     {
         struct starpu_task *task = starpu_task_create();

         task->cl = &increment_cl;
         task->handles[0] = token_handle;

         starpu_task_submit(task);
     }

     int main(int argc, char **argv)
     {
         int rank, size;

         starpu_init(NULL);
         starpu_mpi_initialize_extended(&rank, &size);

         starpu_vector_data_register(&token_handle, 0, (uintptr_t)&token, 1, sizeof(unsigned));

         unsigned nloops = NITER;
         unsigned loop;

         unsigned last_loop = nloops - 1;
         unsigned last_rank = size - 1;

         for (loop = 0; loop < nloops; loop++) {
             int tag = loop*size + rank;

             if (loop == 0 && rank == 0)
             {
                 token = 0;
                 fprintf(stdout, "Start with token value %d\n", token);
             }
             else
             {
                 starpu_mpi_irecv_detached(token_handle, (rank+size-1)%size, tag,
                         MPI_COMM_WORLD, NULL, NULL);
             }

             increment_token();

             if (loop == last_loop && rank == last_rank)
             {
                 starpu_data_acquire(token_handle, STARPU_R);
                 fprintf(stdout, "Finished: token value %d\n", token);
                 starpu_data_release(token_handle);
             }
             else
             {
                 starpu_mpi_isend_detached(token_handle, (rank+1)%size, tag+1,
                         MPI_COMM_WORLD, NULL, NULL);
             }
         }

         starpu_task_wait_for_all();

         starpu_mpi_shutdown();
         starpu_shutdown();

         if (rank == last_rank)
         {
             fprintf(stderr, "[%d] token = %d == %d * %d ?\n", rank, token, nloops, size);
             STARPU_ASSERT(token == nloops*size);
         }


File: starpu.info,  Node: MPI Insert Task Utility,  Next: MPI Collective Operations,  Prev: Simple Example,  Up: StarPU MPI support

9.3 MPI Insert Task Utility
===========================

To save the programmer from having to explicit all communications,
StarPU provides an "MPI Insert Task Utility". The principe is that the
application decides a distribution of the data over the MPI nodes by
allocating it and notifying StarPU of that decision, i.e. tell StarPU
which MPI node "owns" which data. All MPI nodes then process the whole
task graph, and StarPU automatically determines which node actually
execute which task, as well as the required MPI transfers.

 -- Function: int starpu_data_set_tag (starpu_data_handle_t HANDLE, int
          TAG)
     Tell StarPU-MPI which MPI tag to use when exchanging the data.

 -- Function: int starpu_data_get_tag (starpu_data_handle_t HANDLE)
     Returns the MPI tag to be used when exchanging the data.

 -- Function: int starpu_data_set_rank (starpu_data_handle_t HANDLE,
          int RANK)
     Tell StarPU-MPI which MPI node "owns" a given data, that is, the
     node which will always keep an up-to-date value, and will by
     default execute tasks which write to it.

 -- Function: int starpu_data_get_rank (starpu_data_handle_t HANDLE)
     Returns the last value set by `starpu_data_set_rank'.

 -- Macro: STARPU_EXECUTE_ON_NODE
     this macro is used when calling `starpu_mpi_insert_task', and must
     be followed by a integer value which specified the node on which
     to execute the codelet.

 -- Macro: STARPU_EXECUTE_ON_DATA
     this macro is used when calling `starpu_mpi_insert_task', and must
     be followed by a data handle to specify that the node owning the
     given data will execute the codelet.

 -- Function: int starpu_mpi_insert_task (MPI_Comm COMM, struct
          starpu_codelet *CODELET, ...)
     Create and submit a task corresponding to CODELET with the
     following arguments.  The argument list must be zero-terminated.

     The arguments following the codelets are the same types as for the
     function `starpu_insert_task' defined in *note Insert Task
     Utility::. The extra argument `STARPU_EXECUTE_ON_NODE' followed by
     an integer allows to specify the MPI node to execute the codelet.
     It is also possible to specify that the node owning a specific
     data will execute the codelet, by using `STARPU_EXECUTE_ON_DATA'
     followed by a data handle.

     The internal algorithm is as follows:
       1. Find out whether we (as an MPI node) are to execute the
          codelet because we own the data to be written to. If
          different nodes own data to be written to, the argument
          `STARPU_EXECUTE_ON_NODE' or `STARPU_EXECUTE_ON_DATA' has to
          be used to specify which MPI node will execute the task.

       2. Send and receive data as requested. Nodes owning data which
          need to be read by the task are sending them to the MPI node
          which will execute it. The latter receives them.

       3. Execute the codelet. This is done by the MPI node selected in
          the 1st step of the algorithm.

       4. In the case when different MPI nodes own data to be written
          to, send written data back to their owners.

     The algorithm also includes a cache mechanism that allows not to
     send data twice to the same MPI node, unless the data has been
     modified.


 -- Function: void starpu_mpi_get_data_on_node (MPI_Comm COMM,
          starpu_data_handle_t DATA_HANDLE, int NODE)
     Transfer data DATA_HANDLE to MPI node NODE, sending it from its
     owner if needed. At least the target node and the owner have to
     call the function.

   Here an stencil example showing how to use `starpu_mpi_insert_task'.
One first needs to define a distribution function which specifies the
locality of the data. Note that that distribution information needs to
be given to StarPU by calling `starpu_data_set_rank'.

     /* Returns the MPI node number where data is */
     int my_distrib(int x, int y, int nb_nodes) {
       /* Block distrib */
       return ((int)(x / sqrt(nb_nodes) + (y / sqrt(nb_nodes)) * sqrt(nb_nodes))) % nb_nodes;

       // /* Other examples useful for other kinds of computations */
       // /* / distrib */
       // return (x+y) % nb_nodes;

       // /* Block cyclic distrib */
       // unsigned side = sqrt(nb_nodes);
       // return x % side + (y % side) * size;
     }

   Now the data can be registered within StarPU. Data which are not
owned but will be needed for computations can be registered through the
lazy allocation mechanism, i.e. with a `home_node' set to -1.  StarPU
will automatically allocate the memory when it is used for the first
time.

   One can note an optimization here (the `else if' test): we only
register data which will be needed by the tasks that we will execute.

         unsigned matrix[X][Y];
         starpu_data_handle_t data_handles[X][Y];

         for(x = 0; x < X; x++) {
             for (y = 0; y < Y; y++) {
                 int mpi_rank = my_distrib(x, y, size);
                  if (mpi_rank == my_rank)
                     /* Owning data */
                     starpu_variable_data_register(&data_handles[x][y], 0,
                                                   (uintptr_t)&(matrix[x][y]), sizeof(unsigned));
                 else if (my_rank == my_distrib(x+1, y, size) || my_rank == my_distrib(x-1, y, size)
                       || my_rank == my_distrib(x, y+1, size) || my_rank == my_distrib(x, y-1, size))
                     /* I don't own that index, but will need it for my computations */
                     starpu_variable_data_register(&data_handles[x][y], -1,
                                                   (uintptr_t)NULL, sizeof(unsigned));
                 else
                     /* I know it's useless to allocate anything for this */
                     data_handles[x][y] = NULL;
                 if (data_handles[x][y])
                     starpu_data_set_rank(data_handles[x][y], mpi_rank);
             }
         }

   Now `starpu_mpi_insert_task()' can be called for the different steps
of the application.

         for(loop=0 ; loop<niter; loop++)
             for (x = 1; x < X-1; x++)
                 for (y = 1; y < Y-1; y++)
                     starpu_mpi_insert_task(MPI_COMM_WORLD, &stencil5_cl,
                                            STARPU_RW, data_handles[x][y],
                                            STARPU_R, data_handles[x-1][y],
                                            STARPU_R, data_handles[x+1][y],
                                            STARPU_R, data_handles[x][y-1],
                                            STARPU_R, data_handles[x][y+1],
                                            0);
         starpu_task_wait_for_all();

   I.e. all MPI nodes process the whole task graph, but as mentioned
above, for each task, only the MPI node which owns the data being
written to (here, `data_handles[x][y]') will actually run the task. The
other MPI nodes will automatically send the required data.


File: starpu.info,  Node: MPI Collective Operations,  Prev: MPI Insert Task Utility,  Up: StarPU MPI support

9.4 MPI Collective Operations
=============================

 -- Function: int starpu_mpi_scatter_detached (starpu_data_handle_t
          *DATA_HANDLES, int COUNT, int ROOT, MPI_Comm COMM)
     Scatter data among processes of the communicator based on the
     ownership of the data. For each data of the array DATA_HANDLES, the
     process ROOT sends the data to the process owning this data.
     Processes receiving data must have valid data handles to receive
     them.

 -- Function: int starpu_mpi_gather_detached (starpu_data_handle_t
          *DATA_HANDLES, int COUNT, int ROOT, MPI_Comm COMM)
     Gather data from the different processes of the communicator onto
     the process ROOT. Each process owning data handle in the array
     DATA_HANDLES will send them to the process ROOT. The process ROOT
     must have valid data handles to receive the data.

     if (rank == root)
     {
         /* Allocate the vector */
         vector = malloc(nblocks * sizeof(float *));
         for(x=0 ; x<nblocks ; x++)
         {
             starpu_malloc((void **)&vector[x], block_size*sizeof(float));
         }
     }

     /* Allocate data handles and register data to StarPU */
     data_handles = malloc(nblocks*sizeof(starpu_data_handle_t *));
     for(x = 0; x < nblocks ;  x++)
     {
         int mpi_rank = my_distrib(x, nodes);
         if (rank == root) {
             starpu_vector_data_register(&data_handles[x], 0, (uintptr_t)vector[x],
                                         blocks_size, sizeof(float));
         }
         else if ((mpi_rank == rank) || ((rank == mpi_rank+1 || rank == mpi_rank-1))) {
             /* I own that index, or i will need it for my computations */
             starpu_vector_data_register(&data_handles[x], -1, (uintptr_t)NULL,
                                        block_size, sizeof(float));
         }
         else {
             /* I know it's useless to allocate anything for this */
             data_handles[x] = NULL;
         }
         if (data_handles[x]) {
             starpu_data_set_rank(data_handles[x], mpi_rank);
         }
     }

     /* Scatter the matrix among the nodes */
     starpu_mpi_scatter_detached(data_handles, nblocks, root, MPI_COMM_WORLD);

     /* Calculation */
     for(x = 0; x < nblocks ;  x++) {
         if (data_handles[x]) {
             int owner = starpu_data_get_rank(data_handles[x]);
             if (owner == rank) {
                 starpu_insert_task(&cl, STARPU_RW, data_handles[x], 0);
             }
         }
     }

     /* Gather the matrix on main node */
     starpu_mpi_gather_detached(data_handles, nblocks, 0, MPI_COMM_WORLD);


File: starpu.info,  Node: StarPU FFT support,  Next: C Extensions,  Prev: StarPU MPI support,  Up: Top

10 StarPU FFT support
*********************

StarPU provides `libstarpufft', a library whose design is very similar
to both fftw and cufft, the difference being that it takes benefit from
both CPUs and GPUs. It should however be noted that GPUs do not have
the same precision as CPUs, so the results may different by a
negligible amount

   float, double and long double precisions are available, with the
fftw naming convention:

  1. double precision structures and functions are named e.g.
     `starpufft_execute'

  2. float precision structures and functions are named e.g.
     `starpufftf_execute'

  3. long double precision structures and functions are named e.g.
     `starpufftl_execute'

   The documentation below uses names for double precision, replace
`starpufft_' with `starpufftf_' or `starpufftl_' as appropriate.

   Only complex numbers are supported at the moment.

   The application has to call `starpu_init' before calling starpufft
functions.

   Either main memory pointers or data handles can be provided.

  1. To provide main memory pointers, use `starpufft_start' or
     `starpufft_execute'. Only one FFT can be performed at a time,
     because StarPU will have to register the data on the fly. In the
     `starpufft_start' case, `starpufft_cleanup' needs to be called to
     unregister the data.

  2. To provide data handles (which is preferrable), use
     `starpufft_start_handle' (preferred) or
     `starpufft_execute_handle'. Several FFTs Several FFT tasks can be
     submitted for a given plan, which permits e.g. to start a series
     of FFT with just one plan. `starpufft_start_handle' is preferrable
     since it does not wait for the task completion, and thus permits
     to enqueue a series of tasks.

10.1 Compilation
================

The flags required to compile or link against the FFT library are
accessible with the following commands:

     % pkg-config --cflags starpufft-1.0  # options for the compiler
     % pkg-config --libs starpufft-1.0    # options for the linker

   Also pass the `--static' option if the application is to be linked
statically.

10.2 Initialisation
===================

 -- Function: void * starpufft_malloc (size_t N)
     Allocates memory for N bytes. This is preferred over `malloc',
     since it allocates pinned memory, which allows overlapped
     transfers.

 -- Function: void * starpufft_free (void *P)
     Release memory previously allocated.

 -- Function: struct starpufft_plan * starpufft_plan_dft_1d (int N, int
          SIGN, unsigned FLAGS)
     Initializes a plan for 1D FFT of size N. SIGN can be
     `STARPUFFT_FORWARD' or `STARPUFFT_INVERSE'. FLAGS must be 0.

 -- Function: struct starpufft_plan * starpufft_plan_dft_2d (int N, int
          M, int SIGN, unsigned FLAGS)
     Initializes a plan for 2D FFT of size (N, M). SIGN can be
     `STARPUFFT_FORWARD' or `STARPUFFT_INVERSE'. FLAGS must be 0.

 -- Function: struct starpu_task * starpufft_start (starpufft_plan P,
          void *IN, void *OUT)
     Start an FFT previously planned as P, using IN and OUT as input
     and output. This only submits the task and does not wait for it.
     The application should call `starpufft_cleanup' to unregister the
     data.

 -- Function: struct starpu_task * starpufft_start_handle
          (starpufft_plan P, starpu_data_handle_t IN,
          starpu_data_handle_t OUT)
     Start an FFT previously planned as P, using data handles IN and
     OUT as input and output (assumed to be vectors of elements of the
     expected types). This only submits the task and does not wait for
     it.

 -- Function: void starpufft_execute (starpufft_plan P, void *IN, void
          *OUT)
     Execute an FFT previously planned as P, using IN and OUT as input
     and output. This submits and waits for the task.

 -- Function: void starpufft_execute_handle (starpufft_plan P,
          starpu_data_handle_t IN, starpu_data_handle_t OUT)
     Execute an FFT previously planned as P, using data handles IN and
     OUT as input and output (assumed to be vectors of elements of the
     expected types). This submits and waits for the task.

 -- Function: void starpufft_cleanup (starpufft_plan P)
     Releases data for plan P, in the `starpufft_start' case.

 -- Function: void starpufft_destroy_plan (starpufft_plan P)
     Destroys plan P, i.e. release all CPU (fftw) and GPU (cufft)
     resources.


File: starpu.info,  Node: C Extensions,  Next: SOCL OpenCL Extensions,  Prev: StarPU FFT support,  Up: Top

11 C Extensions
***************

When GCC plug-in support is available, StarPU builds a plug-in for the
GNU Compiler Collection (GCC), which defines extensions to languages of
the C family (C, C++, Objective-C) that make it easier to write StarPU
code(1).

   Those extensions include syntactic sugar for defining tasks and
their implementations, invoking a task, and manipulating data buffers.
Use of these extensions can be made conditional on the availability of
the plug-in, leading to valid C sequential code when the plug-in is not
used (*note Conditional Extensions::).

   When StarPU has been installed with its GCC plug-in, programs that
use these extensions can be compiled this way:

     $ gcc -c -fplugin=`pkg-config starpu-1.0 --variable=gccplugin` foo.c

When the plug-in is not available, the above `pkg-config' command
returns the empty string.

   In addition, the `-fplugin-arg-starpu-verbose' flag can be used to
obtain feedback from the compiler as it analyzes the C extensions used
in source files.

   This section describes the C extensions implemented by StarPU's GCC
plug-in.  It does not require detailed knowledge of the StarPU library.

   Note: as of StarPU 1.0.0rc4, this is still an area under development
and subject to change.

* Menu:

* Defining Tasks::              Defining StarPU tasks
* Synchronization and Other Pragmas:: Synchronization, and more.
* Registered Data Buffers::     Manipulating data buffers
* Conditional Extensions::      Using C extensions only when available

   ---------- Footnotes ----------

   (1) This feature is only available for GCC 4.5 and later; it is
known to work with GCC 4.5, 4.6, and 4.7.  You may need to install a
specific `-dev' package of your distro, such as `gcc-4.6-plugin-dev' on
Debian and derivatives.  In addition, the plug-in's test suite is only
run when GNU Guile (http://www.gnu.org/software/guile/) is found at
`configure'-time.  Building the GCC plug-in can be disabled by
configuring with `--disable-gcc-extensions'.


File: starpu.info,  Node: Defining Tasks,  Next: Synchronization and Other Pragmas,  Up: C Extensions

11.1 Defining Tasks
===================

The StarPU GCC plug-in views "tasks" as "extended" C functions:

  1. tasks may have several implementations--e.g., one for CPUs, one
     written in OpenCL, one written in CUDA;

  2. tasks may have several implementations of the same target--e.g.,
     several CPU implementations;

  3. when a task is invoked, it may run in parallel, and StarPU is free
     to choose any of its implementations.

   Tasks and their implementations must be _declared_.  These
declarations are annotated with "attributes" (*note attributes in GNU
C: (gcc)Attribute Syntax.): the declaration of a task is a regular C
function declaration with an additional `task' attribute, and task
implementations are declared with a `task_implementation' attribute.

   The following function attributes are provided:

`task'
     Declare the given function as a StarPU task.  Its return type must
     be `void', and it must not be defined--instead, a definition will
     automatically be provided by the compiler.

     Under the hood, declaring a task leads to the declaration of the
     corresponding `codelet' (*note Codelet and Tasks::).  If one or
     more task implementations are declared in the same compilation
     unit, then the codelet and the function itself are also defined;
     they inherit the scope of the task.

     Scalar arguments to the task are passed by value and copied to the
     target device if need be--technically, they are passed as the
     `cl_arg' buffer (*note `cl_arg': Codelets and Tasks.).

     Pointer arguments are assumed to be registered data buffers--the
     `buffers' argument of a task (*note `buffers': Codelets and
     Tasks.); `const'-qualified pointer arguments are viewed as
     read-only buffers (`STARPU_R'), and non-`const'-qualified buffers
     are assumed to be used read-write (`STARPU_RW').  In addition, the
     `output' type attribute can be as a type qualifier for output
     pointer or array parameters (`STARPU_W').

`task_implementation (TARGET, TASK)'
     Declare the given function as an implementation of TASK to run on
     TARGET.  TARGET must be a string, currently one of `"cpu"',
     `"opencl"', or `"cuda"'.


   Here is an example:

     #define __output  __attribute__ ((output))

     static void matmul (const float *A, const float *B,
                         __output float *C,
                         unsigned nx, unsigned ny, unsigned nz)
       __attribute__ ((task));

     static void matmul_cpu (const float *A, const float *B,
                             __output float *C,
                             unsigned nx, unsigned ny, unsigned nz)
       __attribute__ ((task_implementation ("cpu", matmul)));


     static void
     matmul_cpu (const float *A, const float *B, __output float *C,
                 unsigned nx, unsigned ny, unsigned nz)
     {
       unsigned i, j, k;

       for (j = 0; j < ny; j++)
         for (i = 0; i < nx; i++)
           {
             for (k = 0; k < nz; k++)
               C[j * nx + i] += A[j * nz + k] * B[k * nx + i];
           }
     }

A `matmult' task is defined; it has only one implementation,
`matmult_cpu', which runs on the CPU.  Variables A and B are input
buffers, whereas C is considered an input/output buffer.

   CUDA and OpenCL implementations can be declared in a similar way:

     static void matmul_cuda (const float *A, const float *B, float *C,
                              unsigned nx, unsigned ny, unsigned nz)
       __attribute__ ((task_implementation ("cuda", matmul)));

     static void matmul_opencl (const float *A, const float *B, float *C,
                                unsigned nx, unsigned ny, unsigned nz)
       __attribute__ ((task_implementation ("opencl", matmul)));

The CUDA and OpenCL implementations typically either invoke a kernel
written in CUDA or OpenCL (for similar code, *note CUDA Kernel::, and
*note OpenCL Kernel::), or call a library function that uses CUDA or
OpenCL under the hood, such as CUBLAS functions:

     static void
     matmul_cuda (const float *A, const float *B, float *C,
                  unsigned nx, unsigned ny, unsigned nz)
     {
       cublasSgemm ('n', 'n', nx, ny, nz,
                    1.0f, A, 0, B, 0,
                    0.0f, C, 0);
       cudaStreamSynchronize (starpu_cuda_get_local_stream ());
     }

   A task can be invoked like a regular C function:

     matmul (&A[i * zdim * bydim + k * bzdim * bydim],
             &B[k * xdim * bzdim + j * bxdim * bzdim],
             &C[i * xdim * bydim + j * bxdim * bydim],
             bxdim, bydim, bzdim);

This leads to an "asynchronous invocation", whereby `matmult''s
implementation may run in parallel with the continuation of the caller.

   The next section describes how memory buffers must be handled in
StarPU-GCC code.  For a complete example, see the `gcc-plugin/examples'
directory of the source distribution, and *note the vector-scaling
example: Vector Scaling Using the C Extension.


File: starpu.info,  Node: Synchronization and Other Pragmas,  Next: Registered Data Buffers,  Prev: Defining Tasks,  Up: C Extensions

11.2 Initialization, Termination, and Synchronization
=====================================================

The following pragmas allow user code to control StarPU's life time and
to synchronize with tasks.

`#pragma starpu initialize'
     Initialize StarPU.  This call is compulsory and is _never_ added
     implicitly.  One of the reasons this has to be done explicitly is
     that it provides greater control to user code over its resource
     usage.

`#pragma starpu shutdown'
     Shut down StarPU, giving it an opportunity to write profiling info
     to a file on disk, for instance (*note off-line performance
     feedback: Off-line.).

`#pragma starpu wait'
     Wait for all task invocations to complete, as with
     `starpu_wait_for_all' (*note starpu_wait_for_all: Codelets and
     Tasks.).



File: starpu.info,  Node: Registered Data Buffers,  Next: Conditional Extensions,  Prev: Synchronization and Other Pragmas,  Up: C Extensions

11.3 Registered Data Buffers
============================

Data buffers such as matrices and vectors that are to be passed to tasks
must be "registered".  Registration allows StarPU to handle data
transfers among devices--e.g., transferring an input buffer from the
CPU's main memory to a task scheduled to run a GPU (*note StarPU Data
Management Library::).

   The following pragmas are provided:

`#pragma starpu register PTR [SIZE]'
     Register PTR as a SIZE-element buffer.  When PTR has an array type
     whose size is known, SIZE may be omitted.

`#pragma starpu unregister PTR'
     Unregister the previously-registered memory area pointed to by
     PTR.  As a side-effect, PTR points to a valid copy in main memory.

`#pragma starpu acquire PTR'
     Acquire in main memory an up-to-date copy of the
     previously-registered memory area pointed to by PTR, for
     read-write access.

`#pragma starpu release PTR'
     Release the previously-register memory area pointed to by PTR,
     making it available to the tasks.


   Additionally, the `heap_allocated' variable attribute offers a
simple way to allocate storage for arrays on the heap:

`heap_allocated'
     This attributes applies to local variables with an array type.  Its
     effect is to automatically allocate the array's storage on the
     heap, using `starpu_malloc' under the hood (*note starpu_malloc:
     Basic Data Library API.).  The heap-allocated array is
     automatically freed when the variable's scope is left, as with
     automatic variables.


The following example illustrates use of the `heap_allocated' attribute:

     extern void cholesky(unsigned nblocks, unsigned size,
                         float mat[nblocks][nblocks][size])
       __attribute__ ((task));

     int
     main (int argc, char *argv[])
     {
     #pragma starpu initialize

       /* ... */

       int nblocks, size;
       parse_args (&nblocks, &size);

       /* Allocate an array of the required size on the heap,
          and register it.  */

       {
         float matrix[nblocks][nblocks][size]
           __attribute__ ((heap_allocated));

     #pragma starpu register matrix

         cholesky (nblocks, size, matrix);

     #pragma starpu wait
     #pragma starpu unregister matrix

       }   /* MATRIX is automatically freed here.  */

     #pragma starpu shutdown

       return EXIT_SUCCESS;
     }


File: starpu.info,  Node: Conditional Extensions,  Prev: Registered Data Buffers,  Up: C Extensions

11.4 Using C Extensions Conditionally
=====================================

The C extensions described in this chapter are only available when GCC
and its StarPU plug-in are in use.  Yet, it is possible to make use of
these extensions when they are available--leading to hybrid CPU/GPU
code--and discard them when they are not available--leading to valid
sequential code.

   To that end, the GCC plug-in defines a C preprocessor macro when it
is being used:

 -- Macro: STARPU_GCC_PLUGIN
     Defined for code being compiled with the StarPU GCC plug-in.  When
     defined, this macro expands to an integer denoting the version of
     the supported C extensions.

   The code below illustrates how to define a task and its
implementations in a way that allows it to be compiled without the GCC
plug-in:

     /* The macros below abstract over the attributes specific to
        StarPU-GCC and the name of the CPU implementation.  */
     #ifdef STARPU_GCC_PLUGIN
     # define __task  __attribute__ ((task))
     # define CPU_TASK_IMPL(task)  task ## _cpu
     #else
     # define __task
     # define CPU_TASK_IMPL(task)  task
     #endif

     #include <stdlib.h>

     static void matmul (const float *A, const float *B, float *C,
                         unsigned nx, unsigned ny, unsigned nz) __task;

     #ifdef STARPU_GCC_PLUGIN

     static void matmul_cpu (const float *A, const float *B, float *C,
                             unsigned nx, unsigned ny, unsigned nz)
       __attribute__ ((task_implementation ("cpu", matmul)));

     #endif


     static void
     CPU_TASK_IMPL (matmul) (const float *A, const float *B, float *C,
                             unsigned nx, unsigned ny, unsigned nz)
     {
       /* Code of the CPU kernel here...  */
     }

     int
     main (int argc, char *argv[])
     {
       /* The pragmas below are simply ignored when StarPU-GCC
          is not used.  */
     #pragma starpu initialize

       float A[123][42][7], B[123][42][7], C[123][42][7];

     #pragma starpu register A
     #pragma starpu register B
     #pragma starpu register C

       /* When StarPU-GCC is used, the call below is asynchronous;
          otherwise, it is synchronous.  */
       matmul (A, B, C, 123, 42, 7);

     #pragma starpu wait
     #pragma starpu shutdown

       return EXIT_SUCCESS;
     }

   Note that attributes such as `task' are simply ignored by GCC when
the StarPU plug-in is not loaded, so the `__task' macro could be
omitted altogether.  However, `gcc -Wall' emits a warning for unknown
attributes, which can be inconvenient, and other compilers may be
unable to parse the attribute syntax.  Thus, using macros such as
`__task' above is recommended.


File: starpu.info,  Node: SOCL OpenCL Extensions,  Next: StarPU Basic API,  Prev: C Extensions,  Up: Top

12 SOCL OpenCL Extensions
*************************

SOCL is an extension that aims at implementing the OpenCL standard on
top of StarPU. It allows to gives a (relatively) clean and standardized
API to StarPU.  By allowing OpenCL applications to use StarPU
transparently, it provides users with the latest StarPU enhancements
without any further development, and allows these OpenCL applications
to easily fall back to another OpenCL implementation.

   This section does not require detailed knowledge of the StarPU
library.

   Note: as of StarPU 1.0.0rc4, this is still an area under development
and subject to change.

   TODO


File: starpu.info,  Node: StarPU Basic API,  Next: StarPU Advanced API,  Prev: SOCL OpenCL Extensions,  Up: Top

13 StarPU Basic API
*******************

* Menu:

* Initialization and Termination::  Initialization and Termination methods
* Workers' Properties::         Methods to enumerate workers' properties
* Data Library::                Methods to manipulate data
* Data Interfaces::
* Data Partition::
* Codelets and Tasks::          Methods to construct tasks
* Explicit Dependencies::       Explicit Dependencies
* Implicit Data Dependencies::  Implicit Data Dependencies
* Performance Model API::
* Profiling API::               Profiling API
* CUDA extensions::             CUDA extensions
* OpenCL extensions::           OpenCL extensions
* Cell extensions::             Cell extensions
* Miscellaneous helpers::


File: starpu.info,  Node: Initialization and Termination,  Next: Workers' Properties,  Up: StarPU Basic API

13.1 Initialization and Termination
===================================

 -- Function: int starpu_init (struct starpu_conf *CONF)
     This is StarPU initialization method, which must be called prior
     to any other StarPU call.  It is possible to specify StarPU's
     configuration (e.g. scheduling policy, number of cores, ...) by
     passing a non-null argument. Default configuration is used if the
     passed argument is `NULL'.

     Upon successful completion, this function returns 0. Otherwise,
     `-ENODEV' indicates that no worker was available (so that StarPU
     was not initialized).

 -- Data Type: struct starpu_conf
     This structure is passed to the `starpu_init' function in order to
     configure StarPU.  When the default value is used, StarPU
     automatically selects the number of processing units and takes the
     default scheduling policy. The environment variables overwrite the
     equivalent parameters.

    `const char *sched_policy_name' (default = NULL)
          This is the name of the scheduling policy. This can also be
          specified with the `STARPU_SCHED' environment variable.

    `struct starpu_sched_policy *sched_policy' (default = NULL)
          This is the definition of the scheduling policy. This field
          is ignored if `sched_policy_name' is set.

    `int ncpus' (default = -1)
          This is the number of CPU cores that StarPU can use. This can
          also be specified with the `STARPU_NCPUS' environment
          variable.

    `int ncuda' (default = -1)
          This is the number of CUDA devices that StarPU can use. This
          can also be specified with the `STARPU_NCUDA' environment
          variable.

    `int nopencl' (default = -1)
          This is the number of OpenCL devices that StarPU can use.
          This can also be specified with the `STARPU_NOPENCL'
          environment variable.

    `int nspus' (default = -1)
          This is the number of Cell SPUs that StarPU can use. This can
          also be specified with the `STARPU_NGORDON' environment
          variable.

    `unsigned use_explicit_workers_bindid' (default = 0)
          If this flag is set, the `workers_bindid' array indicates
          where the different workers are bound, otherwise StarPU
          automatically selects where to bind the different workers.
          This can also be specified with the `STARPU_WORKERS_CPUID'
          environment variable.

    `unsigned workers_bindid[STARPU_NMAXWORKERS]'
          If the `use_explicit_workers_bindid' flag is set, this array
          indicates where to bind the different workers. The i-th entry
          of the `workers_bindid' indicates the logical identifier of
          the processor which should execute the i-th worker. Note that
          the logical ordering of the CPUs is either determined by the
          OS, or provided by the `hwloc' library in case it is
          available.

    `unsigned use_explicit_workers_cuda_gpuid' (default = 0)
          If this flag is set, the CUDA workers will be attached to the
          CUDA devices specified in the `workers_cuda_gpuid' array.
          Otherwise, StarPU affects the CUDA devices in a round-robin
          fashion. This can also be specified with the
          `STARPU_WORKERS_CUDAID' environment variable.

    `unsigned workers_cuda_gpuid[STARPU_NMAXWORKERS]'
          If the `use_explicit_workers_cuda_gpuid' flag is set, this
          array contains the logical identifiers of the CUDA devices
          (as used by `cudaGetDevice').

    `unsigned use_explicit_workers_opencl_gpuid' (default = 0)
          If this flag is set, the OpenCL workers will be attached to
          the OpenCL devices specified in the `workers_opencl_gpuid'
          array. Otherwise, StarPU affects the OpenCL devices in a
          round-robin fashion. This can also be specified with the
          `STARPU_WORKERS_OPENCLID' environment variable.

    `unsigned workers_opencl_gpuid[STARPU_NMAXWORKERS]'
          If the `use_explicit_workers_opencl_gpuid' flag is set, this
          array contains the logical identifiers of the OpenCL devices
          to be used.

    `int calibrate' (default = 0)
          If this flag is set, StarPU will calibrate the performance
          models when executing tasks. If this value is equal to -1,
          the default value is used. This can also be specified with
          the `STARPU_CALIBRATE' environment variable.

    `int single_combined_worker' (default = 0)
          By default, StarPU parallel tasks concurrently.  Some
          parallel libraries (e.g. most OpenMP implementations) however
          do not support concurrent calls to parallel code. In such
          case, setting this flag makes StarPU only start one parallel
          task at a time.  This can also be specified with the
          `STARPU_SINGLE_COMBINED_WORKER' environment variable.

    `int disable_asynchronous_copy' (default = 0)
          This flag should be set to 1 to disable asynchronous copies
          between CPUs and accelerators. This can also be specified
          with the `DISABLE_STARPU_ASYNCHRONOUS_COPY' environment
          variable.  The AMD implementation of OpenCL is known to fail
          when copying data asynchronously. When using this
          implementation, it is therefore necessary to disable
          asynchronous data transfers.

 -- Function: int starpu_conf_init (struct starpu_conf *CONF)
     This function initializes the CONF structure passed as argument
     with the default values. In case some configuration parameters are
     already specified through environment variables,
     `starpu_conf_init' initializes the fields of the structure
     according to the environment variables. For instance if
     `STARPU_CALIBRATE' is set, its value is put in the `.ncuda' field
     of the structure passed as argument.

     Upon successful completion, this function returns 0. Otherwise,
     `-EINVAL' indicates that the argument was NULL.

 -- Function: void starpu_shutdown (void)
     This is StarPU termination method. It must be called at the end of
     the application: statistics and other post-mortem debugging
     information are not guaranteed to be available until this method
     has been called.

 -- Function: int starpu_asynchronous_copy_disabled ()
     Return 1 if asynchronous data transfers between CPU and
     accelerators are disabled.


File: starpu.info,  Node: Workers' Properties,  Next: Data Library,  Prev: Initialization and Termination,  Up: StarPU Basic API

13.2 Workers' Properties
========================

 -- Data Type: enum starpu_archtype
     The different values are:
    `STARPU_CPU_WORKER'

    `STARPU_CUDA_WORKER'

    `STARPU_OPENCL_WORKER'

    `STARPU_GORDON_WORKER'

 -- Function: unsigned starpu_worker_get_count (void)
     This function returns the number of workers (i.e. processing units
     executing StarPU tasks). The returned value should be at most
     `STARPU_NMAXWORKERS'.

 -- Function: int starpu_worker_get_count_by_type (enum starpu_archtype
          TYPE)
     Returns the number of workers of the given type indicated by the
     argument. A positive (or null) value is returned in case of
     success, `-EINVAL' indicates that the type is not valid otherwise.

 -- Function: unsigned starpu_cpu_worker_get_count (void)
     This function returns the number of CPUs controlled by StarPU. The
     returned value should be at most `STARPU_MAXCPUS'.

 -- Function: unsigned starpu_cuda_worker_get_count (void)
     This function returns the number of CUDA devices controlled by
     StarPU. The returned value should be at most `STARPU_MAXCUDADEVS'.

 -- Function: unsigned starpu_opencl_worker_get_count (void)
     This function returns the number of OpenCL devices controlled by
     StarPU. The returned value should be at most
     `STARPU_MAXOPENCLDEVS'.

 -- Function: unsigned starpu_spu_worker_get_count (void)
     This function returns the number of Cell SPUs controlled by StarPU.

 -- Function: int starpu_worker_get_id (void)
     This function returns the identifier of the current worker, i.e
     the one associated to the calling thread. The returned value is
     either -1 if the current context is not a StarPU worker (i.e. when
     called from the application outside a task or a callback), or an
     integer between 0 and `starpu_worker_get_count() - 1'.

 -- Function: int starpu_worker_get_ids_by_type (enum starpu_archtype
          TYPE, int *WORKERIDS, int MAXSIZE)
     This function gets the list of identifiers of workers with the
     given type. It fills the workerids array with the identifiers of
     the workers that have the type indicated in the first argument.
     The maxsize argument indicates the size of the workids array. The
     returned value gives the number of identifiers that were put in
     the array. `-ERANGE' is returned is maxsize is lower than the
     number of workers with the appropriate type: in that case, the
     array is filled with the maxsize first elements. To avoid such
     overflows, the value of maxsize can be chosen by the means of the
     `starpu_worker_get_count_by_type' function, or by passing a value
     greater or equal to `STARPU_NMAXWORKERS'.

 -- Function: int starpu_worker_get_devid (int ID)
     This functions returns the device id of the given worker. The
     worker should be identified with the value returned by the
     `starpu_worker_get_id' function. In the case of a CUDA worker,
     this device identifier is the logical device identifier exposed by
     CUDA (used by the `cudaGetDevice' function for instance). The
     device identifier of a CPU worker is the logical identifier of the
     core on which the worker was bound; this identifier is either
     provided by the OS or by the `hwloc' library in case it is
     available.

 -- Function: enum starpu_archtype starpu_worker_get_type (int ID)
     This function returns the type of processing unit associated to a
     worker. The worker identifier is a value returned by the
     `starpu_worker_get_id' function). The returned value indicates the
     architecture of the worker: `STARPU_CPU_WORKER' for a CPU core,
     `STARPU_CUDA_WORKER' for a CUDA device, `STARPU_OPENCL_WORKER' for
     a OpenCL device, and `STARPU_GORDON_WORKER' for a Cell SPU. The
     value returned for an invalid identifier is unspecified.

 -- Function: void starpu_worker_get_name (int ID, char *DST, size_t
          MAXLEN)
     This function allows to get the name of a given worker.  StarPU
     associates a unique human readable string to each processing unit.
     This function copies at most the MAXLEN first bytes of the unique
     string associated to a worker identified by its identifier ID into
     the DST buffer. The caller is responsible for ensuring that the DST
     is a valid pointer to a buffer of MAXLEN bytes at least. Calling
     this function on an invalid identifier results in an unspecified
     behaviour.

 -- Function: unsigned starpu_worker_get_memory_node (unsigned WORKERID)
     This function returns the identifier of the memory node associated
     to the worker identified by WORKERID.

 -- Data Type: enum starpu_node_kind
     todo
    `STARPU_UNUSED'

    `STARPU_CPU_RAM'

    `STARPU_CUDA_RAM'

    `STARPU_OPENCL_RAM'

    `STARPU_SPU_LS'

 -- Function: enum starpu_node_kind starpu_node_get_kind (uint32_t NODE)
     Returns the type of the given node as defined by `enum
     starpu_node_kind'. For example, when defining a new data interface,
     this function should be used in the allocation function to
     determine on which device the memory needs to be allocated.


File: starpu.info,  Node: Data Library,  Next: Data Interfaces,  Prev: Workers' Properties,  Up: StarPU Basic API

13.3 Data Library
=================

* Menu:

* Introduction to Data Library::
* Basic Data Library API::
* Access registered data from the application::

   This section describes the data management facilities provided by
StarPU.

   We show how to use existing data interfaces in *note Data
Interfaces::, but developers can design their own data interfaces if
required.


File: starpu.info,  Node: Introduction to Data Library,  Next: Basic Data Library API,  Up: Data Library

13.3.1 Introduction
-------------------

Data management is done at a high-level in StarPU: rather than
accessing a mere list of contiguous buffers, the tasks may manipulate
data that are described by a high-level construct which we call data
interface.

   An example of data interface is the "vector" interface which
describes a contiguous data array on a spefic memory node. This
interface is a simple structure containing the number of elements in
the array, the size of the elements, and the address of the array in
the appropriate address space (this address may be invalid if there is
no valid copy of the array in the memory node). More informations on
the data interfaces provided by StarPU are given in *note Data
Interfaces::.

   When a piece of data managed by StarPU is used by a task, the task
implementation is given a pointer to an interface describing a valid
copy of the data that is accessible from the current processing unit.

   Every worker is associated to a memory node which is a logical
abstraction of the address space from which the processing unit gets
its data. For instance, the memory node associated to the different CPU
workers represents main memory (RAM), the memory node associated to a
GPU is DRAM embedded on the device.  Every memory node is identified by
a logical index which is accessible from the
`starpu_worker_get_memory_node' function. When registering a piece of
data to StarPU, the specified memory node indicates where the piece of
data initially resides (we also call this memory node the home node of
a piece of data).


File: starpu.info,  Node: Basic Data Library API,  Next: Access registered data from the application,  Prev: Introduction to Data Library,  Up: Data Library

13.3.2 Basic Data Library API
-----------------------------

 -- Function: int starpu_malloc (void **A, size_t DIM)
     This function allocates data of the given size in main memory. It
     will also try to pin it in CUDA or OpenCL, so that data transfers
     from this buffer can be asynchronous, and thus permit data
     transfer and computation overlapping. The allocated buffer must be
     freed thanks to the `starpu_free' function.

 -- Function: int starpu_free (void *A)
     This function frees memory which has previously allocated with
     `starpu_malloc'.

 -- Data Type: enum starpu_access_mode
     This datatype describes a data access mode. The different
     available modes are:
    `STARPU_R': read-only mode.

    `STARPU_W': write-only mode.

    `STARPU_RW': read-write mode.
          This is equivalent to `STARPU_R|STARPU_W'.

    `STARPU_SCRATCH': scratch memory.
          A temporary buffer is allocated for the task, but StarPU does
          not enforce data consistency--i.e. each device has its own
          buffer, independently from each other (even for CPUs), and no
          data transfer is ever performed.  This is useful for
          temporary variables to avoid allocating/freeing buffers
          inside each task.

          Currently, no behavior is defined concerning the relation
          with the `STARPU_R' and `STARPU_W' modes and the value
          provided at registration--i.e., the value of the scratch
          buffer is undefined at entry of the codelet function.  It is
          being considered for future extensions at least to define the
          initial value.  For now, data to be used in `SCRATCH' mode
          should be registered with node `-1' and a `NULL' pointer,
          since the value of the provided buffer is simply ignored for
          now.

    `STARPU_REDUX' reduction mode.

 -- Data Type: starpu_data_handle_t
     StarPU uses `starpu_data_handle_t' as an opaque handle to manage a
     piece of data. Once a piece of data has been registered to StarPU,
     it is associated to a `starpu_data_handle_t' which keeps track of
     the state of the piece of data over the entire machine, so that we
     can maintain data consistency and locate data replicates for
     instance.

 -- Function: void starpu_data_register (starpu_data_handle_t
          *HANDLEPTR, uint32_t HOME_NODE, void *DATA_INTERFACE, struct
          starpu_data_interface_ops *OPS)
     Register a piece of data into the handle located at the HANDLEPTR
     address. The DATA_INTERFACE buffer contains the initial
     description of the data in the home node. The OPS argument is a
     pointer to a structure describing the different methods used to
     manipulate this type of interface. See *note struct
     starpu_data_interface_ops:: for more details on this structure.

     If `home_node' is -1, StarPU will automatically allocate the
     memory when it is used for the first time in write-only mode. Once
     such data handle has been automatically allocated, it is possible
     to access it using any access mode.

     Note that StarPU supplies a set of predefined types of interface
     (e.g. vector or matrix) which can be registered by the means of
     helper functions (e.g.  `starpu_vector_data_register' or
     `starpu_matrix_data_register').

 -- Function: void starpu_data_register_same (starpu_data_handle_t
          *HANDLEDST, starpu_data_handle_t HANDLESRC)
     Register a new piece of data into the handle HANDLEDST with the
     same interface as the handle HANDLESRC.

 -- Function: void starpu_data_unregister (starpu_data_handle_t HANDLE)
     This function unregisters a data handle from StarPU. If the data
     was automatically allocated by StarPU because the home node was
     -1, all automatically allocated buffers are freed. Otherwise, a
     valid copy of the data is put back into the home node in the
     buffer that was initially registered.  Using a data handle that
     has been unregistered from StarPU results in an undefined
     behaviour.

 -- Function: void starpu_data_unregister_no_coherency
          (starpu_data_handle_t HANDLE)
     This is the same as starpu_data_unregister, except that StarPU
     does not put back a valid copy into the home node, in the buffer
     that was initially registered.

 -- Function: void starpu_data_invalidate (starpu_data_handle_t HANDLE)
     Destroy all replicates of the data handle. After data
     invalidation, the first access to the handle must be performed in
     write-only mode. Accessing an invalidated data in read-mode
     results in undefined behaviour.

 -- Function: void starpu_data_set_wt_mask (starpu_data_handle_t
          HANDLE, uint32_t WT_MASK)
     This function sets the write-through mask of a given data, i.e. a
     bitmask of nodes where the data should be always replicated after
     modification. It also prevents the data from being evicted from
     these nodes when memory gets scarse.

 -- Function: int starpu_data_prefetch_on_node (starpu_data_handle_t
          HANDLE, unsigned NODE, unsigned ASYNC)
     Issue a prefetch request for a given data to a given node, i.e.
     requests that the data be replicated to the given node, so that it
     is available there for tasks. If the ASYNC parameter is 0, the
     call will block until the transfer is achieved, else the call will
     return as soon as the request is scheduled (which may however have
     to wait for a task completion).

 -- Function: starpu_data_handle_t starpu_data_lookup (const void *PTR)
     Return the handle corresponding to the data pointed to by the PTR
     host pointer.

 -- Function: int starpu_data_request_allocation (starpu_data_handle_t
          HANDLE, uint32_t NODE)
     Explicitly ask StarPU to allocate room for a piece of data on the
     specified memory node.

 -- Function: void starpu_data_query_status (starpu_data_handle_t
          HANDLE, int MEMORY_NODE, int *IS_ALLOCATED, int *IS_VALID,
          int *IS_REQUESTED)
     Query the status of the handle on the specified memory node.

 -- Function: void starpu_data_advise_as_important
          (starpu_data_handle_t HANDLE, unsigned IS_IMPORTANT)
     This function allows to specify that a piece of data can be
     discarded without impacting the application.

 -- Function: void starpu_data_set_reduction_methods
          (starpu_data_handle_t HANDLE, struct starpu_codelet
          *REDUX_CL, struct starpu_codelet *INIT_CL)
     This sets the codelets to be used for the HANDLE when it is
     accessed in REDUX mode. Per-worker buffers will be initialized
     with the INIT_CL codelet, and reduction between per-worker buffers
     will be done with the REDUX_CL codelet.


File: starpu.info,  Node: Access registered data from the application,  Prev: Basic Data Library API,  Up: Data Library

13.3.3 Access registered data from the application
--------------------------------------------------

 -- Function: int starpu_data_acquire (starpu_data_handle_t HANDLE,
          enum starpu_access_mode MODE)
     The application must call this function prior to accessing
     registered data from main memory outside tasks. StarPU ensures
     that the application will get an up-to-date copy of the data in
     main memory located where the data was originally registered, and
     that all concurrent accesses (e.g. from tasks) will be consistent
     with the access mode specified in the MODE argument.
     `starpu_data_release' must be called once the application does not
     need to access the piece of data anymore.  Note that implicit data
     dependencies are also enforced by `starpu_data_acquire', i.e.
     `starpu_data_acquire' will wait for all tasks scheduled to work on
     the data, unless that they have not been disabled explictly by
     calling `starpu_data_set_default_sequential_consistency_flag' or
     `starpu_data_set_sequential_consistency_flag'.
     `starpu_data_acquire' is a blocking call, so that it cannot be
     called from tasks or from their callbacks (in that case,
     `starpu_data_acquire' returns `-EDEADLK'). Upon successful
     completion, this function returns 0.

 -- Function: int starpu_data_acquire_cb (starpu_data_handle_t HANDLE,
          enum starpu_access_mode MODE, void (*CALLBACK)(void *), void
          *ARG)
     `starpu_data_acquire_cb' is the asynchronous equivalent of
     `starpu_data_release'. When the data specified in the first
     argument is available in the appropriate access mode, the callback
     function is executed.  The application may access the requested
     data during the execution of this callback. The callback function
     must call `starpu_data_release' once the application does not need
     to access the piece of data anymore.  Note that implicit data
     dependencies are also enforced by `starpu_data_acquire_cb' in case
     they are enabled.   Contrary to `starpu_data_acquire', this
     function is non-blocking and may be called from task callbacks.
     Upon successful completion, this function returns 0.

 -- Macro: STARPU_DATA_ACQUIRE_CB (starpu_data_handle_t HANDLE, enum
          starpu_access_mode MODE, code)
     `STARPU_DATA_ACQUIRE_CB' is the same as `starpu_data_acquire_cb',
     except that the code to be executed in a callback is directly
     provided as a macro parameter, and the data handle is
     automatically released after it. This permits to easily execute
     code which depends on the value of some registered data. This is
     non-blocking too and may be called from task callbacks.

 -- Function: void starpu_data_release (starpu_data_handle_t HANDLE)
     This function releases the piece of data acquired by the
     application either by `starpu_data_acquire' or by
     `starpu_data_acquire_cb'.


File: starpu.info,  Node: Data Interfaces,  Next: Data Partition,  Prev: Data Library,  Up: StarPU Basic API

13.4 Data Interfaces
====================

* Menu:

* Registering Data::
* Accessing Data Interfaces::


File: starpu.info,  Node: Registering Data,  Next: Accessing Data Interfaces,  Up: Data Interfaces

13.4.1 Registering Data
-----------------------

There are several ways to register a memory region so that it can be
managed by StarPU.  The functions below allow the registration of
vectors, 2D matrices, 3D matrices as well as  BCSR and CSR sparse
matrices.

 -- Function: void starpu_void_data_register (starpu_data_handle_t
          *HANDLE)
     Register a void interface. There is no data really associated to
     that interface, but it may be used as a synchronization mechanism.
     It also permits to express an abstract piece of data that is
     managed by the application internally: this makes it possible to
     forbid the concurrent execution of different tasks accessing the
     same "void" data in read-write concurrently.

 -- Function: void starpu_variable_data_register (starpu_data_handle_t
          *HANDLE, uint32_t HOME_NODE, uintptr_t PTR, size_t SIZE)
     Register the SIZE-byte element pointed to by PTR, which is
     typically a scalar, and initialize HANDLE to represent this data
     item.

          float var;
          starpu_data_handle_t var_handle;
          starpu_variable_data_register(&var_handle, 0, (uintptr_t)&var, sizeof(var));

 -- Function: void starpu_vector_data_register (starpu_data_handle_t
          *HANDLE, uint32_t HOME_NODE, uintptr_t PTR, uint32_t NX,
          size_t ELEMSIZE)
     Register the NX ELEMSIZE-byte elements pointed to by PTR and
     initialize HANDLE to represent it.

          float vector[NX];
          starpu_data_handle_t vector_handle;
          starpu_vector_data_register(&vector_handle, 0, (uintptr_t)vector, NX,
                                      sizeof(vector[0]));

 -- Function: void starpu_matrix_data_register (starpu_data_handle_t
          *HANDLE, uint32_t HOME_NODE, uintptr_t PTR, uint32_t LD,
          uint32_t NX, uint32_t NY, size_t ELEMSIZE)
     Register the NXxNY 2D matrix of ELEMSIZE-byte elements pointed by
     PTR and initialize HANDLE to represent it.  LD specifies the
     number of elements between rows.  a value greater than NX adds
     padding, which can be useful for alignment purposes.

          float *matrix;
          starpu_data_handle_t matrix_handle;
          matrix = (float*)malloc(width * height * sizeof(float));
          starpu_matrix_data_register(&matrix_handle, 0, (uintptr_t)matrix,
                                      width, width, height, sizeof(float));

 -- Function: void starpu_block_data_register (starpu_data_handle_t
          *HANDLE, uint32_t HOME_NODE, uintptr_t PTR, uint32_t LDY,
          uint32_t LDZ, uint32_t NX, uint32_t NY, uint32_t NZ, size_t
          ELEMSIZE)
     Register the NXxNYxNZ 3D matrix of ELEMSIZE-byte elements pointed
     by PTR and initialize HANDLE to represent it.  Again, LDY and LDZ
     specify the number of elements between rows and between z planes.

          float *block;
          starpu_data_handle_t block_handle;
          block = (float*)malloc(nx*ny*nz*sizeof(float));
          starpu_block_data_register(&block_handle, 0, (uintptr_t)block,
                                     nx, nx*ny, nx, ny, nz, sizeof(float));

 -- Function: void starpu_bcsr_data_register (starpu_data_handle_t
          *HANDLE, uint32_t HOME_NODE, uint32_t NNZ, uint32_t NROW,
          uintptr_t NZVAL, uint32_t *COLIND, uint32_t *ROWPTR, uint32_t
          FIRSTENTRY, uint32_t R, uint32_t C, size_t ELEMSIZE)
     This variant of `starpu_data_register' uses the BCSR (Blocked
     Compressed Sparse Row Representation) sparse matrix interface.
     Register the sparse matrix made of NNZ non-zero values of size
     ELEMSIZE stored in NZVAL and initializes HANDLE to represent it.
     Blocks have size R * C. NROW is the number of rows (in terms of
     blocks), COLIND is the list of positions of the non-zero entries
     on the row, ROWPTR is the index (in nzval) of the first entry of
     the row.  FRISTENTRY is the index of the first entry of the given
     arrays (usually 0 or 1).

 -- Function: void starpu_csr_data_register (starpu_data_handle_t
          *HANDLE, uint32_t HOME_NODE, uint32_t NNZ, uint32_t NROW,
          uintptr_t NZVAL, uint32_t *COLIND, uint32_t *ROWPTR, uint32_t
          FIRSTENTRY, size_t ELEMSIZE)
     This variant of `starpu_data_register' uses the CSR (Compressed
     Sparse Row Representation) sparse matrix interface.  TODO

 -- Function: void * starpu_data_get_interface_on_node
          (starpu_data_handle_t HANDLE, unsigned MEMORY_NODE)
     Return the interface associated with HANDLE on MEMORY_NODE.


File: starpu.info,  Node: Accessing Data Interfaces,  Prev: Registering Data,  Up: Data Interfaces

13.4.2 Accessing Data Interfaces
--------------------------------

Each data interface is provided with a set of field access functions.
The ones using a `void *' parameter aimed to be used in codelet
implementations (see for example the code in *note Vector Scaling Using
StarPu's API::).

 -- Data Type: enum starpu_data_interface_id
     The different values are:
    `STARPU_MATRIX_INTERFACE_ID'

    `STARPU_BLOCK_INTERFACE_ID'

    `STARPU_VECTOR_INTERFACE_ID'

    `STARPU_CSR_INTERFACE_ID'

    `STARPU_BCSR_INTERFACE_ID'

    `STARPU_VARIABLE_INTERFACE_ID'

    `STARPU_VOID_INTERFACE_ID'

    `STARPU_MULTIFORMAT_INTERFACE_ID'

    `STARPU_NINTERFACES_ID': number of data interfaces

* Menu:

* Accessing Handle::
* Accessing Variable Data Interfaces::
* Accessing Vector Data Interfaces::
* Accessing Matrix Data Interfaces::
* Accessing Block Data Interfaces::
* Accessing BCSR Data Interfaces::
* Accessing CSR Data Interfaces::


File: starpu.info,  Node: Accessing Handle,  Next: Accessing Variable Data Interfaces,  Up: Accessing Data Interfaces

13.4.2.1 Handle
...............

 -- Function: void * starpu_handle_to_pointer (starpu_data_handle_t
          HANDLE, uint32_t NODE)
     Return the pointer associated with HANDLE on node NODE or `NULL'
     if HANDLE's interface does not support this operation or data for
     this handle is not allocated on that node.

 -- Function: void * starpu_handle_get_local_ptr (starpu_data_handle_t
          HANDLE)
     Return the local pointer associated with HANDLE or `NULL' if
     HANDLE's interface does not have data allocated locally

 -- Function: enum starpu_data_interface_id
starpu_handle_get_interface_id (starpu_data_handle_t HANDLE)
     Return the unique identifier of the interface associated with the
     given HANDLE.


File: starpu.info,  Node: Accessing Variable Data Interfaces,  Next: Accessing Vector Data Interfaces,  Prev: Accessing Handle,  Up: Accessing Data Interfaces

13.4.2.2 Variable Data Interfaces
.................................

 -- Function: size_t starpu_variable_get_elemsize (starpu_data_handle_t
          HANDLE)
     Return the size of the variable designated by HANDLE.

 -- Function: uintptr_t starpu_variable_get_local_ptr
          (starpu_data_handle_t HANDLE)
     Return a pointer to the variable designated by HANDLE.

 -- Macro: STARPU_VARIABLE_GET_PTR (void *INTERFACE)
     Return a pointer to the variable designated by INTERFACE.

 -- Macro: STARPU_VARIABLE_GET_ELEMSIZE (void *INTERFACE)
     Return the size of the variable designated by INTERFACE.


File: starpu.info,  Node: Accessing Vector Data Interfaces,  Next: Accessing Matrix Data Interfaces,  Prev: Accessing Variable Data Interfaces,  Up: Accessing Data Interfaces

13.4.2.3 Vector Data Interfaces
...............................

 -- Function: uint32_t starpu_vector_get_nx (starpu_data_handle_t
          HANDLE)
     Return the number of elements registered into the array designated
     by HANDLE.

 -- Function: size_t starpu_vector_get_elemsize (starpu_data_handle_t
          HANDLE)
     Return the size of each element of the array designated by HANDLE.

 -- Function: uintptr_t starpu_vector_get_local_ptr
          (starpu_data_handle_t HANDLE)
     Return the local pointer associated with HANDLE.

 -- Macro: STARPU_VECTOR_GET_PTR (void *INTERFACE)
     Return a pointer to the array designated by INTERFACE, valid on
     CPUs and CUDA only. For OpenCL, the device handle and offset need
     to be used instead.

 -- Macro: STARPU_VECTOR_GET_DEV_HANDLE (void *INTERFACE)
     Return a device handle for the array designated by INTERFACE, to
     be used on OpenCL. the offset documented below has to be used in
     addition to this.

 -- Macro: STARPU_VECTOR_GET_OFFSET (void *INTERFACE)
     Return the offset in the array designated by INTERFACE, to be used
     with the device handle.

 -- Macro: STARPU_VECTOR_GET_NX (void *INTERFACE)
     Return the number of elements registered into the array designated
     by INTERFACE.

 -- Macro: STARPU_VECTOR_GET_ELEMSIZE (void *INTERFACE)
     Return the size of each element of the array designated by
     INTERFACE.


File: starpu.info,  Node: Accessing Matrix Data Interfaces,  Next: Accessing Block Data Interfaces,  Prev: Accessing Vector Data Interfaces,  Up: Accessing Data Interfaces

13.4.2.4 Matrix Data Interfaces
...............................

 -- Function: uint32_t starpu_matrix_get_nx (starpu_data_handle_t
          HANDLE)
     Return the number of elements on the x-axis of the matrix
     designated by HANDLE.

 -- Function: uint32_t starpu_matrix_get_ny (starpu_data_handle_t
          HANDLE)
     Return the number of elements on the y-axis of the matrix
     designated by HANDLE.

 -- Function: uint32_t starpu_matrix_get_local_ld (starpu_data_handle_t
          HANDLE)
     Return the number of elements between each row of the matrix
     designated by HANDLE. Maybe be equal to nx when there is no
     padding.

 -- Function: uintptr_t starpu_matrix_get_local_ptr
          (starpu_data_handle_t HANDLE)
     Return the local pointer associated with HANDLE.

 -- Function: size_t starpu_matrix_get_elemsize (starpu_data_handle_t
          HANDLE)
     Return the size of the elements registered into the matrix
     designated by HANDLE.

 -- Macro: STARPU_MATRIX_GET_PTR (void *INTERFACE)
     Return a pointer to the matrix designated by INTERFACE, valid on
     CPUs and CUDA devices only. For OpenCL devices, the device handle
     and offset need to be used instead.

 -- Macro: STARPU_MATRIX_GET_DEV_HANDLE (void *INTERFACE)
     Return a device handle for the matrix designated by INTERFACE, to
     be used on OpenCL. The offset documented below has to be used in
     addition to this.

 -- Macro: STARPU_MATRIX_GET_OFFSET (void *INTERFACE)
     Return the offset in the matrix designated by INTERFACE, to be
     used with the device handle.

 -- Macro: STARPU_MATRIX_GET_NX (void *INTERFACE)
     Return the number of elements on the x-axis of the matrix
     designated by INTERFACE.

 -- Macro: STARPU_MATRIX_GET_NY (void *INTERFACE)
     Return the number of elements on the y-axis of the matrix
     designated by INTERFACE.

 -- Macro: STARPU_MATRIX_GET_LD (void *INTERFACE)
     Return the number of elements between each row of the matrix
     designated by INTERFACE. May be equal to nx when there is no
     padding.

 -- Macro: STARPU_MATRIX_GET_ELEMSIZE (void *INTERFACE)
     Return the size of the elements registered into the matrix
     designated by INTERFACE.


File: starpu.info,  Node: Accessing Block Data Interfaces,  Next: Accessing BCSR Data Interfaces,  Prev: Accessing Matrix Data Interfaces,  Up: Accessing Data Interfaces

13.4.2.5 Block Data Interfaces
..............................

 -- Function: uint32_t starpu_block_get_nx (starpu_data_handle_t HANDLE)
     Return the number of elements on the x-axis of the block
     designated by HANDLE.

 -- Function: uint32_t starpu_block_get_ny (starpu_data_handle_t HANDLE)
     Return the number of elements on the y-axis of the block
     designated by HANDLE.

 -- Function: uint32_t starpu_block_get_nz (starpu_data_handle_t HANDLE)
     Return the number of elements on the z-axis of the block
     designated by HANDLE.

 -- Function: uint32_t starpu_block_get_local_ldy (starpu_data_handle_t
          HANDLE)
     Return the number of elements between each row of the block
     designated by HANDLE, in the format of the current memory node.

 -- Function: uint32_t starpu_block_get_local_ldz (starpu_data_handle_t
          HANDLE)
     Return the number of elements between each z plane of the block
     designated by HANDLE, in the format of the current memory node.

 -- Function: uintptr_t starpu_block_get_local_ptr
          (starpu_data_handle_t HANDLE)
     Return the local pointer associated with HANDLE.

 -- Function: size_t starpu_block_get_elemsize (starpu_data_handle_t
          HANDLE)
     Return the size of the elements of the block designated by HANDLE.

 -- Macro: STARPU_BLOCK_GET_PTR (void *INTERFACE)
     Return a pointer to the block designated by INTERFACE.

 -- Macro: STARPU_BLOCK_GET_DEV_HANDLE (void *INTERFACE)
     Return a device handle for the block designated by INTERFACE, to
     be used on OpenCL. The offset document below has to be used in
     addition to this.

 -- Macro: STARPU_BLOCK_GET_OFFSET (void *INTERFACE)
     Return the offset in the block designated by INTERFACE, to be used
     with the device handle.

 -- Macro: STARPU_BLOCK_GET_NX (void *INTERFACE)
     Return the number of elements on the x-axis of the block
     designated by HANDLE.

 -- Macro: STARPU_BLOCK_GET_NY (void *INTERFACE)
     Return the number of elements on the y-axis of the block
     designated by HANDLE.

 -- Macro: STARPU_BLOCK_GET_NZ (void *INTERFACE)
     Return the number of elements on the z-axis of the block
     designated by HANDLE.

 -- Macro: STARPU_BLOCK_GET_LDY (void *INTERFACE)
     Return the number of elements between each row of the block
     designated by INTERFACE. May be equal to nx when there is no
     padding.

 -- Macro: STARPU_BLOCK_GET_LDZ (void *INTERFACE)
     Return the number of elements between each z plane of the block
     designated by INTERFACE. May be equal to nx*ny when there is no
     padding.

 -- Macro: STARPU_BLOCK_GET_ELEMSIZE (void *INTERFACE)
     Return the size of the elements of the matrix designated by
     INTERFACE.


File: starpu.info,  Node: Accessing BCSR Data Interfaces,  Next: Accessing CSR Data Interfaces,  Prev: Accessing Block Data Interfaces,  Up: Accessing Data Interfaces

13.4.2.6 BCSR Data Interfaces
.............................

 -- Function: uint32_t starpu_bcsr_get_nnz (starpu_data_handle_t HANDLE)
     Return the number of non-zero elements in the matrix designated by
     HANDLE.

 -- Function: uint32_t starpu_bcsr_get_nrow (starpu_data_handle_t
          HANDLE)
     Return the number of rows (in terms of blocks of size r*c) in the
     matrix designated by HANDLE.

 -- Function: uint32_t starpu_bcsr_get_firstentry (starpu_data_handle_t
          HANDLE)
     Return the index at which all arrays (the column indexes, the row
     pointers...)  of the matrix desginated by HANDLE start.

 -- Function: uintptr_t starpu_bcsr_get_local_nzval
          (starpu_data_handle_t HANDLE)
     Return a pointer to the non-zero values of the matrix designated
     by HANDLE.

 -- Function: uint32_t * starpu_bcsr_get_local_colind
          (starpu_data_handle_t HANDLE)
     Return a pointer to the column index, which holds the positions of
     the non-zero entries in the matrix designated by HANDLE.

 -- Function: uint32_t * starpu_bcsr_get_local_rowptr
          (starpu_data_handle_t HANDLE)
     Return the row pointer array of the matrix designated by HANDLE.

 -- Function: uint32_t starpu_bcsr_get_r (starpu_data_handle_t HANDLE)
     Return the number of rows in a block.

 -- Function: uint32_t starpu_bcsr_get_c (starpu_data_handle_t HANDLE)
     Return the numberof columns in a block.

 -- Function: size_t starpu_bcsr_get_elemsize (starpu_data_handle_t
          HANDLE)
     Return the size of the elements in the matrix designated by HANDLE.

 -- Macro: STARPU_BCSR_GET_NNZ (void *INTERFACE)
     Return the number of non-zero values in the matrix designated by
     INTERFACE.

 -- Macro: STARPU_BCSR_GET_NZVAL (void *INTERFACE)
     Return a pointer to the non-zero values of the matrix designated
     by INTERFACE.

 -- Macro: STARPU_BCSR_GET_COLIND (void *INTERFACE)
     Return a pointer to the column index of the matrix designated by
     INTERFACE.

 -- Macro: STARPU_BCSR_GET_ROWPTR (void *INTERFACE)
     Return a pointer to the row pointer array of the matrix designated
     by INTERFACE.


File: starpu.info,  Node: Accessing CSR Data Interfaces,  Prev: Accessing BCSR Data Interfaces,  Up: Accessing Data Interfaces

13.4.2.7 CSR Data Interfaces
............................

 -- Function: uint32_t starpu_csr_get_nnz (starpu_data_handle_t HANDLE)
     Return the number of non-zero values in the matrix designated by
     HANDLE.

 -- Function: uint32_t starpu_csr_get_nrow (starpu_data_handle_t HANDLE)
     Return the size of the row pointer array of the matrix designated
     by HANDLE.

 -- Function: uint32_t starpu_csr_get_firstentry (starpu_data_handle_t
          HANDLE)
     Return the index at which all arrays (the column indexes, the row
     pointers...)  of the matrix designated by HANDLE start.

 -- Function: uintptr_t starpu_csr_get_local_nzval
          (starpu_data_handle_t HANDLE)
     Return a local pointer to the non-zero values of the matrix
     designated by HANDLE.

 -- Function: uint32_t * starpu_csr_get_local_colind
          (starpu_data_handle_t HANDLE)
     Return a local pointer to the column index of the matrix
     designated by HANDLE.

 -- Function: uint32_t * starpu_csr_get_local_rowptr
          (starpu_data_handle_t HANDLE)
     Return a local pointer to the row pointer array of the matrix
     designated by HANDLE.

 -- Function: size_t starpu_csr_get_elemsize (starpu_data_handle_t
          HANDLE)
     Return the size of the elements registered into the matrix
     designated by HANDLE.

 -- Macro: STARPU_CSR_GET_NNZ (void *INTERFACE)
     Return the number of non-zero values in the matrix designated by
     INTERFACE.

 -- Macro: STARPU_CSR_GET_NROW (void *INTERFACE)
     Return the size of the row pointer array of the matrix designated
     by INTERFACE.

 -- Macro: STARPU_CSR_GET_NZVAL (void *INTERFACE)
     Return a pointer to the non-zero values of the matrix designated
     by INTERFACE.

 -- Macro: STARPU_CSR_GET_COLIND (void *INTERFACE)
     Return a pointer to the column index of the matrix designated by
     INTERFACE.

 -- Macro: STARPU_CSR_GET_ROWPTR (void *INTERFACE)
     Return a pointer to the row pointer array of the matrix designated
     by INTERFACE.

 -- Macro: STARPU_CSR_GET_FIRSTENTRY (void *INTERFACE)
     Return the index at which all arrays (the column indexes, the row
     pointers...)  of the INTERFACE start.

 -- Macro: STARPU_CSR_GET_ELEMSIZE (void *INTERFACE)
     Return the size of the elements registered into the matrix
     designated by INTERFACE.


File: starpu.info,  Node: Data Partition,  Next: Codelets and Tasks,  Prev: Data Interfaces,  Up: StarPU Basic API

13.5 Data Partition
===================

* Menu:

* Basic API::
* Predefined filter functions::


File: starpu.info,  Node: Basic API,  Next: Predefined filter functions,  Up: Data Partition

13.5.1 Basic API
----------------

 -- Data Type: struct starpu_data_filter
     The filter structure describes a data partitioning operation, to
     be given to the `starpu_data_partition' function, see *note
     starpu_data_partition:: for an example. The different fields are:

    `void (*filter_func)(void *father_interface, void* child_interface, struct starpu_data_filter *, unsigned id, unsigned nparts)'
          This function fills the `child_interface' structure with
          interface information for the `id'-th child of the parent
          `father_interface' (among `nparts').

    `unsigned nchildren'
          This is the number of parts to partition the data into.

    `unsigned (*get_nchildren)(struct starpu_data_filter *, starpu_data_handle_t initial_handle)'
          This returns the number of children. This can be used instead
          of `nchildren' when the number of children depends on the
          actual data (e.g. the number of blocks in a sparse matrix).

    `struct starpu_data_interface_ops *(*get_child_ops)(struct starpu_data_filter *, unsigned id)'
          In case the resulting children use a different data
          interface, this function returns which interface is used by
          child number `id'.

    `unsigned filter_arg'
          Allow to define an additional parameter for the filter
          function.

    `void *filter_arg_ptr'
          Allow to define an additional pointer parameter for the filter
          function, such as the sizes of the different parts.

 -- Function: void starpu_data_partition (starpu_data_handle_t
          INITIAL_HANDLE, struct starpu_data_filter *F)
     This requests partitioning one StarPU data INITIAL_HANDLE into
     several subdata according to the filter F, as shown in the
     following example:

          struct starpu_data_filter f = {
              .filter_func = starpu_vertical_block_filter_func,
              .nchildren = nslicesx,
              .get_nchildren = NULL,
              .get_child_ops = NULL
          };
          starpu_data_partition(A_handle, &f);

 -- Function: void starpu_data_unpartition (starpu_data_handle_t
          ROOT_DATA, uint32_t GATHERING_NODE)
     This unapplies one filter, thus unpartitioning the data. The
     pieces of data are collected back into one big piece in the
     GATHERING_NODE (usually 0).
          starpu_data_unpartition(A_handle, 0);

 -- Function: int starpu_data_get_nb_children (starpu_data_handle_t
          HANDLE)
     This function returns the number of children.

 -- Function: starpu_data_handle_t starpu_data_get_child
          (starpu_data_handle_t HANDLE, unsigned I)
     Return the Ith child of the given HANDLE, which must have been
     partitionned beforehand.

 -- Function: starpu_data_handle_t starpu_data_get_sub_data
          (starpu_data_handle_t ROOT_DATA, unsigned DEPTH, ... )
     After partitioning a StarPU data by applying a filter,
     `starpu_data_get_sub_data' can be used to get handles for each of
     the data portions. ROOT_DATA is the parent data that was
     partitioned. DEPTH is the number of filters to traverse (in case
     several filters have been applied, to e.g. partition in row
     blocks, and then in column blocks), and the subsequent parameters
     are the indexes. The function returns a handle to the subdata.
          h = starpu_data_get_sub_data(A_handle, 1, taskx);

 -- Function: starpu_data_handle_t starpu_data_vget_sub_data
          (starpu_data_handle_t ROOT_DATA, unsigned DEPTH, va_list PA)
     This function is similar to `starpu_data_get_sub_data' but uses a
     va_list for the parameter list.

 -- Function: void starpu_data_map_filters (starpu_data_handle_t
          ROOT_DATA, unsigned NFILTERS, ...)
     Applies NFILTERS filters to the handle designated by ROOT_HANDLE
     recursively. NFILTERS pointers to variables of the type
     starpu_data_filter should be given.

 -- Function: void starpu_data_vmap_filters (starpu_data_handle_t
          ROOT_DATA, unsigned NFILTERS, va_list PA)
     Applies NFILTERS filters to the handle designated by ROOT_HANDLE
     recursively. It uses a va_list of pointers to variables of the
     typer starpu_data_filter.


File: starpu.info,  Node: Predefined filter functions,  Prev: Basic API,  Up: Data Partition

13.5.2 Predefined filter functions
----------------------------------

* Menu:

* Partitioning BCSR Data::
* Partitioning BLAS interface::
* Partitioning Vector Data::
* Partitioning Block Data::

   This section gives a partial list of the predefined partitioning
functions.  Examples on how to use them are shown in *note Partitioning
Data::. The complete list can be found in `starpu_data_filters.h' .


File: starpu.info,  Node: Partitioning BCSR Data,  Next: Partitioning BLAS interface,  Up: Predefined filter functions

13.5.2.1 Partitioning BCSR Data
...............................

 -- Function: void starpu_canonical_block_filter_bcsr (void
          *FATHER_INTERFACE, void *CHILD_INTERFACE, struct
          starpu_data_filter *F, unsigned ID, unsigned NPARTS)
     This partitions a block-sparse matrix into dense matrices.

 -- Function: void starpu_vertical_block_filter_func_csr (void
          *FATHER_INTERFACE, void *CHILD_INTERFACE, struct
          starpu_data_filter *F, unsigned ID, unsigned NPARTS)
     This partitions a block-sparse matrix into vertical block-sparse
     matrices.


File: starpu.info,  Node: Partitioning BLAS interface,  Next: Partitioning Vector Data,  Prev: Partitioning BCSR Data,  Up: Predefined filter functions

13.5.2.2 Partitioning BLAS interface
....................................

 -- Function: void starpu_block_filter_func (void *FATHER_INTERFACE,
          void *CHILD_INTERFACE, struct starpu_data_filter *F, unsigned
          ID, unsigned NPARTS)
     This partitions a dense Matrix into horizontal blocks.

 -- Function: void starpu_vertical_block_filter_func (void
          *FATHER_INTERFACE, void *CHILD_INTERFACE, struct
          starpu_data_filter *F, unsigned ID, unsigned NPARTS)
     This partitions a dense Matrix into vertical blocks.


File: starpu.info,  Node: Partitioning Vector Data,  Next: Partitioning Block Data,  Prev: Partitioning BLAS interface,  Up: Predefined filter functions

13.5.2.3 Partitioning Vector Data
.................................

 -- Function: void starpu_block_filter_func_vector (void
          *FATHER_INTERFACE, void *CHILD_INTERFACE, struct
          starpu_data_filter *F, unsigned ID, unsigned NPARTS)
     Return in `*CHILD_INTERFACE' the IDth element of the vector
     represented by FATHER_INTERFACE once partitioned in NPARTS chunks
     of equal size.

 -- Function: void starpu_vector_list_filter_func (void
          *FATHER_INTERFACE, void *CHILD_INTERFACE, struct
          starpu_data_filter *F, unsigned ID, unsigned NPARTS)
     Return in `*CHILD_INTERFACE' the IDth element of the vector
     represented by FATHER_INTERFACE once partitioned into NPARTS
     chunks according to the `filter_arg_ptr' field of `*F'.

     The `filter_arg_ptr' field must point to an array of NPARTS
     `uint32_t' elements, each of which specifies the number of elements
     in each chunk of the partition.

 -- Function: void starpu_vector_divide_in_2_filter_func (void
          *FATHER_INTERFACE, void *CHILD_INTERFACE, struct
          starpu_data_filter *F, unsigned ID, unsigned NPARTS)
     Return in `*CHILD_INTERFACE' the IDth element of the vector
     represented by FATHER_INTERFACE once partitioned in two chunks of
     equal size, ignoring NPARTS.  Thus, ID must be `0' or `1'.


File: starpu.info,  Node: Partitioning Block Data,  Prev: Partitioning Vector Data,  Up: Predefined filter functions

13.5.2.4 Partitioning Block Data
................................

 -- Function: void starpu_block_filter_func_block (void
          *FATHER_INTERFACE, void *CHILD_INTERFACE, struct
          starpu_data_filter *F, unsigned ID, unsigned NPARTS)
     This partitions a 3D matrix along the X axis.


File: starpu.info,  Node: Codelets and Tasks,  Next: Explicit Dependencies,  Prev: Data Partition,  Up: StarPU Basic API

13.6 Codelets and Tasks
=======================

This section describes the interface to manipulate codelets and tasks.

 -- Data Type: enum starpu_codelet_type
     Describes the type of parallel task. The different values are:
    `STARPU_SEQ' (default) for classical sequential tasks.

    `STARPU_SPMD' for a parallel task whose threads are handled by
          StarPU, the code has to use `starpu_combined_worker_get_size'
          and `starpu_combined_worker_get_rank' to distribute the work

    `STARPU_FORKJOIN' for a parallel task whose threads are started by
          the codelet function, which has to use
          `starpu_combined_worker_get_size' to determine how many
          threads should be started.
     See *note Parallel Tasks:: for details.

 -- Macro: STARPU_CPU
     This macro is used when setting the field `where' of a `struct
     starpu_codelet' to specify the codelet may be executed on a CPU
     processing unit.

 -- Macro: STARPU_CUDA
     This macro is used when setting the field `where' of a `struct
     starpu_codelet' to specify the codelet may be executed on a CUDA
     processing unit.

 -- Macro: STARPU_SPU
     This macro is used when setting the field `where' of a `struct
     starpu_codelet' to specify the codelet may be executed on a SPU
     processing unit.

 -- Macro: STARPU_GORDON
     This macro is used when setting the field `where' of a `struct
     starpu_codelet' to specify the codelet may be executed on a Cell
     processing unit.

 -- Macro: STARPU_OPENCL
     This macro is used when setting the field `where' of a `struct
     starpu_codelet' to specify the codelet may be executed on a OpenCL
     processing unit.

 -- Macro: STARPU_MULTIPLE_CPU_IMPLEMENTATIONS
     Setting the field `cpu_func' of a `struct starpu_codelet' with
     this macro indicates the codelet will have several
     implementations. The use of this macro is deprecated. One should
     always only define the field `cpu_funcs'.

 -- Macro: STARPU_MULTIPLE_CUDA_IMPLEMENTATIONS
     Setting the field `cuda_func' of a `struct starpu_codelet' with
     this macro indicates the codelet will have several
     implementations. The use of this macro is deprecated. One should
     always only define the field `cuda_funcs'.

 -- Macro: STARPU_MULTIPLE_OPENCL_IMPLEMENTATIONS
     Setting the field `opencl_func' of a `struct starpu_codelet' with
     this macro indicates the codelet will have several
     implementations. The use of this macro is deprecated. One should
     always only define the field `opencl_funcs'.

 -- Data Type: struct starpu_codelet
     The codelet structure describes a kernel that is possibly
     implemented on various targets. For compatibility, make sure to
     initialize the whole structure to zero.

    `uint32_t where' (optional)
          Indicates which types of processing units are able to execute
          the codelet. The different values `STARPU_CPU',
          `STARPU_CUDA', `STARPU_SPU', `STARPU_GORDON', `STARPU_OPENCL'
          can be combined to specify on which types of processing units
          the codelet can be executed.  `STARPU_CPU|STARPU_CUDA' for
          instance indicates that the codelet is implemented for both
          CPU cores and CUDA devices while `STARPU_GORDON' indicates
          that it is only available on Cell SPUs. If the field is
          unset, its value will be automatically set based on the
          availability of the `XXX_funcs' fields defined below.

    `int (*can_execute)(unsigned workerid, struct starpu_task *task, unsigned nimpl)' (optional)
          Defines a function which should return 1 if the worker
          designated by WORKERID can execute the NIMPLth implementation
          of the givenTASK, 0 otherwise.

    `enum starpu_codelet_type type' (optional)
          The default is `STARPU_SEQ', i.e. usual sequential
          implementation. Other values (`STARPU_SPMD' or
          `STARPU_FORKJOIN' declare that a parallel implementation is
          also available. See *note Parallel Tasks:: for details.

    `int max_parallelism' (optional)
          If a parallel implementation is available, this denotes the
          maximum combined worker size that StarPU will use to execute
          parallel tasks for this codelet.

    `starpu_cpu_func_t cpu_func' (optional)
          This field has been made deprecated. One should use instead
          the `cpu_funcs' field.

    `starpu_cpu_func_t cpu_funcs[STARPU_MAXIMPLEMENTATIONS]' (optional)
          Is an array of function pointers to the CPU implementations
          of the codelet.  It must be terminated by a NULL value.  The
          functions prototype must be: `void cpu_func(void *buffers[],
          void *cl_arg)'. The first argument being the array of data
          managed by the data management library, and the second
          argument is a pointer to the argument passed from the `cl_arg'
          field of the `starpu_task' structure.  If the `where' field
          is set, then the `cpu_funcs' field is ignored if `STARPU_CPU'
          does not appear in the `where' field, it must be non-null
          otherwise.

    `starpu_cuda_func_t cuda_func' (optional)
          This field has been made deprecated. One should use instead
          the `cuda_funcs' field.

    `starpu_cuda_func_t cuda_funcs[STARPU_MAXIMPLEMENTATIONS]' (optional)
          Is an array of function pointers to the CUDA implementations
          of the codelet.  It must be terminated by a NULL value.  _The
          functions must be host-functions written in the CUDA runtime
          API_. Their prototype must be: `void cuda_func(void
          *buffers[], void *cl_arg);'.  If the `where' field is set,
          then the `cuda_funcs' field is ignored if `STARPU_CUDA' does
          not appear in the `where' field, it must be non-null
          otherwise.

    `starpu_opencl_func_t opencl_func' (optional)
          This field has been made deprecated. One should use instead
          the `opencl_funcs' field.

    `starpu_opencl_func_t opencl_funcs[STARPU_MAXIMPLEMENTATIONS]' (optional)
          Is an array of function pointers to the OpenCL
          implementations of the codelet.  It must be terminated by a
          NULL value.  The functions prototype must be: `void
          opencl_func(void *buffers[], void *cl_arg);'.  If the `where'
          field is set, then the `opencl_funcs' field is ignored if
          `STARPU_OPENCL' does not appear in the `where' field, it must
          be non-null otherwise.

    `uint8_t gordon_func' (optional)
          This field has been made deprecated. One should use instead
          the `gordon_funcs' field.

    `uint8_t gordon_funcs[STARPU_MAXIMPLEMENTATIONS]' (optional)
          Is an array of index of the Cell SPU implementations of the
          codelet within the Gordon library.  It must be terminated by
          a NULL value.  See Gordon documentation for more details on
          how to register a kernel and retrieve its index.

    `unsigned nbuffers'
          Specifies the number of arguments taken by the codelet. These
          arguments are managed by the DSM and are accessed from the
          `void *buffers[]' array. The constant argument passed with
          the `cl_arg' field of the `starpu_task' structure is not
          counted in this number.  This value should not be above
          `STARPU_NMAXBUFS'.

    `enum starpu_access_mode modes[STARPU_NMAXBUFS]'
          Is an array of `enum starpu_access_mode'. It describes the
          required access modes to the data neeeded by the codelet (e.g.
          `STARPU_RW'). The number of entries in this array must be
          specified in the `nbuffers' field (defined above), and should
          not exceed `STARPU_NMAXBUFS'.  If unsufficient, this value
          can be set with the `--enable-maxbuffers' option when
          configuring StarPU.

    `struct starpu_perfmodel *model' (optional)
          This is a pointer to the task duration performance model
          associated to this codelet. This optional field is ignored
          when set to `NULL'.

    `struct starpu_perfmodel *power_model' (optional)
          This is a pointer to the task power consumption performance
          model associated to this codelet. This optional field is
          ignored when set to `NULL'.  In the case of parallel
          codelets, this has to account for all processing units
          involved in the parallel execution.

    `unsigned long per_worker_stats[STARPU_NMAXWORKERS]' (optional)
          Statistics collected at runtime: this is filled by StarPU and
          should not be accessed directly, but for example by calling
          the `starpu_display_codelet_stats' function (See *note
          starpu_display_codelet_stats:: for details).

    `const char *name' (optional)
          Define the name of the codelet. This can be useful for
          debugging purposes.


 -- Function: void starpu_codelet_init (struct starpu_codelet *CL)
     Initialize CL with default values. Codelets should preferably be
     initialized statically as shown in *note Defining a Codelet::.
     However such a initialisation is not always possible, e.g. when
     using C++.

 -- Data Type: enum starpu_task_status
     State of a task, can be either of
    `STARPU_TASK_INVALID' The task has just been initialized.

    `STARPU_TASK_BLOCKED' The task has just been submitted, and its dependencies has not been checked yet.

    `STARPU_TASK_READY' The task is ready for execution.

    `STARPU_TASK_RUNNING' The task is running on some worker.

    `STARPU_TASK_FINISHED' The task is finished executing.

    `STARPU_TASK_BLOCKED_ON_TAG' The task is waiting for a tag.

    `STARPU_TASK_BLOCKED_ON_TASK' The task is waiting for a task.

    `STARPU_TASK_BLOCKED_ON_DATA' The task is waiting for some data.

 -- Data Type: struct starpu_buffer_descr
     This type is used to describe a data handle along with an access
     mode.
    `starpu_data_handle_t handle' describes a data,

    `enum starpu_access_mode mode' describes its access mode

 -- Data Type: struct starpu_task
     The `starpu_task' structure describes a task that can be offloaded
     on the various processing units managed by StarPU. It instantiates
     a codelet. It can either be allocated dynamically with the
     `starpu_task_create' method, or declared statically. In the latter
     case, the programmer has to zero the `starpu_task' structure and
     to fill the different fields properly. The indicated default
     values correspond to the configuration of a task allocated with
     `starpu_task_create'.

    `struct starpu_codelet *cl'
          Is a pointer to the corresponding `struct starpu_codelet'
          data structure. This describes where the kernel should be
          executed, and supplies the appropriate implementations. When
          set to `NULL', no code is executed during the tasks, such
          empty tasks can be useful for synchronization purposes.

    `struct starpu_buffer_descr buffers[STARPU_NMAXBUFS]'
          This field has been made deprecated. One should use instead
          the `handles' field to specify the handles to the data
          accessed by the task. The access modes are now defined in the
          `mode' field of the `struct starpu_codelet cl' field defined
          above.

    `starpu_data_handle_t handles[STARPU_NMAXBUFS]'
          Is an array of `starpu_data_handle_t'. It specifies the
          handles to the different pieces of data accessed by the task.
          The number of entries in this array must be specified in the
          `nbuffers' field of the `struct starpu_codelet' structure,
          and should not exceed `STARPU_NMAXBUFS'.  If unsufficient,
          this value can be set with the `--enable-maxbuffers' option
          when configuring StarPU.

    `void *interfaces[STARPU_NMAXBUFS]'
          The actual data pointers to the memory node where execution
          will happen, managed by the DSM.

    `void *cl_arg' (optional; default: `NULL')
          This pointer is passed to the codelet through the second
          argument of the codelet implementation (e.g. `cpu_func' or
          `cuda_func').  In the specific case of the Cell processor,
          see the `cl_arg_size' argument.

    `size_t cl_arg_size' (optional, Cell-specific)
          In the case of the Cell processor, the `cl_arg' pointer is
          not directly given to the SPU function. A buffer of size
          `cl_arg_size' is allocated on the SPU. This buffer is then
          filled with the `cl_arg_size' bytes starting at address
          `cl_arg'. In this case, the argument given to the SPU codelet
          is therefore not the `cl_arg' pointer, but the address of the
          buffer in local store (LS) instead. This field is ignored for
          CPU, CUDA and OpenCL codelets, where the `cl_arg' pointer is
          given as such.

    `void (*callback_func)(void *)' (optional) (default: `NULL')
          This is a function pointer of prototype `void (*f)(void *)'
          which specifies a possible callback. If this pointer is
          non-null, the callback function is executed _on the host_
          after the execution of the task. The callback is passed the
          value contained in the `callback_arg' field. No callback is
          executed if the field is set to `NULL'.

    `void *callback_arg' (optional) (default: `NULL')
          This is the pointer passed to the callback function. This
          field is ignored if the `callback_func' is set to `NULL'.

    `unsigned use_tag' (optional) (default: `0')
          If set, this flag indicates that the task should be
          associated with the tag contained in the `tag_id' field. Tag
          allow the application to synchronize with the task and to
          express task dependencies easily.

    `starpu_tag_t tag_id'
          This fields contains the tag associated to the task if the
          `use_tag' field was set, it is ignored otherwise.

    `unsigned synchronous'
          If this flag is set, the `starpu_task_submit' function is
          blocking and returns only when the task has been executed (or
          if no worker is able to process the task). Otherwise,
          `starpu_task_submit' returns immediately.

    `int priority' (optional) (default: `STARPU_DEFAULT_PRIO')
          This field indicates a level of priority for the task. This
          is an integer value that must be set between the return
          values of the `starpu_sched_get_min_priority' function for
          the least important tasks, and that of the
          `starpu_sched_get_max_priority' for the most important tasks
          (included). The `STARPU_MIN_PRIO' and `STARPU_MAX_PRIO' macros
          are provided for convenience and respectively returns value of
          `starpu_sched_get_min_priority' and
          `starpu_sched_get_max_priority'.  Default priority is
          `STARPU_DEFAULT_PRIO', which is always defined as 0 in order
          to allow static task initialization.  Scheduling strategies
          that take priorities into account can use this parameter to
          take better scheduling decisions, but the scheduling policy
          may also ignore it.

    `unsigned execute_on_a_specific_worker' (default: `0')
          If this flag is set, StarPU will bypass the scheduler and
          directly affect this task to the worker specified by the
          `workerid' field.

    `unsigned workerid' (optional)
          If the `execute_on_a_specific_worker' field is set, this
          field indicates which is the identifier of the worker that
          should process this task (as returned by
          `starpu_worker_get_id'). This field is ignored if
          `execute_on_a_specific_worker' field is set to 0.

    `starpu_task_bundle_t bundle' (optional)
          The bundle that includes this task. If no bundle is used,
          this should be NULL.

    `int detach' (optional) (default: `1')
          If this flag is set, it is not possible to synchronize with
          the task by the means of `starpu_task_wait' later on.
          Internal data structures are only guaranteed to be freed once
          `starpu_task_wait' is called if the flag is not set.

    `int destroy' (optional) (default: `0' for starpu_task_init, `1' for starpu_task_create)
          If this flag is set, the task structure will automatically be
          freed, either after the execution of the callback if the task
          is detached, or during `starpu_task_wait' otherwise. If this
          flag is not set, dynamically allocated data structures will
          not be freed until `starpu_task_destroy' is called
          explicitly. Setting this flag for a statically allocated task
          structure will result in undefined behaviour. The flag is set
          to 1 when the task is created by calling
          `starpu_task_create()'. Note that `starpu_task_wait_for_all'
          will not free any task.

    `int regenerate' (optional)
          If this flag is set, the task will be re-submitted to StarPU
          once it has been executed. This flag must not be set if the
          destroy flag is set too.

    `enum starpu_task_status status' (optional)
          Current state of the task.

    `struct starpu_task_profiling_info *profiling_info' (optional)
          Profiling information for the task.

    `double predicted' (output field)
          Predicted duration of the task. This field is only set if the
          scheduling strategy used performance models.

    `double predicted_transfer' (optional)
          Predicted data transfer duration for the task in
          microseconds. This field is only valid if the scheduling
          strategy uses performance models.

    `struct starpu_task *prev'
          A pointer to the previous task. This should only be used by
          StarPU.

    `struct starpu_task *next'
          A pointer to the next task. This should only be used by
          StarPU.

    `unsigned int mf_skip'
          This is only used for tasks that use multiformat handle. This
          should only be used by StarPU.

    `void *starpu_private'
          This is private to StarPU, do not modify. If the task is
          allocated by hand (without starpu_task_create), this field
          should be set to NULL.

    `int magic'
          This field is set when initializing a task. It prevents a
          task from being submitted if it has not been properly
          initialized.

 -- Function: void starpu_task_init (struct starpu_task *TASK)
     Initialize TASK with default values. This function is implicitly
     called by `starpu_task_create'. By default, tasks initialized with
     `starpu_task_init' must be deinitialized explicitly with
     `starpu_task_deinit'. Tasks can also be initialized statically,
     using `STARPU_TASK_INITIALIZER' defined below.

 -- Macro: STARPU_TASK_INITIALIZER
     It is possible to initialize statically allocated tasks with this
     value. This is equivalent to initializing a starpu_task structure
     with the `starpu_task_init' function defined above.

 -- Function: struct starpu_task * starpu_task_create (void)
     Allocate a task structure and initialize it with default values.
     Tasks allocated dynamically with `starpu_task_create' are
     automatically freed when the task is terminated. This means that
     the task pointer can not be used any more once the task is
     submitted, since it can be executed at any time (unless
     dependencies make it wait) and thus freed at any time.  If the
     destroy flag is explicitly unset, the resources used by the task
     have to be freed by calling `starpu_task_destroy'.

 -- Function: void starpu_task_deinit (struct starpu_task *TASK)
     Release all the structures automatically allocated to execute
     TASK, but not the task structure itself. It is thus useful for
     statically allocated tasks for instance.  It is called
     automatically by `starpu_task_destroy'.  It has to be called only
     after explicitly waiting for the task or after `starpu_shutdown'
     (waiting for the callback is not enough, since starpu still
     manipulates the task after calling the callback).

 -- Function: void starpu_task_destroy (struct starpu_task *TASK)
     Free the resource allocated during `starpu_task_create' and
     associated with TASK. This function is already called automatically
     after the execution of a task when the `destroy' flag of the
     `starpu_task' structure is set, which is the default for tasks
     created by `starpu_task_create'.  Calling this function on a
     statically allocated task results in an undefined behaviour.

 -- Function: int starpu_task_wait (struct starpu_task *TASK)
     This function blocks until TASK has been executed. It is not
     possible to synchronize with a task more than once. It is not
     possible to wait for synchronous or detached tasks.

     Upon successful completion, this function returns 0. Otherwise,
     `-EINVAL' indicates that the specified task was either synchronous
     or detached.

 -- Function: int starpu_task_submit (struct starpu_task *TASK)
     This function submits TASK to StarPU. Calling this function does
     not mean that the task will be executed immediately as there can
     be data or task (tag) dependencies that are not fulfilled yet:
     StarPU will take care of scheduling this task with respect to such
     dependencies.  This function returns immediately if the
     `synchronous' field of the `starpu_task' structure was set to 0,
     and block until the termination of the task otherwise. It is also
     possible to synchronize the application with asynchronous tasks by
     the means of tags, using the `starpu_tag_wait' function for
     instance.

     In case of success, this function returns 0, a return value of
     `-ENODEV' means that there is no worker able to process this task
     (e.g. there is no GPU available and this task is only implemented
     for CUDA devices).

 -- Function: int starpu_task_wait_for_all (void)
     This function blocks until all the tasks that were submitted are
     terminated. It does not destroy these tasks.

 -- Function: struct starpu_task * starpu_task_get_current (void)
     This function returns the task currently executed by the worker, or
     NULL if it is called either from a thread that is not a task or
     simply because there is no task being executed at the moment.

 -- Function: void starpu_display_codelet_stats (struct starpu_codelet
          *CL)
     Output on `stderr' some statistics on the codelet CL.

 -- Function: int starpu_task_wait_for_no_ready (void)
     This function waits until there is no more ready task.


File: starpu.info,  Node: Explicit Dependencies,  Next: Implicit Data Dependencies,  Prev: Codelets and Tasks,  Up: StarPU Basic API

13.7 Explicit Dependencies
==========================

 -- Function: void starpu_task_declare_deps_array (struct starpu_task
          *TASK, unsigned NDEPS, struct starpu_task *TASK_ARRAY[])
     Declare task dependencies between a TASK and an array of tasks of
     length NDEPS. This function must be called prior to the submission
     of the task, but it may called after the submission or the
     execution of the tasks in the array, provided the tasks are still
     valid (ie. they were not automatically destroyed). Calling this
     function on a task that was already submitted or with an entry of
     TASK_ARRAY that is not a valid task anymore results in an
     undefined behaviour. If NDEPS is null, no dependency is added. It
     is possible to call `starpu_task_declare_deps_array' multiple
     times on the same task, in this case, the dependencies are added.
     It is possible to have redundancy in the task dependencies.

 -- Data Type: starpu_tag_t
     This type defines a task logical identifer. It is possible to
     associate a task with a unique "tag" chosen by the application,
     and to express dependencies between tasks by the means of those
     tags. To do so, fill the `tag_id' field of the `starpu_task'
     structure with a tag number (can be arbitrary) and set the
     `use_tag' field to 1.

     If `starpu_tag_declare_deps' is called with this tag number, the
     task will not be started until the tasks which holds the declared
     dependency tags are completed.

 -- Function: void starpu_tag_declare_deps (starpu_tag_t ID, unsigned
          NDEPS, ...)
     Specify the dependencies of the task identified by tag ID. The
     first argument specifies the tag which is configured, the second
     argument gives the number of tag(s) on which ID depends. The
     following arguments are the tags which have to be terminated to
     unlock the task.

     This function must be called before the associated task is
     submitted to StarPU with `starpu_task_submit'.

     Because of the variable arity of `starpu_tag_declare_deps', note
     that the last arguments _must_ be of type `starpu_tag_t': constant
     values typically need to be explicitly casted. Using the
     `starpu_tag_declare_deps_array' function avoids this hazard.

          /*  Tag 0x1 depends on tags 0x32 and 0x52 */
          starpu_tag_declare_deps((starpu_tag_t)0x1,
                  2, (starpu_tag_t)0x32, (starpu_tag_t)0x52);

 -- Function: void starpu_tag_declare_deps_array (starpu_tag_t ID,
          unsigned NDEPS, starpu_tag_t *ARRAY)
     This function is similar to `starpu_tag_declare_deps', except that
     its does not take a variable number of arguments but an array of
     tags of size NDEPS.
          /*  Tag 0x1 depends on tags 0x32 and 0x52 */
          starpu_tag_t tag_array[2] = {0x32, 0x52};
          starpu_tag_declare_deps_array((starpu_tag_t)0x1, 2, tag_array);

 -- Function: int starpu_tag_wait (starpu_tag_t ID)
     This function blocks until the task associated to tag ID has been
     executed. This is a blocking call which must therefore not be
     called within tasks or callbacks, but only from the application
     directly.  It is possible to synchronize with the same tag
     multiple times, as long as the `starpu_tag_remove' function is not
     called.  Note that it is still possible to synchronize with a tag
     associated to a task which `starpu_task' data structure was freed
     (e.g. if the `destroy' flag of the `starpu_task' was enabled).

 -- Function: int starpu_tag_wait_array (unsigned NTAGS, starpu_tag_t
          *ID)
     This function is similar to `starpu_tag_wait' except that it
     blocks until _all_ the NTAGS tags contained in the ID array are
     terminated.

 -- Function: void starpu_tag_remove (starpu_tag_t ID)
     This function releases the resources associated to tag ID. It can
     be called once the corresponding task has been executed and when
     there is no other tag that depend on this tag anymore.

 -- Function: void starpu_tag_notify_from_apps (starpu_tag_t ID)
     This function explicitly unlocks tag ID. It may be useful in the
     case of applications which execute part of their computation
     outside StarPU tasks (e.g. third-party libraries).  It is also
     provided as a convenient tool for the programmer, for instance to
     entirely construct the task DAG before actually giving StarPU the
     opportunity to execute the tasks.


File: starpu.info,  Node: Implicit Data Dependencies,  Next: Performance Model API,  Prev: Explicit Dependencies,  Up: StarPU Basic API

13.8 Implicit Data Dependencies
===============================

In this section, we describe how StarPU makes it possible to insert
implicit task dependencies in order to enforce sequential data
consistency. When this data consistency is enabled on a specific data
handle, any data access will appear as sequentially consistent from the
application. For instance, if the application submits two tasks that
access the same piece of data in read-only mode, and then a third task
that access it in write mode, dependencies will be added between the
two first tasks and the third one. Implicit data dependencies are also
inserted in the case of data accesses from the application.

 -- Function: void starpu_data_set_default_sequential_consistency_flag
          (unsigned FLAG)
     Set the default sequential consistency flag. If a non-zero value
     is passed, a sequential data consistency will be enforced for all
     handles registered after this function call, otherwise it is
     disabled. By default, StarPU enables sequential data consistency.
     It is also possible to select the data consistency mode of a
     specific data handle with the
     `starpu_data_set_sequential_consistency_flag' function.

 -- Function: unsigned
starpu_data_get_default_sequential_consistency_flag (void)
     Return the default sequential consistency flag

 -- Function: void starpu_data_set_sequential_consistency_flag
          (starpu_data_handle_t HANDLE, unsigned FLAG)
     Sets the data consistency mode associated to a data handle. The
     consistency mode set using this function has the priority over the
     default mode which can be set with
     `starpu_data_set_default_sequential_consistency_flag'.


File: starpu.info,  Node: Performance Model API,  Next: Profiling API,  Prev: Implicit Data Dependencies,  Up: StarPU Basic API

13.9 Performance Model API
==========================

 -- Data Type: enum starpu_perf_archtype
     Enumerates the various types of architectures.  CPU types range
     within STARPU_CPU_DEFAULT (1 CPU), STARPU_CPU_DEFAULT+1 (2 CPUs),
     ... STARPU_CPU_DEFAULT + STARPU_MAXCPUS - 1 (STARPU_MAXCPUS CPUs).
     CUDA types range within STARPU_CUDA_DEFAULT (GPU number 0),
     STARPU_CUDA_DEFAULT + 1 (GPU number 1), ..., STARPU_CUDA_DEFAULT +
     STARPU_MAXCUDADEVS - 1 (GPU number STARPU_MAXCUDADEVS - 1).
     OpenCL types range within STARPU_OPENCL_DEFAULT (GPU number 0),
     STARPU_OPENCL_DEFAULT + 1 (GPU number 1), ...,
     STARPU_OPENCL_DEFAULT + STARPU_MAXOPENCLDEVS - 1 (GPU number
     STARPU_MAXOPENCLDEVS - 1).
    `STARPU_CPU_DEFAULT'

    `STARPU_CUDA_DEFAULT'

    `STARPU_OPENCL_DEFAULT'

    `STARPU_GORDON_DEFAULT'

 -- Data Type: enum starpu_perfmodel_type
     The possible values are:
    `STARPU_PER_ARCH' for application-provided per-arch cost model functions.

    `STARPU_COMMON' for application-provided common cost model function, with per-arch factor.

    `STARPU_HISTORY_BASED' for automatic history-based cost model.

    `STARPU_REGRESSION_BASED' for automatic linear regression-based cost model (alpha * size ^ beta).

    `STARPU_NL_REGRESSION_BASED' for automatic non-linear regression-based cost mode (a * size ^ b + c).

 -- Data Type: struct starpu_perfmodel
     contains all information about a performance model. At least the
     `type' and `symbol' fields have to be filled when defining a
     performance model for a codelet. If not provided, other fields
     have to be zero.

    `type'
          is the type of performance model `enum starpu_perfmodel_type':
          `STARPU_HISTORY_BASED', `STARPU_REGRESSION_BASED',
          `STARPU_NL_REGRESSION_BASED': No other fields needs to be
          provided, this is purely history-based. `STARPU_PER_ARCH':
          `per_arch' has to be filled with functions which return the
          cost in micro-seconds. `STARPU_COMMON': `cost_function' has
          to be filled with a function that returns the cost in
          micro-seconds on a CPU, timing on other archs will be
          determined by multiplying by an arch-specific factor.

    `const char *symbol'
          is the symbol name for the performance model, which will be
          used as file name to store the model.

    `double (*cost_model)(struct starpu_buffer_descr *)'
          This field is deprecated. Use instead the `cost_function'
          field.

    `double (*cost_function)(struct starpu_task *, unsigned nimpl)'
          Used by `STARPU_COMMON': takes a task and implementation
          number, and must return a task duration estimation in
          micro-seconds.

    `size_t (*size_base)(struct starpu_task *, unsigned nimpl)'
          Used by `STARPU_HISTORY_BASED' and
          `STARPU_*REGRESSION_BASED'. If not NULL, takes a task and
          implementation number, and returns the size to be used as
          index for history and regression.

    `struct starpu_per_arch_perfmodel per_arch[STARPU_NARCH_VARIATIONS][STARPU_MAXIMPLEMENTATIONS]'
          Used by `STARPU_PER_ARCH': array of `struct
          starpu_per_arch_perfmodel' structures.

    `unsigned is_loaded'
          Whether the performance model is already loaded from the disk.

    `unsigned benchmarking'
          Whether the performance model is still being calibrated.

    `pthread_rwlock_t model_rwlock'
          Lock to protect concurrency between loading from disk (W),
          updating the values (W), and making a performance estimation
          (R).


 -- Data Type: struct starpu_regression_model
    `double sumlny' sum of ln(measured)

    `double sumlnx' sum of ln(size)

    `double sumlnx2' sum of ln(size)^2

    `unsigned long minx' minimum size

    `unsigned long maxx' maximum size

    `double sumlnxlny' sum of ln(size)*ln(measured)

    `double alpha' 	 estimated = alpha * size ^ beta

    `double beta'

    `unsigned valid' whether the linear regression model is valid (i.e. enough measures)

    `double a, b, c' estimaed = a size ^b + c

    `unsigned nl_valid' whether the non-linear regression model is valid (i.e. enough measures)

    `unsigned nsample' number of sample values for non-linear regression

 -- Data Type: struct starpu_per_arch_perfmodel
     contains information about the performance model of a given arch.

    `double (*cost_model)(struct starpu_buffer_descr *t)'
          This field is deprecated. Use instead the `cost_function'
          field.

    `double (*cost_function)(struct starpu_task *task, enum starpu_perf_archtype arch, unsigned nimpl)'
          Used by `STARPU_PER_ARCH', must point to functions which take
          a task, the target arch and implementation number (as mere
          conveniency, since the array is already indexed by these),
          and must return a task duration estimation in micro-seconds.

    `size_t (*size_base)(struct starpu_task *, enum'
          starpu_perf_archtype arch, unsigned nimpl) Same as in *note
          struct starpu_perfmodel::, but per-arch, in case it depends
          on the architecture-specific implementation.

    `struct starpu_htbl32_node *history'
          The history of performance measurements.

    `struct starpu_history_list *list'
          Used by `STARPU_HISTORY_BASED' and
          `STARPU_NL_REGRESSION_BASED', records all execution history
          measures.

    `struct starpu_regression_model regression'
          Used by `STARPU_HISTORY_REGRESION_BASED' and
          `STARPU_NL_REGRESSION_BASED', contains the estimated factors
          of the regression.


 -- Function: int starpu_load_history_debug (const char *SYMBOL, struct
          starpu_perfmodel *MODEL)
     loads a given performance model. The MODEL structure has to be
     completely zero, and will be filled with the information saved in
     `~/.starpu'.

 -- Function: void starpu_perfmodel_debugfilepath (struct
          starpu_perfmodel *MODEL, enum starpu_perf_archtype ARCH, char
          *PATH, size_t MAXLEN, unsigned nimpl)
     returns the path to the debugging information for the performance
     model.

 -- Function: void starpu_perfmodel_get_arch_name (enum
          starpu_perf_archtype ARCH, char *ARCHNAME, size_t MAXLEN,
          unsigned nimpl)
     returns the architecture name for ARCH.

 -- Function: void starpu_force_bus_sampling (void)
     forces sampling the bus performance model again.

 -- Function: enum starpu_perf_archtype starpu_worker_get_perf_archtype
          (int WORKERID)
     returns the architecture type of a given worker.

 -- Function: int starpu_list_models (FILE *OUTPUT)
     prints a list of all performance models on OUTPUT.

 -- Function: void starpu_bus_print_bandwidth (FILE *F)
     prints a matrix of bus bandwidths on F.


File: starpu.info,  Node: Profiling API,  Next: CUDA extensions,  Prev: Performance Model API,  Up: StarPU Basic API

13.10 Profiling API
===================

 -- Function: int starpu_profiling_status_set (int STATUS)
     Thie function sets the profiling status. Profiling is activated by
     passing `STARPU_PROFILING_ENABLE' in STATUS. Passing
     `STARPU_PROFILING_DISABLE' disables profiling. Calling this
     function resets all profiling measurements. When profiling is
     enabled, the `profiling_info' field of the `struct starpu_task'
     structure points to a valid `struct starpu_task_profiling_info'
     structure containing information about the execution of the task.

     Negative return values indicate an error, otherwise the previous
     status is returned.

 -- Function: int starpu_profiling_status_get (void)
     Return the current profiling status or a negative value in case
     there was an error.

 -- Function: void starpu_set_profiling_id (int NEW_ID)
     This function sets the ID used for profiling trace filename

 -- Data Type: struct starpu_task_profiling_info
     This structure contains information about the execution of a task.
     It is accessible from the `.profiling_info' field of the
     `starpu_task' structure if profiling was enabled. The different
     fields are:

    `struct timespec submit_time'
          Date of task submission (relative to the initialization of
          StarPU).

    `struct timespec push_start_time'
          Time when the task was submitted to the scheduler.

    `struct timespec push_end_time'
          Time when the scheduler finished with the task submission.

    `struct timespec pop_start_time'
          Time when the scheduler started to be requested for a task,
          and eventually gave that task.

    `struct timespec pop_end_time'
          Time when the scheduler finished providing the task for
          execution.

    `struct timespec acquire_data_start_time'
          Time when the worker started fetching input data.

    `struct timespec acquire_data_end_time'
          Time when the worker finished fetching input data.

    `struct timespec start_time'
          Date of task execution beginning (relative to the
          initialization of StarPU).

    `struct timespec end_time'
          Date of task execution termination (relative to the
          initialization of StarPU).

    `struct timespec release_data_start_time'
          Time when the worker started releasing data.

    `struct timespec release_data_end_time'
          Time when the worker finished releasing data.

    `struct timespec callback_start_time'
          Time when the worker started the application callback for the
          task.

    `struct timespec callback_end_time'
          Time when the worker finished the application callback for
          the task.

    `workerid'
          Identifier of the worker which has executed the task.

    `uint64_t used_cycles'
          Number of cycles used by the task, only available in the
          MoviSim

    `uint64_t stall_cycles'
          Number of cycles stalled within the task, only available in
          the MoviSim

    `double power_consumed'
          Power consumed by the task, only available in the MoviSim


 -- Data Type: struct starpu_worker_profiling_info
     This structure contains the profiling information associated to a
     worker. The different fields are:

    `struct timespec start_time'
          Starting date for the reported profiling measurements.

    `struct timespec total_time'
          Duration of the profiling measurement interval.

    `struct timespec executing_time'
          Time spent by the worker to execute tasks during the
          profiling measurement interval.

    `struct timespec sleeping_time'
          Time spent idling by the worker during the profiling
          measurement interval.

    `int executed_tasks'
          Number of tasks executed by the worker during the profiling
          measurement interval.

    `uint64_t used_cycles'
          Number of cycles used by the worker, only available in the
          MoviSim

    `uint64_t stall_cycles'
          Number of cycles stalled within the worker, only available in
          the MoviSim

    `double power_consumed'
          Power consumed by the worker, only available in the MoviSim


 -- Function: int starpu_worker_get_profiling_info (int WORKERID,
          struct starpu_worker_profiling_info *WORKER_INFO)
     Get the profiling info associated to the worker identified by
     WORKERID, and reset the profiling measurements. If the WORKER_INFO
     argument is NULL, only reset the counters associated to worker
     WORKERID.

     Upon successful completion, this function returns 0. Otherwise, a
     negative value is returned.

 -- Data Type: struct starpu_bus_profiling_info
     The different fields are:
    `struct timespec start_time'
          Time of bus profiling startup.

    `struct timespec total_time'
          Total time of bus profiling.

    `int long long transferred_bytes'
          Number of bytes transferred during profiling.

    `int transfer_count'
          Number of transfers during profiling.

 -- Function: int starpu_bus_get_profiling_info (int BUSID, struct
          starpu_bus_profiling_info *BUS_INFO)
     Get the profiling info associated to the worker designated by
     WORKERID, and reset the profiling measurements. If worker_info is
     NULL, only reset the counters.

 -- Function: int starpu_bus_get_count (void)
     Return the number of buses in the machine.

 -- Function: int starpu_bus_get_id (int SRC, int DST)
     Return the identifier of the bus between SRC and DST

 -- Function: int starpu_bus_get_src (int BUSID)
     Return the source point of bus BUSID

 -- Function: int starpu_bus_get_dst (int BUSID)
     Return the destination point of bus BUSID

 -- Function: double starpu_timing_timespec_delay_us (struct timespec
          *START, struct timespec *END)
     Returns the time elapsed between START and END in microseconds.

 -- Function: double starpu_timing_timespec_to_us (struct timespec *TS)
     Converts the given timespec TS into microseconds.

 -- Function: void starpu_bus_profiling_helper_display_summary (void)
     Displays statistics about the bus on stderr.

 -- Function: void starpu_worker_profiling_helper_display_summary (void)
     Displays statistics about the workers on stderr.


File: starpu.info,  Node: CUDA extensions,  Next: OpenCL extensions,  Prev: Profiling API,  Up: StarPU Basic API

13.11 CUDA extensions
=====================

 -- Macro: STARPU_USE_CUDA
     This macro is defined when StarPU has been installed with CUDA
     support. It should be used in your code to detect the availability
     of CUDA as shown in *note Full source code for the 'Scaling a
     Vector' example::.

 -- Function: cudaStream_t starpu_cuda_get_local_stream (void)
     This function gets the current worker's CUDA stream.  StarPU
     provides a stream for every CUDA device controlled by StarPU. This
     function is only provided for convenience so that programmers can
     easily use asynchronous operations within codelets without having
     to create a stream by hand. Note that the application is not
     forced to use the stream provided by
     `starpu_cuda_get_local_stream' and may also create its own streams.
     Synchronizing with `cudaThreadSynchronize()' is allowed, but will
     reduce the likelihood of having all transfers overlapped.

 -- Function: const struct cudaDeviceProp *
starpu_cuda_get_device_properties (unsigned WORKERID)
     This function returns a pointer to device properties for worker
     WORKERID (assumed to be a CUDA worker).

 -- Function: size_t starpu_cuda_get_global_mem_size (int DEVID)
     Return the size of the global memory of CUDA device DEVID.

 -- Function: void starpu_cuda_report_error (const char *FUNC, const
          char *FILE, int LINE, cudaError_t STATUS)
     Report a CUDA error.

 -- Macro: STARPU_CUDA_REPORT_ERROR (cudaError_t STATUS)
     Calls starpu_cuda_report_error, passing the current function, file
     and line position.

 -- Function: void starpu_helper_cublas_init (void)
     This function initializes CUBLAS on every CUDA device.  The CUBLAS
     library must be initialized prior to any CUBLAS call. Calling
     `starpu_helper_cublas_init' will initialize CUBLAS on every CUDA
     device controlled by StarPU. This call blocks until CUBLAS has
     been properly initialized on every device.

 -- Function: void starpu_helper_cublas_shutdown (void)
     This function synchronously deinitializes the CUBLAS library on
     every CUDA device.

 -- Function: void starpu_cublas_report_error (const char *FUNC, const
          char *FILE, int LINE, cublasStatus STATUS)
     Report a cublas error.

 -- Macro: STARPU_CUBLAS_REPORT_ERROR (cublasStatus STATUS)
     Calls starpu_cublas_report_error, passing the current function,
     file and line position.


File: starpu.info,  Node: OpenCL extensions,  Next: Cell extensions,  Prev: CUDA extensions,  Up: StarPU Basic API

13.12 OpenCL extensions
=======================

* Menu:

* Writing OpenCL kernels::      Writing OpenCL kernels
* Compiling OpenCL kernels::    Compiling OpenCL kernels
* Loading OpenCL kernels::      Loading OpenCL kernels
* OpenCL statistics::           Collecting statistics from OpenCL
* OpenCL utilities::            Utilities for OpenCL

 -- Macro: STARPU_USE_OPENCL
     This macro is defined when StarPU has been installed with OpenCL
     support. It should be used in your code to detect the availability
     of OpenCL as shown in *note Full source code for the 'Scaling a
     Vector' example::.


File: starpu.info,  Node: Writing OpenCL kernels,  Next: Compiling OpenCL kernels,  Up: OpenCL extensions

13.12.1 Writing OpenCL kernels
------------------------------

 -- Function: size_t starpu_opencl_get_global_mem_size (int DEVID)
     Return the size of global device memory in bytes.

 -- Function: void starpu_opencl_get_context (int DEVID, cl_context
          *CONTEXT)
     Places the OpenCL context of the device designated by DEVID into
     CONTEXT.

 -- Function: void starpu_opencl_get_device (int DEVID, cl_device_id
          *DEVICE)
     Places the cl_device_id corresponding to DEVID in DEVICE.

 -- Function: void starpu_opencl_get_queue (int DEVID, cl_command_queue
          *QUEUE)
     Places the command queue of the the device designated by DEVID
     into QUEUE.

 -- Function: void starpu_opencl_get_current_context (cl_context
          *CONTEXT)
     Return the context of the current worker.

 -- Function: void starpu_opencl_get_current_queue (cl_command_queue
          *QUEUE)
     Return the computation kernel command queue of the current worker.

 -- Function: int starpu_opencl_set_kernel_args (cl_int *ERR, cl_kernel
          *KERNEL, ...)
     Sets the arguments of a given kernel. The list of arguments must
     be given as (size_t SIZE_OF_THE_ARGUMENT, cl_mem *
     POINTER_TO_THE_ARGUMENT).  The last argument must be 0. Returns
     the number of arguments that were successfully set. In case of
     failure, ERR is set to the error returned by OpenCL.


File: starpu.info,  Node: Compiling OpenCL kernels,  Next: Loading OpenCL kernels,  Prev: Writing OpenCL kernels,  Up: OpenCL extensions

13.12.2 Compiling OpenCL kernels
--------------------------------

Source codes for OpenCL kernels can be stored in a file or in a string.
StarPU provides functions to build the program executable for each
available OpenCL device as a `cl_program' object. This program
executable can then be loaded within a specific queue as explained in
the next section. These are only helpers, Applications can also fill a
`starpu_opencl_program' array by hand for more advanced use (e.g.
different programs on the different OpenCL devices, for relocation
purpose for instance).

 -- Data Type: struct starpu_opencl_program
     Stores the OpenCL programs as compiled for the different OpenCL
     devices.
    `cl_program programs[STARPU_MAXOPENCLDEVS]'
          Stores each program for each OpenCL device.

 -- Function: int starpu_opencl_load_opencl_from_file (const char
          *SOURCE_FILE_NAME, struct starpu_opencl_program
          *OPENCL_PROGRAMS, const char* BUILD_OPTIONS)
     This function compiles an OpenCL source code stored in a file.

 -- Function: int starpu_opencl_load_opencl_from_string (const char
          *OPENCL_PROGRAM_SOURCE, struct starpu_opencl_program
          *OPENCL_PROGRAMS, const char* BUILD_OPTIONS)
     This function compiles an OpenCL source code stored in a string.

 -- Function: int starpu_opencl_unload_opencl (struct
          starpu_opencl_program *OPENCL_PROGRAMS)
     This function unloads an OpenCL compiled code.


File: starpu.info,  Node: Loading OpenCL kernels,  Next: OpenCL statistics,  Prev: Compiling OpenCL kernels,  Up: OpenCL extensions

13.12.3 Loading OpenCL kernels
------------------------------

 -- Function: int starpu_opencl_load_kernel (cl_kernel *KERNEL,
          cl_command_queue *QUEUE, struct starpu_opencl_program
          *OPENCL_PROGRAMS, const char *KERNEL_NAME, int DEVID)
     Create a kernel KERNEL for device DEVID, on its computation command
     queue returned in QUEUE, using program OPENCL_PROGRAMS and name
     KERNEL_NAME

 -- Function: int starpu_opencl_release_kernel (cl_kernel KERNEL)
     Release the given KERNEL, to be called after kernel execution.


File: starpu.info,  Node: OpenCL statistics,  Next: OpenCL utilities,  Prev: Loading OpenCL kernels,  Up: OpenCL extensions

13.12.4 OpenCL statistics
-------------------------

 -- Function: int starpu_opencl_collect_stats (cl_event EVENT)
     This function allows to collect statistics on a kernel execution.
     After termination of the kernels, the OpenCL codelet should call
     this function to pass it the even returned by
     `clEnqueueNDRangeKernel', to let StarPU collect statistics about
     the kernel execution (used cycles, consumed power).


File: starpu.info,  Node: OpenCL utilities,  Prev: OpenCL statistics,  Up: OpenCL extensions

13.12.5 OpenCL utilities
------------------------

 -- Function: void starpu_opencl_display_error (const char *FUNC, const
          char *FILE, int LINE, const char *MSG, cl_int STATUS)
     Given a valid error STATUS, prints the corresponding error message
     on stdout, along with the given function name FUNC, the given
     filename FILE, the given line number LINE and the given message
     MSG.

 -- Macro: STARPU_OPENCL_DISPLAY_ERROR (cl_int STATUS)
     Call the function `starpu_opencl_display_error' with the given
     error STATUS, the current function name, current file and line
     number, and a empty message.

 -- Function: void starpu_opencl_report_error (const char *FUNC, const
          char *FILE, int LINE, const char *MSG, cl_int STATUS)
     Call the function `starpu_opencl_display_error' and abort.

 -- Macro: STARPU_OPENCL_REPORT_ERROR (cl_int STATUS)
     Call the function `starpu_opencl_report_error' with the given
     error STATUS, with the current function name, current file and
     line number, and a empty message.

 -- Macro: STARPU_OPENCL_REPORT_ERROR_WITH_MSG (const char *MSG, cl_int
          STATUS)
     Call the function `starpu_opencl_report_error' with the given
     message and the given error STATUS, with the current function
     name, current file and line number.

 -- Function: cl_int starpu_opencl_allocate_memory (cl_mem *ADDR,
          size_t SIZE, cl_mem_flags FLAGS)
     Allocate SIZE bytes of memory, stored in ADDR. FLAGS must be a
     valid combination of cl_mem_flags values.

 -- Function: cl_int starpu_opencl_copy_ram_to_opencl_async_sync (void
          *PTR, unsigned SRC_NODE, cl_mem BUFFER, unsigned DST_NODE,
          size_t SIZE, size_t OFFSET, cl_event *EVENT, int *RET)
     Copy SIZE bytes asynchronously from the given PTR on SRC_NODE to
     the given BUFFER on DST_NODE.  OFFSET is the offset, in bytes, in
     BUFFER.  EVENT can be used to wait for this particular copy to
     complete. It can be NULL.  This function returns CL_SUCCESS if the
     copy was successful, or a valid OpenCL error code otherwise. The
     integer pointed to by RET is set to -EAGAIN if the asynchronous
     copy was successful, or to 0 if event was NULL.

 -- Function: cl_int starpu_opencl_copy_ram_to_opencl (void *PTR,
          unsigned SRC_NODE, cl_mem BUFFER, unsigned DST_NODE, size_t
          SIZE, size_t OFFSET, cl_event *EVENT)
     Copy SIZE bytes from the given PTR on SRC_NODE to the given BUFFER
     on DST_NODE. OFFSET is the offset, in bytes, in BUFFER. EVENT can
     be used to wait for this particular copy to complete. It can be
     NULL.  This function returns CL_SUCCESS if the copy was
     successful, or a valid OpenCL error code otherwise.

 -- Function: cl_int starpu_opencl_copy_opencl_to_ram_async_sync
          (cl_mem BUFFER, unsigned SRC_NODE, void *PTR, unsigned
          DST_NODE, size_t SIZE, size_t OFFSET, cl_event *EVENT, int
          *RET)
     Copy SIZE bytes asynchronously from the given BUFFER on SRC_NODE
     to the given PTR on DST_NODE.  OFFSET is the offset, in bytes, in
     BUFFER.  EVENT can be used to wait for this particular copy to
     complete. It can be NULL.  This function returns CL_SUCCESS if the
     copy was successful, or a valid OpenCL error code otherwise. The
     integer pointed to by RET is set to -EAGAIN if the asynchronous
     copy was successful, or to 0 if event was NULL.

 -- Function: cl_int starpu_opencl_copy_opencl_to_ram (cl_mem BUFFER,
          unsigned SRC_NODE, void *PTR, unsigned DST_NODE, size_t SIZE,
          size_t OFFSET, cl_event *EVENT)
     Copy SIZE bytes from the given BUFFER on SRC_NODE to the given PTR
     on DST_NODE. OFFSET is the offset, in bytes, in BUFFER. EVENT can
     be used to wait for this particular copy to complete. It can be
     NULL.  This function returns CL_SUCCESS if the copy was
     successful, or a valid OpenCL error code otherwise.


File: starpu.info,  Node: Cell extensions,  Next: Miscellaneous helpers,  Prev: OpenCL extensions,  Up: StarPU Basic API

13.13 Cell extensions
=====================

nothing yet.


File: starpu.info,  Node: Miscellaneous helpers,  Prev: Cell extensions,  Up: StarPU Basic API

13.14 Miscellaneous helpers
===========================

 -- Function: int starpu_data_cpy (starpu_data_handle_t DST_HANDLE,
          starpu_data_handle_t SRC_HANDLE, int ASYNCHRONOUS, void
          (*CALLBACK_FUNC)(void*), void *CALLBACK_ARG)
     Copy the content of the SRC_HANDLE into the DST_HANDLE handle.
     The ASYNCHRONOUS parameter indicates whether the function should
     block or not. In the case of an asynchronous call, it is possible
     to synchronize with the termination of this operation either by
     the means of implicit dependencies (if enabled) or by calling
     `starpu_task_wait_for_all()'. If CALLBACK_FUNC is not `NULL', this
     callback function is executed after the handle has been copied,
     and it is given the CALLBACK_ARG pointer as argument.

 -- Function: void starpu_execute_on_each_worker (void (*FUNC)(void *),
          void *ARG, uint32_t WHERE)
     This function executes the given function on a subset of workers.
     When calling this method, the offloaded function specified by the
     first argument is executed by every StarPU worker that may execute
     the function.  The second argument is passed to the offloaded
     function.  The last argument specifies on which types of
     processing units the function should be executed. Similarly to the
     WHERE field of the `struct starpu_codelet' structure, it is
     possible to specify that the function should be executed on every
     CUDA device and every CPU by passing `STARPU_CPU|STARPU_CUDA'.
     This function blocks until the function has been executed on every
     appropriate processing units, so that it may not be called from a
     callback function for instance.


File: starpu.info,  Node: StarPU Advanced API,  Next: Configuring StarPU,  Prev: StarPU Basic API,  Up: Top

14 StarPU Advanced API
**********************

* Menu:

* Defining a new data interface::
* Multiformat Data Interface::
* Task Bundles::
* Task Lists::
* Using Parallel Tasks::
* Defining a new scheduling policy::
* Expert mode::


File: starpu.info,  Node: Defining a new data interface,  Next: Multiformat Data Interface,  Up: StarPU Advanced API

14.1 Defining a new data interface
==================================

* Menu:

* Data Interface API::  Data Interface API
* An example of data interface::        An example of data interface


File: starpu.info,  Node: Data Interface API,  Next: An example of data interface,  Up: Defining a new data interface

14.1.1 Data Interface API
-------------------------

 -- Data Type: struct starpu_data_interface_ops
     Per-interface data transfer methods.

    `void (*register_data_handle)(starpu_data_handle_t handle, uint32_t home_node, void *data_interface)'
          Register an existing interface into a data handle.

    `starpu_ssize_t (*allocate_data_on_node)(void *data_interface, uint32_t node)'
          Allocate data for the interface on a given node.

    ` void (*free_data_on_node)(void *data_interface, uint32_t node)'
          Free data of the interface on a given node.

    ` const struct starpu_data_copy_methods *copy_methods'
          ram/cuda/spu/opencl synchronous and asynchronous transfer
          methods.

    ` void * (*handle_to_pointer)(starpu_data_handle_t handle, uint32_t node)'
          Return the current pointer (if any) for the handle on the
          given node.

    ` size_t (*get_size)(starpu_data_handle_t handle)'
          Return an estimation of the size of data, for performance
          models.

    ` uint32_t (*footprint)(starpu_data_handle_t handle)'
          Return a 32bit footprint which characterizes the data size.

    ` int (*compare)(void *data_interface_a, void *data_interface_b)'
          Compare the data size of two interfaces.

    ` void (*display)(starpu_data_handle_t handle, FILE *f)'
          Dump the sizes of a handle to a file.

    ` int (*convert_to_gordon)(void *data_interface, uint64_t *ptr, gordon_strideSize_t *ss)'
          Convert the data size to the spu size format. If no SPUs are
          used, this field can be seto NULL.

    `enum starpu_data_interface_id interfaceid'
          An identifier that is unique to each interface.

    `size_t interface_size'
          The size of the interface data descriptor.


 -- Data Type: struct starpu_data_copy_methods
     Defines the per-interface methods.
    `int {ram,cuda,opencl,spu}_to_{ram,cuda,opencl,spu}(void *src_interface, unsigned src_node, void *dst_interface, unsigned dst_node)'
          These 16 functions define how to copy data from the
          SRC_INTERFACE interface on the SRC_NODE node to the
          DST_INTERFACE interface on the DST_NODE node. They return 0
          on success.

    `int (*ram_to_cuda_async)(void *src_interface, unsigned src_node, void *dst_interface, unsigned dst_node, cudaStream_t stream)'
          Define how to copy data from the SRC_INTERFACE interface on
          the SRC_NODE node (in RAM) to the DST_INTERFACE interface on
          the DST_NODE node (on a CUDA device), using the given STREAM.
          Return 0 on success.

    `int (*cuda_to_ram_async)(void *src_interface, unsigned src_node, void *dst_interface, unsigned dst_node, cudaStream_t stream)'
          Define how to copy data from the SRC_INTERFACE interface on
          the SRC_NODE node (on a CUDA device) to the DST_INTERFACE
          interface on the DST_NODE node (in RAM), using the given
          STREAM. Return 0 on success.

    `int (*cuda_to_cuda_async)(void *src_interface, unsigned src_node, void *dst_interface, unsigned dst_node, cudaStream_t stream)'
          Define how to copy data from the SRC_INTERFACE interface on
          the SRC_NODE node (on a CUDA device) to the DST_INTERFACE
          interface on the DST_NODE node (on another CUDA device),
          using the given STREAM.  Return 0 on success.

    `int (*ram_to_opencl_async)(void *src_interface, unsigned src_node, void *dst_interface, unsigned dst_node, /* cl_event * */ void *event)'
          Define how to copy data from the SRC_INTERFACE interface on
          the SRC_NODE node (in RAM) to the DST_INTERFACE interface on
          the DST_NODE node (on an OpenCL device), using EVENT, a
          pointer to a cl_event. Return 0 on success.

    `int (*opencl_to_ram_async)(void *src_interface, unsigned src_node, void *dst_interface, unsigned dst_node, /* cl_event * */ void *event)'
          Define how to copy data from the SRC_INTERFACE interface on
          the SRC_NODE node (on an OpenCL device) to the DST_INTERFACE
          interface on the DST_NODE node (in RAM), using the given
          EVENT, a pointer to a cl_event. Return 0 on success.

    `int (*opencl_to_opencl_async)(void *src_interface, unsigned src_node, void *dst_interface, unsigned dst_node, /* cl_event * */ void *event)'
          Define how to copy data from the SRC_INTERFACE interface on
          the SRC_NODE node (on an OpenCL device) to the DST_INTERFACE
          interface on the DST_NODE node (on another OpenCL device),
          using the given EVENT, a pointer to a cl_event. Return 0 on
          success.

 -- Function: uint32_t starpu_crc32_be_n (void *INPUT, size_t N,
          uint32_t INPUTCRC)
     Compute the CRC of a byte buffer seeded by the inputcrc "current
     state". The return value should be considered as the new "current
     state" for future CRC computation. This is used for computing data
     size footprint.

 -- Function: uint32_t starpu_crc32_be (uint32_t INPUT, uint32_t
          INPUTCRC)
     Compute the CRC of a 32bit number seeded by the inputcrc "current
     state". The return value should be considered as the new "current
     state" for future CRC computation. This is used for computing data
     size footprint.

 -- Function: uint32_t starpu_crc32_string (char *STR, uint32_t
          INPUTCRC)
     Compute the CRC of a string seeded by the inputcrc "current state".
     The return value should be considered as the new "current state"
     for future CRC computation. This is used for computing data size
     footprint.


File: starpu.info,  Node: An example of data interface,  Prev: Data Interface API,  Up: Defining a new data interface

14.1.2 An example of data interface
-----------------------------------

 -- Function: int starpu_data_interface_get_next_id ()
     Returns the next available id for a newly created data interface.

   Let's define a new data interface to manage complex numbers.

     /* interface for complex numbers */
     struct starpu_complex_interface
     {
             double *real;
             double *imaginary;
             int nx;
     };

   Registering such a data to StarPU is easily done using the function
`starpu_data_register' (*note Basic Data Library API::). The last
parameter of the function, `interface_complex_ops', will be described
below.

     void starpu_complex_data_register(starpu_data_handle_t *handle,
          uint32_t home_node, double *real, double *imaginary, int nx)
     {
             struct starpu_complex_interface complex =
             {
                     .real = real,
                     .imaginary = imaginary,
                     .nx = nx
             };

             if (interface_complex_ops.interfaceid == -1)
             {
                     interface_complex_ops.interfaceid = starpu_data_interface_get_next_id();
             }

             starpu_data_register(handleptr, home_node, &complex, &interface_complex_ops);
     }

   Different operations need to be defined for a data interface through
the type `struct starpu_data_interface_ops' (*note Data Interface
API::). We only define here the basic operations needed to run simple
applications. The source code for the different functions can be found
in the file `examples/interface/complex_interface.c'.

     static struct starpu_data_interface_ops interface_complex_ops =
     {
             .register_data_handle = complex_register_data_handle,
             .allocate_data_on_node = complex_allocate_data_on_node,
             .copy_methods = &complex_copy_methods,
             .get_size = complex_get_size,
             .footprint = complex_footprint,
             .interfaceid = -1,
             .interface_size = sizeof(struct starpu_complex_interface),
     };

   Functions need to be defined to access the different fields of the
complex interface from a StarPU data handle.

     double *starpu_complex_get_real(starpu_data_handle_t handle)
     {
             struct starpu_complex_interface *complex_interface =
               (struct starpu_complex_interface *) starpu_data_get_interface_on_node(handle, 0);
             return complex_interface->real;
     }

     double *starpu_complex_get_imaginary(starpu_data_handle_t handle);
     int starpu_complex_get_nx(starpu_data_handle_t handle);

   Similar functions need to be defined to access the different fields
of the complex interface from a `void *' pointer to be used within
codelet implemetations.

     #define STARPU_COMPLEX_GET_REAL(interface)	\
             (((struct starpu_complex_interface *)(interface))->real)
     #define STARPU_COMPLEX_GET_IMAGINARY(interface)	\
             (((struct starpu_complex_interface *)(interface))->imaginary)
     #define STARPU_COMPLEX_GET_NX(interface)	\
             (((struct starpu_complex_interface *)(interface))->nx)

   Complex data interfaces can then be registered to StarPU.

     double real = 45.0;
     double imaginary = 12.0;
     starpu_complex_data_register(&handle1, 0, &real, &imaginary, 1);
     starpu_insert_task(&cl_display, STARPU_R, handle1, 0);

   and used by codelets.

     void display_complex_codelet(void *descr[], __attribute__ ((unused)) void *_args)
     {
             int nx = STARPU_COMPLEX_GET_NX(descr[0]);
             double *real = STARPU_COMPLEX_GET_REAL(descr[0]);
             double *imaginary = STARPU_COMPLEX_GET_IMAGINARY(descr[0]);
             int i;

             for(i=0 ; i<nx ; i++)
             {
                     fprintf(stderr, "Complex[%d] = %3.2f + %3.2f i\n", i, real[i], imaginary[i]);
             }
     }

   The whole code for this complex data interface is available in the
directory `examples/interface/'.


File: starpu.info,  Node: Multiformat Data Interface,  Next: Task Bundles,  Prev: Defining a new data interface,  Up: StarPU Advanced API

14.2 Multiformat Data Interface
===============================

 -- Data Type: struct starpu_multiformat_data_interface_ops
     The different fields are:
    `size_t cpu_elemsize'
          the size of each element on CPUs,

    `size_t opencl_elemsize'
          the size of each element on OpenCL devices,

    `struct starpu_codelet *cpu_to_opencl_cl'
          pointer to a codelet which converts from CPU to OpenCL

    `struct starpu_codelet *opencl_to_cpu_cl'
          pointer to a codelet which converts from OpenCL to CPU

    `size_t cuda_elemsize'
          the size of each element on CUDA devices,

    `struct starpu_codelet *cpu_to_cuda_cl'
          pointer to a codelet which converts from CPU to CUDA

    `struct starpu_codelet *cuda_to_cpu_cl'
          pointer to a codelet which converts from CUDA to CPU

 -- Function: void starpu_multiformat_data_register
          (starpu_data_handle_t *HANDLE, uint32_t HOME_NODE, void *PTR,
          uint32_t NOBJECTS, struct
          starpu_multiformat_data_interface_ops *FORMAT_OPS)
     Register a piece of data that can be represented in different
     ways, depending upon the processing unit that manipulates it. It
     allows the programmer, for instance, to use an array of structures
     when working on a CPU, and a structure of arrays when working on a
     GPU.

     NOBJECTS is the number of elements in the data. FORMAT_OPS
     describes the format.

 -- Macro: STARPU_MULTIFORMAT_GET_CPU_PTR (void *INTERFACE)
     returns the local pointer to the data with CPU format.

 -- Macro: STARPU_MULTIFORMAT_GET_CUDA_PTR (void *INTERFACE)
     returns the local pointer to the data with CUDA format.

 -- Macro: STARPU_MULTIFORMAT_GET_OPENCL_PTR (void *INTERFACE)
     returns the local pointer to the data with OpenCL format.

 -- Macro: STARPU_MULTIFORMAT_GET_NX (void *INTERFACE)
     returns the number of elements in the data.


File: starpu.info,  Node: Task Bundles,  Next: Task Lists,  Prev: Multiformat Data Interface,  Up: StarPU Advanced API

14.3 Task Bundles
=================

 -- Data Type: starpu_task_bundle_t
     Opaque structure describing a list of tasks that should be
     scheduled on the same worker whenever it's possible. It must be
     considered as a hint given to the scheduler as there is no
     guarantee that they will be executed on the same worker.

 -- Function: void starpu_task_bundle_create (starpu_task_bundle_t
          *BUNDLE)
     Factory function creating and initializing BUNDLE, when the call
     returns, memory needed is allocated and BUNDLE is ready to use.

 -- Function: int starpu_task_bundle_insert (starpu_task_bundle_t
          BUNDLE, struct starpu_task *TASK)
     Insert TASK in BUNDLE. Until TASK is removed from BUNDLE its
     expected length and data transfer time will be considered along
     those of the other tasks of BUNDLE.  This function mustn't be
     called if BUNDLE is already closed and/or TASK is already
     submitted.

 -- Function: int starpu_task_bundle_remove (starpu_task_bundle_t
          BUNDLE, struct starpu_task *TASK)
     Remove TASK from BUNDLE.  Of course TASK must have been previously
     inserted BUNDLE.  This function mustn't be called if BUNDLE is
     already closed and/or TASK is already submitted. Doing so would
     result in undefined behaviour.

 -- Function: void starpu_task_bundle_close (starpu_task_bundle_t
          BUNDLE)
     Inform the runtime that the user won't modify BUNDLE anymore, it
     means no more inserting or removing task. Thus the runtime can
     destroy it when possible.


File: starpu.info,  Node: Task Lists,  Next: Using Parallel Tasks,  Prev: Task Bundles,  Up: StarPU Advanced API

14.4 Task Lists
===============

 -- Data Type: struct starpu_task_list
     Stores a double-chained list of tasks

 -- Function: void starpu_task_list_init (struct starpu_task_list *LIST)
     Initialize a list structure

 -- Function: void starpu_task_list_push_front (struct starpu_task_list
          *LIST, struct starpu_task *TASK)
     Push a task at the front of a list

 -- Function: void starpu_task_list_push_back (struct starpu_task_list
          *LIST, struct starpu_task *TASK)
     Push a task at the back of a list

 -- Function: struct starpu_task * starpu_task_list_front (struct
          starpu_task_list *LIST)
     Get the front of the list (without removing it)

 -- Function: struct starpu_task * starpu_task_list_back (struct
          starpu_task_list *LIST)
     Get the back of the list (without removing it)

 -- Function: int starpu_task_list_empty (struct starpu_task_list *LIST)
     Test if a list is empty

 -- Function: void starpu_task_list_erase (struct starpu_task_list
          *LIST, struct starpu_task *TASK)
     Remove an element from the list

 -- Function: struct starpu_task * starpu_task_list_pop_front (struct
          starpu_task_list *LIST)
     Remove the element at the front of the list

 -- Function: struct starpu_task * starpu_task_list_pop_back (struct
          starpu_task_list *LIST)
     Remove the element at the back of the list

 -- Function: struct starpu_task * starpu_task_list_begin (struct
          starpu_task_list *LIST)
     Get the first task of the list.

 -- Function: struct starpu_task * starpu_task_list_end (struct
          starpu_task_list *LIST)
     Get the end of the list.

 -- Function: struct starpu_task * starpu_task_list_next (struct
          starpu_task *TASK)
     Get the next task of the list. This is not erase-safe.


File: starpu.info,  Node: Using Parallel Tasks,  Next: Defining a new scheduling policy,  Prev: Task Lists,  Up: StarPU Advanced API

14.5 Using Parallel Tasks
=========================

These are used by parallel tasks:

 -- Function: int starpu_combined_worker_get_size (void)
     Return the size of the current combined worker, i.e. the total
     number of cpus running the same task in the case of SPMD parallel
     tasks, or the total number of threads that the task is allowed to
     start in the case of FORKJOIN parallel tasks.

 -- Function: int starpu_combined_worker_get_rank (void)
     Return the rank of the current thread within the combined worker.
     Can only be used in FORKJOIN parallel tasks, to know which part of
     the task to work on.

   Most of these are used for schedulers which support parallel tasks.

 -- Function: unsigned starpu_combined_worker_get_count (void)
     Return the number of different combined workers.

 -- Function: int starpu_combined_worker_get_id (void)
     Return the identifier of the current combined worker.

 -- Function: int starpu_combined_worker_assign_workerid (int NWORKERS,
          int WORKERID_ARRAY[])
     Register a new combined worker and get its identifier

 -- Function: int starpu_combined_worker_get_description (int WORKERID,
          int *WORKER_SIZE, int **COMBINED_WORKERID)
     Get the description of a combined worker

 -- Function: int starpu_combined_worker_can_execute_task (unsigned
          WORKERID, struct starpu_task *TASK, unsigned NIMPL)
     Variant of starpu_worker_can_execute_task compatible with combined
     workers


File: starpu.info,  Node: Defining a new scheduling policy,  Next: Expert mode,  Prev: Using Parallel Tasks,  Up: StarPU Advanced API

14.6 Defining a new scheduling policy
=====================================

TODO

   A full example showing how to define a new scheduling policy is
available in the StarPU sources in the directory `examples/scheduler/'.

* Menu:

* Scheduling Policy API:: Scheduling Policy API
* Source code::

